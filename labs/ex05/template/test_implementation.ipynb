{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import sample_data, load_data, standardize\n",
    "\n",
    "# load data.\n",
    "height, weight, gender = load_data()\n",
    "\n",
    "# build sampled x and y.\n",
    "seed = 1\n",
    "y = np.expand_dims(gender, axis=1)\n",
    "X = np.c_[height.reshape(-1), weight.reshape(-1)]\n",
    "y, X = sample_data(y, X, seed, size_samples=200)\n",
    "x, mean_x, std_x = standardize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse loss by least square: 0.04396742864168485\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from implementations import *\n",
    "from plots import visualization\n",
    "\n",
    "def least_square_classification_demo(y, x):\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    # w = least squares with respect to tx and y\n",
    "    w, mse = least_squares(y, tx)\n",
    "    print('mse loss by least square: {}'.format(mse))\n",
    "    visualization(y, x, mean_x, std_x, w, \"classification_by_least_square\")\n",
    "    \n",
    "least_square_classification_demo(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3)\n",
      "Gradient Descent(0/9999): loss=0.255\n",
      "Gradient Descent(1/9999): loss=0.24967237922679447\n",
      "Gradient Descent(2/9999): loss=0.24449998556055832\n",
      "Gradient Descent(3/9999): loss=0.239477847383057\n",
      "Gradient Descent(4/9999): loss=0.23460116367450934\n",
      "Gradient Descent(5/9999): loss=0.22986529789130158\n",
      "Gradient Descent(6/9999): loss=0.22526577206933354\n",
      "Gradient Descent(7/9999): loss=0.2207982611445564\n",
      "Gradient Descent(8/9999): loss=0.21645858748257854\n",
      "Gradient Descent(9/9999): loss=0.2122427156095262\n",
      "Gradient Descent(10/9999): loss=0.2081467471366351\n",
      "Gradient Descent(11/9999): loss=0.2041669158713368\n",
      "Gradient Descent(12/9999): loss=0.200299583107875\n",
      "Gradient Descent(13/9999): loss=0.1965412330907482\n",
      "Gradient Descent(14/9999): loss=0.19288846864452902\n",
      "Gradient Descent(15/9999): loss=0.18933800696385503\n",
      "Gradient Descent(16/9999): loss=0.18588667555761357\n",
      "Gradient Descent(17/9999): loss=0.18253140834157677\n",
      "Gradient Descent(18/9999): loss=0.17926924187395105\n",
      "Gradient Descent(19/9999): loss=0.17609731172852022\n",
      "Gradient Descent(20/9999): loss=0.1730128490002557\n",
      "Gradient Descent(21/9999): loss=0.1700131769384657\n",
      "Gradient Descent(22/9999): loss=0.16709570770273585\n",
      "Gradient Descent(23/9999): loss=0.16425793923709672\n",
      "Gradient Descent(24/9999): loss=0.16149745225802006\n",
      "Gradient Descent(25/9999): loss=0.15881190735201717\n",
      "Gradient Descent(26/9999): loss=0.15619904217876468\n",
      "Gradient Descent(27/9999): loss=0.15365666877584336\n",
      "Gradient Descent(28/9999): loss=0.15118267096131552\n",
      "Gradient Descent(29/9999): loss=0.14877500183051462\n",
      "Gradient Descent(30/9999): loss=0.14643168134355158\n",
      "Gradient Descent(31/9999): loss=0.14415079400017766\n",
      "Gradient Descent(32/9999): loss=0.1419304865987667\n",
      "Gradient Descent(33/9999): loss=0.13976896607630315\n",
      "Gradient Descent(34/9999): loss=0.13766449742637793\n",
      "Gradient Descent(35/9999): loss=0.13561540169230632\n",
      "Gradient Descent(36/9999): loss=0.13362005403259197\n",
      "Gradient Descent(37/9999): loss=0.1316768818560626\n",
      "Gradient Descent(38/9999): loss=0.12978436302410523\n",
      "Gradient Descent(39/9999): loss=0.12794102411752442\n",
      "Gradient Descent(40/9999): loss=0.12614543876563938\n",
      "Gradient Descent(41/9999): loss=0.12439622603532502\n",
      "Gradient Descent(42/9999): loss=0.12269204887778913\n",
      "Gradient Descent(43/9999): loss=0.1210316126309583\n",
      "Gradient Descent(44/9999): loss=0.11941366357542689\n",
      "Gradient Descent(45/9999): loss=0.11783698754199858\n",
      "Gradient Descent(46/9999): loss=0.11630040856892339\n",
      "Gradient Descent(47/9999): loss=0.11480278760700582\n",
      "Gradient Descent(48/9999): loss=0.11334302127082495\n",
      "Gradient Descent(49/9999): loss=0.11192004063437587\n",
      "Gradient Descent(50/9999): loss=0.11053281006950319\n",
      "Gradient Descent(51/9999): loss=0.10918032612555838\n",
      "Gradient Descent(52/9999): loss=0.10786161644877187\n",
      "Gradient Descent(53/9999): loss=0.10657573873988635\n",
      "Gradient Descent(54/9999): loss=0.10532177974865205\n",
      "Gradient Descent(55/9999): loss=0.10409885430383735\n",
      "Gradient Descent(56/9999): loss=0.10290610437745759\n",
      "Gradient Descent(57/9999): loss=0.1017426981819733\n",
      "Gradient Descent(58/9999): loss=0.10060782929925642\n",
      "Gradient Descent(59/9999): loss=0.09950071584016607\n",
      "Gradient Descent(60/9999): loss=0.09842059963362045\n",
      "Gradient Descent(61/9999): loss=0.09736674544409091\n",
      "Gradient Descent(62/9999): loss=0.0963384402164861\n",
      "Gradient Descent(63/9999): loss=0.09533499234743026\n",
      "Gradient Descent(64/9999): loss=0.09435573098197846\n",
      "Gradient Descent(65/9999): loss=0.09340000533484666\n",
      "Gradient Descent(66/9999): loss=0.09246718403526735\n",
      "Gradient Descent(67/9999): loss=0.09155665449461692\n",
      "Gradient Descent(68/9999): loss=0.09066782229598966\n",
      "Gradient Descent(69/9999): loss=0.08980011060492682\n",
      "Gradient Descent(70/9999): loss=0.08895295960053583\n",
      "Gradient Descent(71/9999): loss=0.0881258259262645\n",
      "Gradient Descent(72/9999): loss=0.08731818215962246\n",
      "Gradient Descent(73/9999): loss=0.08652951630016646\n",
      "Gradient Descent(74/9999): loss=0.08575933127509393\n",
      "Gradient Descent(75/9999): loss=0.08500714446181087\n",
      "Gradient Descent(76/9999): loss=0.08427248722686594\n",
      "Gradient Descent(77/9999): loss=0.08355490448066227\n",
      "Gradient Descent(78/9999): loss=0.08285395424738323\n",
      "Gradient Descent(79/9999): loss=0.08216920724958657\n",
      "Gradient Descent(80/9999): loss=0.0815002465069432\n",
      "Gradient Descent(81/9999): loss=0.08084666694861474\n",
      "Gradient Descent(82/9999): loss=0.080208075038784\n",
      "Gradient Descent(83/9999): loss=0.0795840884148691\n",
      "Gradient Descent(84/9999): loss=0.07897433553796994\n",
      "Gradient Descent(85/9999): loss=0.078378455355112\n",
      "Gradient Descent(86/9999): loss=0.07779609697286843\n",
      "Gradient Descent(87/9999): loss=0.07722691934195695\n",
      "Gradient Descent(88/9999): loss=0.07667059095242239\n",
      "Gradient Descent(89/9999): loss=0.07612678953903036\n",
      "Gradient Descent(90/9999): loss=0.07559520179651132\n",
      "Gradient Descent(91/9999): loss=0.07507552310430658\n",
      "Gradient Descent(92/9999): loss=0.07456745726048211\n",
      "Gradient Descent(93/9999): loss=0.07407071622448642\n",
      "Gradient Descent(94/9999): loss=0.07358501986844215\n",
      "Gradient Descent(95/9999): loss=0.07311009573667089\n",
      "Gradient Descent(96/9999): loss=0.07264567881316313\n",
      "Gradient Descent(97/9999): loss=0.07219151129671446\n",
      "Gradient Descent(98/9999): loss=0.07174734238346\n",
      "Gradient Descent(99/9999): loss=0.0713129280565487\n",
      "Gradient Descent(100/9999): loss=0.07088803088270793\n",
      "Gradient Descent(101/9999): loss=0.07047241981545913\n",
      "Gradient Descent(102/9999): loss=0.07006587000475205\n",
      "Gradient Descent(103/9999): loss=0.0696681626127957\n",
      "Gradient Descent(104/9999): loss=0.06927908463587\n",
      "Gradient Descent(105/9999): loss=0.06889842873191201\n",
      "Gradient Descent(106/9999): loss=0.06852599305367595\n",
      "Gradient Descent(107/9999): loss=0.06816158108727559\n",
      "Gradient Descent(108/9999): loss=0.06780500149592245\n",
      "Gradient Descent(109/9999): loss=0.06745606796868181\n",
      "Gradient Descent(110/9999): loss=0.06711459907407329\n",
      "Gradient Descent(111/9999): loss=0.06678041811835034\n",
      "Gradient Descent(112/9999): loss=0.06645335300829813\n",
      "Gradient Descent(113/9999): loss=0.06613323611839515\n",
      "Gradient Descent(114/9999): loss=0.06581990416218977\n",
      "Gradient Descent(115/9999): loss=0.06551319806774787\n",
      "Gradient Descent(116/9999): loss=0.06521296285703321\n",
      "Gradient Descent(117/9999): loss=0.06491904752908664\n",
      "Gradient Descent(118/9999): loss=0.06463130494687572\n",
      "Gradient Descent(119/9999): loss=0.06434959172769011\n",
      "Gradient Descent(120/9999): loss=0.06407376813696315\n",
      "Gradient Descent(121/9999): loss=0.06380369798540411\n",
      "Gradient Descent(122/9999): loss=0.06353924852932927\n",
      "Gradient Descent(123/9999): loss=0.06328029037408495\n",
      "Gradient Descent(124/9999): loss=0.06302669738045813\n",
      "Gradient Descent(125/9999): loss=0.06277834657397521\n",
      "Gradient Descent(126/9999): loss=0.0625351180569919\n",
      "Gradient Descent(127/9999): loss=0.06229689492348159\n",
      "Gradient Descent(128/9999): loss=0.062063563176432135\n",
      "Gradient Descent(129/9999): loss=0.061835011647764324\n",
      "Gradient Descent(130/9999): loss=0.061611131920688934\n",
      "Gradient Descent(131/9999): loss=0.06139181825442098\n",
      "Gradient Descent(132/9999): loss=0.06117696751117391\n",
      "Gradient Descent(133/9999): loss=0.06096647908535833\n",
      "Gradient Descent(134/9999): loss=0.06076025483491293\n",
      "Gradient Descent(135/9999): loss=0.060558199014697556\n",
      "Gradient Descent(136/9999): loss=0.06036021821188095\n",
      "Gradient Descent(137/9999): loss=0.060166221283258\n",
      "Gradient Descent(138/9999): loss=0.0599761192944336\n",
      "Gradient Descent(139/9999): loss=0.05978982546081242\n",
      "Gradient Descent(140/9999): loss=0.05960725509033597\n",
      "Gradient Descent(141/9999): loss=0.059428325527910435\n",
      "Gradient Descent(142/9999): loss=0.059252956101470335\n",
      "Gradient Descent(143/9999): loss=0.05908106806962606\n",
      "Gradient Descent(144/9999): loss=0.05891258457084333\n",
      "Gradient Descent(145/9999): loss=0.05874743057410603\n",
      "Gradient Descent(146/9999): loss=0.058585532831015204\n",
      "Gradient Descent(147/9999): loss=0.05842681982927753\n",
      "Gradient Descent(148/9999): loss=0.05827122174753969\n",
      "Gradient Descent(149/9999): loss=0.05811867041152551\n",
      "Gradient Descent(150/9999): loss=0.057969099251434925\n",
      "Gradient Descent(151/9999): loss=0.0578224432605644\n",
      "Gradient Descent(152/9999): loss=0.05767863895511045\n",
      "Gradient Descent(153/9999): loss=0.057537624335119386\n",
      "Gradient Descent(154/9999): loss=0.0573993388465467\n",
      "Gradient Descent(155/9999): loss=0.057263723344391775\n",
      "Gradient Descent(156/9999): loss=0.05713072005687412\n",
      "Gradient Descent(157/9999): loss=0.05700027255061887\n",
      "Gradient Descent(158/9999): loss=0.05687232569682\n",
      "Gradient Descent(159/9999): loss=0.056746825638351026\n",
      "Gradient Descent(160/9999): loss=0.056623719757794025\n",
      "Gradient Descent(161/9999): loss=0.056502956646358335\n",
      "Gradient Descent(162/9999): loss=0.05638448607366202\n",
      "Gradient Descent(163/9999): loss=0.05626825895834928\n",
      "Gradient Descent(164/9999): loss=0.0561542273395184\n",
      "Gradient Descent(165/9999): loss=0.05604234434893554\n",
      "Gradient Descent(166/9999): loss=0.055932564184010304\n",
      "Gradient Descent(167/9999): loss=0.055824842081510145\n",
      "Gradient Descent(168/9999): loss=0.05571913429199124\n",
      "Gradient Descent(169/9999): loss=0.05561539805492406\n",
      "Gradient Descent(170/9999): loss=0.055513591574492954\n",
      "Gradient Descent(171/9999): loss=0.05541367399604927\n",
      "Gradient Descent(172/9999): loss=0.055315605383198775\n",
      "Gradient Descent(173/9999): loss=0.05521934669550415\n",
      "Gradient Descent(174/9999): loss=0.05512485976678442\n",
      "Gradient Descent(175/9999): loss=0.05503210728399362\n",
      "Gradient Descent(176/9999): loss=0.05494105276666145\n",
      "Gradient Descent(177/9999): loss=0.0548516605468795\n",
      "Gradient Descent(178/9999): loss=0.05476389574981676\n",
      "Gradient Descent(179/9999): loss=0.05467772427474922\n",
      "Gradient Descent(180/9999): loss=0.05459311277658827\n",
      "Gradient Descent(181/9999): loss=0.054510028647893415\n",
      "Gradient Descent(182/9999): loss=0.05442844000135513\n",
      "Gradient Descent(183/9999): loss=0.0543483156527347\n",
      "Gradient Descent(184/9999): loss=0.054269625104247\n",
      "Gradient Descent(185/9999): loss=0.05419233852837434\n",
      "Gradient Descent(186/9999): loss=0.05411642675209824\n",
      "Gradient Descent(187/9999): loss=0.05404186124153778\n",
      "Gradient Descent(188/9999): loss=0.05396861408698242\n",
      "Gradient Descent(189/9999): loss=0.053896657988308345\n",
      "Gradient Descent(190/9999): loss=0.05382596624076734\n",
      "Gradient Descent(191/9999): loss=0.05375651272113771\n",
      "Gradient Descent(192/9999): loss=0.05368827187422689\n",
      "Gradient Descent(193/9999): loss=0.05362121869971604\n",
      "Gradient Descent(194/9999): loss=0.05355532873933692\n",
      "Gradient Descent(195/9999): loss=0.0534905780643717\n",
      "Gradient Descent(196/9999): loss=0.053426943263466746\n",
      "Gradient Descent(197/9999): loss=0.05336440143075185\n",
      "Gradient Descent(198/9999): loss=0.053302930154255954\n",
      "Gradient Descent(199/9999): loss=0.05324250750461171\n",
      "Gradient Descent(200/9999): loss=0.053183112024040566\n",
      "Gradient Descent(201/9999): loss=0.05312472271561089\n",
      "Gradient Descent(202/9999): loss=0.05306731903276127\n",
      "Gradient Descent(203/9999): loss=0.05301088086908237\n",
      "Gradient Descent(204/9999): loss=0.05295538854834981\n",
      "Gradient Descent(205/9999): loss=0.05290082281480122\n",
      "Gradient Descent(206/9999): loss=0.05284716482365126\n",
      "Gradient Descent(207/9999): loss=0.05279439613183804\n",
      "Gradient Descent(208/9999): loss=0.05274249868899429\n",
      "Gradient Descent(209/9999): loss=0.0526914548286379\n",
      "Gradient Descent(210/9999): loss=0.05264124725957556\n",
      "Gradient Descent(211/9999): loss=0.05259185905751387\n",
      "Gradient Descent(212/9999): loss=0.052543273656872586\n",
      "Gradient Descent(213/9999): loss=0.052495474842794355\n",
      "Gradient Descent(214/9999): loss=0.052448446743345985\n",
      "Gradient Descent(215/9999): loss=0.05240217382190613\n",
      "Gradient Descent(216/9999): loss=0.05235664086973448\n",
      "Gradient Descent(217/9999): loss=0.05231183299871761\n",
      "Gradient Descent(218/9999): loss=0.05226773563428716\n",
      "Gradient Descent(219/9999): loss=0.05222433450850541\n",
      "Gradient Descent(220/9999): loss=0.05218161565331443\n",
      "Gradient Descent(221/9999): loss=0.052139565393944105\n",
      "Gradient Descent(222/9999): loss=0.052098170342475186\n",
      "Gradient Descent(223/9999): loss=0.05205741739155331\n",
      "Gradient Descent(224/9999): loss=0.05201729370825025\n",
      "Gradient Descent(225/9999): loss=0.05197778672806819\n",
      "Gradient Descent(226/9999): loss=0.05193888414908405\n",
      "Gradient Descent(227/9999): loss=0.051900573926229654\n",
      "Gradient Descent(228/9999): loss=0.05186284426570476\n",
      "Gradient Descent(229/9999): loss=0.05182568361951919\n",
      "Gradient Descent(230/9999): loss=0.05178908068016127\n",
      "Gradient Descent(231/9999): loss=0.05175302437538886\n",
      "Gradient Descent(232/9999): loss=0.051717503863140495\n",
      "Gradient Descent(233/9999): loss=0.05168250852656297\n",
      "Gradient Descent(234/9999): loss=0.05164802796915319\n",
      "Gradient Descent(235/9999): loss=0.0516140520100106\n",
      "Gradient Descent(236/9999): loss=0.05158057067919834\n",
      "Gradient Descent(237/9999): loss=0.051547574213209914\n",
      "Gradient Descent(238/9999): loss=0.05151505305053847\n",
      "Gradient Descent(239/9999): loss=0.05148299782734714\n",
      "Gradient Descent(240/9999): loss=0.05145139937323693\n",
      "Gradient Descent(241/9999): loss=0.05142024870711046\n",
      "Gradient Descent(242/9999): loss=0.051389537033128735\n",
      "Gradient Descent(243/9999): loss=0.0513592557367592\n",
      "Gradient Descent(244/9999): loss=0.05132939638091219\n",
      "Gradient Descent(245/9999): loss=0.05129995070216445\n",
      "Gradient Descent(246/9999): loss=0.05127091060706682\n",
      "Gradient Descent(247/9999): loss=0.05124226816853456\n",
      "Gradient Descent(248/9999): loss=0.05121401562231816\n",
      "Gradient Descent(249/9999): loss=0.05118614536355282\n",
      "Gradient Descent(250/9999): loss=0.05115864994338442\n",
      "Gradient Descent(251/9999): loss=0.05113152206567049\n",
      "Gradient Descent(252/9999): loss=0.051104754583754304\n",
      "Gradient Descent(253/9999): loss=0.05107834049731009\n",
      "Gradient Descent(254/9999): loss=0.05105227294925824\n",
      "Gradient Descent(255/9999): loss=0.051026545222748146\n",
      "Gradient Descent(256/9999): loss=0.051001150738207726\n",
      "Gradient Descent(257/9999): loss=0.05097608305045759\n",
      "Gradient Descent(258/9999): loss=0.0509513358458886\n",
      "Gradient Descent(259/9999): loss=0.05092690293970132\n",
      "Gradient Descent(260/9999): loss=0.05090277827320568\n",
      "Gradient Descent(261/9999): loss=0.05087895591117979\n",
      "Gradient Descent(262/9999): loss=0.05085543003928618\n",
      "Gradient Descent(263/9999): loss=0.05083219496154441\n",
      "Gradient Descent(264/9999): loss=0.05080924509785876\n",
      "Gradient Descent(265/9999): loss=0.05078657498159924\n",
      "Gradient Descent(266/9999): loss=0.050764179257235344\n",
      "Gradient Descent(267/9999): loss=0.05074205267802115\n",
      "Gradient Descent(268/9999): loss=0.05072019010373\n",
      "Gradient Descent(269/9999): loss=0.05069858649843862\n",
      "Gradient Descent(270/9999): loss=0.050677236928358696\n",
      "Gradient Descent(271/9999): loss=0.050656136559715244\n",
      "Gradient Descent(272/9999): loss=0.050635280656670785\n",
      "Gradient Descent(273/9999): loss=0.050614664579293776\n",
      "Gradient Descent(274/9999): loss=0.05059428378157099\n",
      "Gradient Descent(275/9999): loss=0.050574133809462095\n",
      "Gradient Descent(276/9999): loss=0.050554210298996434\n",
      "Gradient Descent(277/9999): loss=0.05053450897440985\n",
      "Gradient Descent(278/9999): loss=0.05051502564632179\n",
      "Gradient Descent(279/9999): loss=0.05049575620995102\n",
      "Gradient Descent(280/9999): loss=0.05047669664336944\n",
      "Gradient Descent(281/9999): loss=0.05045784300579312\n",
      "Gradient Descent(282/9999): loss=0.05043919143590956\n",
      "Gradient Descent(283/9999): loss=0.050420738150240466\n",
      "Gradient Descent(284/9999): loss=0.05040247944153931\n",
      "Gradient Descent(285/9999): loss=0.050384411677222865\n",
      "Gradient Descent(286/9999): loss=0.05036653129783583\n",
      "Gradient Descent(287/9999): loss=0.05034883481554806\n",
      "Gradient Descent(288/9999): loss=0.050331318812683386\n",
      "Gradient Descent(289/9999): loss=0.05031397994027954\n",
      "Gradient Descent(290/9999): loss=0.05029681491667855\n",
      "Gradient Descent(291/9999): loss=0.05027982052614659\n",
      "Gradient Descent(292/9999): loss=0.05026299361752307\n",
      "Gradient Descent(293/9999): loss=0.05024633110289806\n",
      "Gradient Descent(294/9999): loss=0.0502298299563174\n",
      "Gradient Descent(295/9999): loss=0.05021348721251515\n",
      "Gradient Descent(296/9999): loss=0.050197299965672404\n",
      "Gradient Descent(297/9999): loss=0.05018126536820228\n",
      "Gradient Descent(298/9999): loss=0.05016538062956025\n",
      "Gradient Descent(299/9999): loss=0.05014964301507934\n",
      "Gradient Descent(300/9999): loss=0.050134049844829685\n",
      "Gradient Descent(301/9999): loss=0.050118598492501824\n",
      "Gradient Descent(302/9999): loss=0.050103286384313474\n",
      "Gradient Descent(303/9999): loss=0.05008811099793875\n",
      "Gradient Descent(304/9999): loss=0.05007306986146005\n",
      "Gradient Descent(305/9999): loss=0.05005816055234151\n",
      "Gradient Descent(306/9999): loss=0.050043380696423806\n",
      "Gradient Descent(307/9999): loss=0.05002872796694002\n",
      "Gradient Descent(308/9999): loss=0.05001420008355167\n",
      "Gradient Descent(309/9999): loss=0.049999794811405034\n",
      "Gradient Descent(310/9999): loss=0.0499855099602067\n",
      "Gradient Descent(311/9999): loss=0.04997134338331859\n",
      "Gradient Descent(312/9999): loss=0.04995729297687149\n",
      "Gradient Descent(313/9999): loss=0.04994335667889697\n",
      "Gradient Descent(314/9999): loss=0.04992953246847737\n",
      "Gradient Descent(315/9999): loss=0.04991581836491316\n",
      "Gradient Descent(316/9999): loss=0.049902212426907594\n",
      "Gradient Descent(317/9999): loss=0.049888712751768184\n",
      "Gradient Descent(318/9999): loss=0.04987531747462457\n",
      "Gradient Descent(319/9999): loss=0.04986202476766262\n",
      "Gradient Descent(320/9999): loss=0.04984883283937416\n",
      "Gradient Descent(321/9999): loss=0.04983573993382236\n",
      "Gradient Descent(322/9999): loss=0.049822744329921856\n",
      "Gradient Descent(323/9999): loss=0.04980984434073407\n",
      "Gradient Descent(324/9999): loss=0.04979703831277683\n",
      "Gradient Descent(325/9999): loss=0.049784324625348146\n",
      "Gradient Descent(326/9999): loss=0.04977170168986384\n",
      "Gradient Descent(327/9999): loss=0.04975916794920889\n",
      "Gradient Descent(328/9999): loss=0.04974672187710188\n",
      "Gradient Descent(329/9999): loss=0.04973436197747274\n",
      "Gradient Descent(330/9999): loss=0.04972208678385291\n",
      "Gradient Descent(331/9999): loss=0.04970989485877826\n",
      "Gradient Descent(332/9999): loss=0.049697784793204\n",
      "Gradient Descent(333/9999): loss=0.049685755205931804\n",
      "Gradient Descent(334/9999): loss=0.04967380474304831\n",
      "Gradient Descent(335/9999): loss=0.04966193207737549\n",
      "Gradient Descent(336/9999): loss=0.04965013590793181\n",
      "Gradient Descent(337/9999): loss=0.04963841495940484\n",
      "Gradient Descent(338/9999): loss=0.049626767981634144\n",
      "Gradient Descent(339/9999): loss=0.04961519374910514\n",
      "Gradient Descent(340/9999): loss=0.049603691060453044\n",
      "Gradient Descent(341/9999): loss=0.04959225873797691\n",
      "Gradient Descent(342/9999): loss=0.04958089562716372\n",
      "Gradient Descent(343/9999): loss=0.04956960059622196\n",
      "Gradient Descent(344/9999): loss=0.04955837253562489\n",
      "Gradient Descent(345/9999): loss=0.049547210357662956\n",
      "Gradient Descent(346/9999): loss=0.04953611299600533\n",
      "Gradient Descent(347/9999): loss=0.049525079405270435\n",
      "Gradient Descent(348/9999): loss=0.04951410856060509\n",
      "Gradient Descent(349/9999): loss=0.04950319945727229\n",
      "Gradient Descent(350/9999): loss=0.04949235111024728\n",
      "Gradient Descent(351/9999): loss=0.0494815625538219\n",
      "Gradient Descent(352/9999): loss=0.04947083284121687\n",
      "Gradient Descent(353/9999): loss=0.04946016104420209\n",
      "Gradient Descent(354/9999): loss=0.049449546252724325\n",
      "Gradient Descent(355/9999): loss=0.04943898757454287\n",
      "Gradient Descent(356/9999): loss=0.049428484134872226\n",
      "Gradient Descent(357/9999): loss=0.049418035076032193\n",
      "Gradient Descent(358/9999): loss=0.04940763955710507\n",
      "Gradient Descent(359/9999): loss=0.049397296753599565\n",
      "Gradient Descent(360/9999): loss=0.049387005857121816\n",
      "Gradient Descent(361/9999): loss=0.049376766075052876\n",
      "Gradient Descent(362/9999): loss=0.04936657663023278\n",
      "Gradient Descent(363/9999): loss=0.04935643676065087\n",
      "Gradient Descent(364/9999): loss=0.0493463457191427\n",
      "Gradient Descent(365/9999): loss=0.049336302773092644\n",
      "Gradient Descent(366/9999): loss=0.0493263072041428\n",
      "Gradient Descent(367/9999): loss=0.04931635830790776\n",
      "Gradient Descent(368/9999): loss=0.049306455393694916\n",
      "Gradient Descent(369/9999): loss=0.0492965977842307\n",
      "Gradient Descent(370/9999): loss=0.049286784815392164\n",
      "Gradient Descent(371/9999): loss=0.04927701583594397\n",
      "Gradient Descent(372/9999): loss=0.04926729020728075\n",
      "Gradient Descent(373/9999): loss=0.049257607303174744\n",
      "Gradient Descent(374/9999): loss=0.04924796650952832\n",
      "Gradient Descent(375/9999): loss=0.049238367224131566\n",
      "Gradient Descent(376/9999): loss=0.04922880885642486\n",
      "Gradient Descent(377/9999): loss=0.04921929082726608\n",
      "Gradient Descent(378/9999): loss=0.04920981256870264\n",
      "Gradient Descent(379/9999): loss=0.04920037352374797\n",
      "Gradient Descent(380/9999): loss=0.0491909731461626\n",
      "Gradient Descent(381/9999): loss=0.04918161090023962\n",
      "Gradient Descent(382/9999): loss=0.049172286260594465\n",
      "Gradient Descent(383/9999): loss=0.04916299871195881\n",
      "Gradient Descent(384/9999): loss=0.049153747748978925\n",
      "Gradient Descent(385/9999): loss=0.049144532876017645\n",
      "Gradient Descent(386/9999): loss=0.04913535360696072\n",
      "Gradient Descent(387/9999): loss=0.04912620946502681\n",
      "Gradient Descent(388/9999): loss=0.049117099982581396\n",
      "Gradient Descent(389/9999): loss=0.04910802470095439\n",
      "Gradient Descent(390/9999): loss=0.04909898317026148\n",
      "Gradient Descent(391/9999): loss=0.04908997494922895\n",
      "Gradient Descent(392/9999): loss=0.04908099960502212\n",
      "Gradient Descent(393/9999): loss=0.04907205671307716\n",
      "Gradient Descent(394/9999): loss=0.049063145856936385\n",
      "Gradient Descent(395/9999): loss=0.04905426662808667\n",
      "Gradient Descent(396/9999): loss=0.04904541862580128\n",
      "Gradient Descent(397/9999): loss=0.04903660145698483\n",
      "Gradient Descent(398/9999): loss=0.0490278147360213\n",
      "Gradient Descent(399/9999): loss=0.04901905808462514\n",
      "Gradient Descent(400/9999): loss=0.04901033113169537\n",
      "Gradient Descent(401/9999): loss=0.04900163351317256\n",
      "Gradient Descent(402/9999): loss=0.04899296487189883\n",
      "Gradient Descent(403/9999): loss=0.0489843248574803\n",
      "Gradient Descent(404/9999): loss=0.048975713126152766\n",
      "Gradient Descent(405/9999): loss=0.048967129340649757\n",
      "Gradient Descent(406/9999): loss=0.048958573170073204\n",
      "Gradient Descent(407/9999): loss=0.04895004428976696\n",
      "Gradient Descent(408/9999): loss=0.048941542381192665\n",
      "Gradient Descent(409/9999): loss=0.04893306713180805\n",
      "Gradient Descent(410/9999): loss=0.048924618234947835\n",
      "Gradient Descent(411/9999): loss=0.048916195389707\n",
      "Gradient Descent(412/9999): loss=0.048907798300826165\n",
      "Gradient Descent(413/9999): loss=0.04889942667857956\n",
      "Gradient Descent(414/9999): loss=0.04889108023866513\n",
      "Gradient Descent(415/9999): loss=0.04888275870209664\n",
      "Gradient Descent(416/9999): loss=0.04887446179509837\n",
      "Gradient Descent(417/9999): loss=0.04886618924900144\n",
      "Gradient Descent(418/9999): loss=0.0488579408001426\n",
      "Gradient Descent(419/9999): loss=0.048849716189764726\n",
      "Gradient Descent(420/9999): loss=0.048841515163919616\n",
      "Gradient Descent(421/9999): loss=0.04883333747337242\n",
      "Gradient Descent(422/9999): loss=0.048825182873508295\n",
      "Gradient Descent(423/9999): loss=0.048817051124240576\n",
      "Gradient Descent(424/9999): loss=0.04880894198992118\n",
      "Gradient Descent(425/9999): loss=0.048800855239252386\n",
      "Gradient Descent(426/9999): loss=0.04879279064520073\n",
      "Gradient Descent(427/9999): loss=0.04878474798491246\n",
      "Gradient Descent(428/9999): loss=0.04877672703963061\n",
      "Gradient Descent(429/9999): loss=0.04876872759461392\n",
      "Gradient Descent(430/9999): loss=0.04876074943905719\n",
      "Gradient Descent(431/9999): loss=0.0487527923660134\n",
      "Gradient Descent(432/9999): loss=0.04874485617231719\n",
      "Gradient Descent(433/9999): loss=0.04873694065851011\n",
      "Gradient Descent(434/9999): loss=0.048729045628767016\n",
      "Gradient Descent(435/9999): loss=0.048721170890824415\n",
      "Gradient Descent(436/9999): loss=0.04871331625590971\n",
      "Gradient Descent(437/9999): loss=0.048705481538672366\n",
      "Gradient Descent(438/9999): loss=0.04869766655711598\n",
      "Gradient Descent(439/9999): loss=0.04868987113253209\n",
      "Gradient Descent(440/9999): loss=0.04868209508943507\n",
      "Gradient Descent(441/9999): loss=0.048674338255498446\n",
      "Gradient Descent(442/9999): loss=0.048666600461492425\n",
      "Gradient Descent(443/9999): loss=0.04865888154122262\n",
      "Gradient Descent(444/9999): loss=0.04865118133147019\n",
      "Gradient Descent(445/9999): loss=0.04864349967193294\n",
      "Gradient Descent(446/9999): loss=0.0486358364051678\n",
      "Gradient Descent(447/9999): loss=0.048628191376534194\n",
      "Gradient Descent(448/9999): loss=0.04862056443413891\n",
      "Gradient Descent(449/9999): loss=0.04861295542878172\n",
      "Gradient Descent(450/9999): loss=0.0486053642139022\n",
      "Gradient Descent(451/9999): loss=0.04859779064552781\n",
      "Gradient Descent(452/9999): loss=0.04859023458222265\n",
      "Gradient Descent(453/9999): loss=0.0485826958850375\n",
      "Gradient Descent(454/9999): loss=0.04857517441746079\n",
      "Gradient Descent(455/9999): loss=0.04856767004537051\n",
      "Gradient Descent(456/9999): loss=0.04856018263698713\n",
      "Gradient Descent(457/9999): loss=0.048552712062827436\n",
      "Gradient Descent(458/9999): loss=0.04854525819565926\n",
      "Gradient Descent(459/9999): loss=0.048537820910457115\n",
      "Gradient Descent(460/9999): loss=0.04853040008435882\n",
      "Gradient Descent(461/9999): loss=0.04852299559662281\n",
      "Gradient Descent(462/9999): loss=0.04851560732858655\n",
      "Gradient Descent(463/9999): loss=0.04850823516362537\n",
      "Gradient Descent(464/9999): loss=0.04850087898711263\n",
      "Gradient Descent(465/9999): loss=0.04849353868638023\n",
      "Gradient Descent(466/9999): loss=0.04848621415068026\n",
      "Gradient Descent(467/9999): loss=0.048478905271147045\n",
      "Gradient Descent(468/9999): loss=0.0484716119407604\n",
      "Gradient Descent(469/9999): loss=0.04846433405430916\n",
      "Gradient Descent(470/9999): loss=0.048457071508355744\n",
      "Gradient Descent(471/9999): loss=0.04844982420120135\n",
      "Gradient Descent(472/9999): loss=0.048442592032851735\n",
      "Gradient Descent(473/9999): loss=0.04843537490498386\n",
      "Gradient Descent(474/9999): loss=0.048428172720913015\n",
      "Gradient Descent(475/9999): loss=0.048420985385560694\n",
      "Gradient Descent(476/9999): loss=0.048413812805423256\n",
      "Gradient Descent(477/9999): loss=0.04840665488854077\n",
      "Gradient Descent(478/9999): loss=0.048399511544467026\n",
      "Gradient Descent(479/9999): loss=0.048392382684239746\n",
      "Gradient Descent(480/9999): loss=0.04838526822035158\n",
      "Gradient Descent(481/9999): loss=0.04837816806672156\n",
      "Gradient Descent(482/9999): loss=0.0483710821386673\n",
      "Gradient Descent(483/9999): loss=0.04836401035287748\n",
      "Gradient Descent(484/9999): loss=0.04835695262738518\n",
      "Gradient Descent(485/9999): loss=0.048349908881541524\n",
      "Gradient Descent(486/9999): loss=0.04834287903598992\n",
      "Gradient Descent(487/9999): loss=0.04833586301264086\n",
      "Gradient Descent(488/9999): loss=0.048328860734647094\n",
      "Gradient Descent(489/9999): loss=0.0483218721263795\n",
      "Gradient Descent(490/9999): loss=0.04831489711340319\n",
      "Gradient Descent(491/9999): loss=0.048307935622454314\n",
      "Gradient Descent(492/9999): loss=0.0483009875814172\n",
      "Gradient Descent(493/9999): loss=0.048294052919301944\n",
      "Gradient Descent(494/9999): loss=0.04828713156622247\n",
      "Gradient Descent(495/9999): loss=0.04828022345337514\n",
      "Gradient Descent(496/9999): loss=0.04827332851301752\n",
      "Gradient Descent(497/9999): loss=0.04826644667844791\n",
      "Gradient Descent(498/9999): loss=0.04825957788398497\n",
      "Gradient Descent(499/9999): loss=0.04825272206494796\n",
      "Gradient Descent(500/9999): loss=0.04824587915763721\n",
      "Gradient Descent(501/9999): loss=0.04823904909931521\n",
      "Gradient Descent(502/9999): loss=0.04823223182818777\n",
      "Gradient Descent(503/9999): loss=0.04822542728338587\n",
      "Gradient Descent(504/9999): loss=0.04821863540494756\n",
      "Gradient Descent(505/9999): loss=0.048211856133800565\n",
      "Gradient Descent(506/9999): loss=0.048205089411744846\n",
      "Gradient Descent(507/9999): loss=0.04819833518143587\n",
      "Gradient Descent(508/9999): loss=0.04819159338636797\n",
      "Gradient Descent(509/9999): loss=0.04818486397085814\n",
      "Gradient Descent(510/9999): loss=0.04817814688003015\n",
      "Gradient Descent(511/9999): loss=0.04817144205979889\n",
      "Gradient Descent(512/9999): loss=0.048164749456855185\n",
      "Gradient Descent(513/9999): loss=0.04815806901865081\n",
      "Gradient Descent(514/9999): loss=0.048151400693383646\n",
      "Gradient Descent(515/9999): loss=0.048144744429983566\n",
      "Gradient Descent(516/9999): loss=0.048138100178098144\n",
      "Gradient Descent(517/9999): loss=0.04813146788807884\n",
      "Gradient Descent(518/9999): loss=0.048124847510967524\n",
      "Gradient Descent(519/9999): loss=0.04811823899848324\n",
      "Gradient Descent(520/9999): loss=0.048111642303009045\n",
      "Gradient Descent(521/9999): loss=0.048105057377579384\n",
      "Gradient Descent(522/9999): loss=0.048098484175867505\n",
      "Gradient Descent(523/9999): loss=0.048091922652173304\n",
      "Gradient Descent(524/9999): loss=0.04808537276141117\n",
      "Gradient Descent(525/9999): loss=0.0480788344590984\n",
      "Gradient Descent(526/9999): loss=0.04807230770134349\n",
      "Gradient Descent(527/9999): loss=0.04806579244483491\n",
      "Gradient Descent(528/9999): loss=0.04805928864683003\n",
      "Gradient Descent(529/9999): loss=0.048052796265144214\n",
      "Gradient Descent(530/9999): loss=0.048046315258140224\n",
      "Gradient Descent(531/9999): loss=0.0480398455847177\n",
      "Gradient Descent(532/9999): loss=0.048033387204303016\n",
      "Gradient Descent(533/9999): loss=0.04802694007683922\n",
      "Gradient Descent(534/9999): loss=0.04802050416277618\n",
      "Gradient Descent(535/9999): loss=0.04801407942306105\n",
      "Gradient Descent(536/9999): loss=0.04800766581912864\n",
      "Gradient Descent(537/9999): loss=0.04800126331289236\n",
      "Gradient Descent(538/9999): loss=0.04799487186673503\n",
      "Gradient Descent(539/9999): loss=0.04798849144350004\n",
      "Gradient Descent(540/9999): loss=0.04798212200648262\n",
      "Gradient Descent(541/9999): loss=0.047975763519421274\n",
      "Gradient Descent(542/9999): loss=0.047969415946489465\n",
      "Gradient Descent(543/9999): loss=0.047963079252287356\n",
      "Gradient Descent(544/9999): loss=0.04795675340183378\n",
      "Gradient Descent(545/9999): loss=0.04795043836055836\n",
      "Gradient Descent(546/9999): loss=0.047944134094293815\n",
      "Gradient Descent(547/9999): loss=0.047937840569268315\n",
      "Gradient Descent(548/9999): loss=0.04793155775209809\n",
      "Gradient Descent(549/9999): loss=0.04792528560978017\n",
      "Gradient Descent(550/9999): loss=0.04791902410968525\n",
      "Gradient Descent(551/9999): loss=0.047912773219550654\n",
      "Gradient Descent(552/9999): loss=0.04790653290747352\n",
      "Gradient Descent(553/9999): loss=0.0479003031419041\n",
      "Gradient Descent(554/9999): loss=0.0478940838916391\n",
      "Gradient Descent(555/9999): loss=0.04788787512581535\n",
      "Gradient Descent(556/9999): loss=0.04788167681390331\n",
      "Gradient Descent(557/9999): loss=0.047875488925701024\n",
      "Gradient Descent(558/9999): loss=0.04786931143132796\n",
      "Gradient Descent(559/9999): loss=0.04786314430121905\n",
      "Gradient Descent(560/9999): loss=0.0478569875061189\n",
      "Gradient Descent(561/9999): loss=0.047850841017076026\n",
      "Gradient Descent(562/9999): loss=0.04784470480543724\n",
      "Gradient Descent(563/9999): loss=0.04783857884284216\n",
      "Gradient Descent(564/9999): loss=0.04783246310121785\n",
      "Gradient Descent(565/9999): loss=0.047826357552773445\n",
      "Gradient Descent(566/9999): loss=0.04782026216999506\n",
      "Gradient Descent(567/9999): loss=0.047814176925640724\n",
      "Gradient Descent(568/9999): loss=0.04780810179273529\n",
      "Gradient Descent(569/9999): loss=0.04780203674456563\n",
      "Gradient Descent(570/9999): loss=0.04779598175467591\n",
      "Gradient Descent(571/9999): loss=0.0477899367968628\n",
      "Gradient Descent(572/9999): loss=0.04778390184517087\n",
      "Gradient Descent(573/9999): loss=0.04777787687388822\n",
      "Gradient Descent(574/9999): loss=0.047771861857541946\n",
      "Gradient Descent(575/9999): loss=0.04776585677089378\n",
      "Gradient Descent(576/9999): loss=0.04775986158893601\n",
      "Gradient Descent(577/9999): loss=0.04775387628688714\n",
      "Gradient Descent(578/9999): loss=0.04774790084018792\n",
      "Gradient Descent(579/9999): loss=0.04774193522449733\n",
      "Gradient Descent(580/9999): loss=0.047735979415688676\n",
      "Gradient Descent(581/9999): loss=0.04773003338984567\n",
      "Gradient Descent(582/9999): loss=0.047724097123258744\n",
      "Gradient Descent(583/9999): loss=0.047718170592421366\n",
      "Gradient Descent(584/9999): loss=0.04771225377402635\n",
      "Gradient Descent(585/9999): loss=0.04770634664496242\n",
      "Gradient Descent(586/9999): loss=0.04770044918231059\n",
      "Gradient Descent(587/9999): loss=0.04769456136334087\n",
      "Gradient Descent(588/9999): loss=0.04768868316550894\n",
      "Gradient Descent(589/9999): loss=0.04768281456645271\n",
      "Gradient Descent(590/9999): loss=0.04767695554398931\n",
      "Gradient Descent(591/9999): loss=0.04767110607611187\n",
      "Gradient Descent(592/9999): loss=0.04766526614098636\n",
      "Gradient Descent(593/9999): loss=0.04765943571694862\n",
      "Gradient Descent(594/9999): loss=0.047653614782501484\n",
      "Gradient Descent(595/9999): loss=0.04764780331631173\n",
      "Gradient Descent(596/9999): loss=0.047642001297207325\n",
      "Gradient Descent(597/9999): loss=0.04763620870417455\n",
      "Gradient Descent(598/9999): loss=0.047630425516355386\n",
      "Gradient Descent(599/9999): loss=0.04762465171304469\n",
      "Gradient Descent(600/9999): loss=0.047618887273687675\n",
      "Gradient Descent(601/9999): loss=0.04761313217787722\n",
      "Gradient Descent(602/9999): loss=0.04760738640535149\n",
      "Gradient Descent(603/9999): loss=0.047601649935991336\n",
      "Gradient Descent(604/9999): loss=0.04759592274981782\n",
      "Gradient Descent(605/9999): loss=0.04759020482699006\n",
      "Gradient Descent(606/9999): loss=0.047584496147802635\n",
      "Gradient Descent(607/9999): loss=0.047578796692683466\n",
      "Gradient Descent(608/9999): loss=0.04757310644219143\n",
      "Gradient Descent(609/9999): loss=0.04756742537701431\n",
      "Gradient Descent(610/9999): loss=0.047561753477966554\n",
      "Gradient Descent(611/9999): loss=0.04755609072598716\n",
      "Gradient Descent(612/9999): loss=0.047550437102137584\n",
      "Gradient Descent(613/9999): loss=0.0475447925875998\n",
      "Gradient Descent(614/9999): loss=0.04753915716367414\n",
      "Gradient Descent(615/9999): loss=0.047533530811777566\n",
      "Gradient Descent(616/9999): loss=0.0475279135134415\n",
      "Gradient Descent(617/9999): loss=0.04752230525031016\n",
      "Gradient Descent(618/9999): loss=0.04751670600413859\n",
      "Gradient Descent(619/9999): loss=0.04751111575679088\n",
      "Gradient Descent(620/9999): loss=0.04750553449023846\n",
      "Gradient Descent(621/9999): loss=0.047499962186558255\n",
      "Gradient Descent(622/9999): loss=0.04749439882793108\n",
      "Gradient Descent(623/9999): loss=0.04748884439663985\n",
      "Gradient Descent(624/9999): loss=0.04748329887506815\n",
      "Gradient Descent(625/9999): loss=0.04747776224569841\n",
      "Gradient Descent(626/9999): loss=0.047472234491110446\n",
      "Gradient Descent(627/9999): loss=0.04746671559397991\n",
      "Gradient Descent(628/9999): loss=0.04746120553707674\n",
      "Gradient Descent(629/9999): loss=0.04745570430326371\n",
      "Gradient Descent(630/9999): loss=0.047450211875495\n",
      "Gradient Descent(631/9999): loss=0.04744472823681469\n",
      "Gradient Descent(632/9999): loss=0.04743925337035544\n",
      "Gradient Descent(633/9999): loss=0.04743378725933703\n",
      "Gradient Descent(634/9999): loss=0.04742832988706515\n",
      "Gradient Descent(635/9999): loss=0.04742288123692999\n",
      "Gradient Descent(636/9999): loss=0.04741744129240491\n",
      "Gradient Descent(637/9999): loss=0.047412010037045295\n",
      "Gradient Descent(638/9999): loss=0.047406587454487184\n",
      "Gradient Descent(639/9999): loss=0.047401173528446125\n",
      "Gradient Descent(640/9999): loss=0.047395768242716005\n",
      "Gradient Descent(641/9999): loss=0.04739037158116778\n",
      "Gradient Descent(642/9999): loss=0.04738498352774843\n",
      "Gradient Descent(643/9999): loss=0.047379604066479765\n",
      "Gradient Descent(644/9999): loss=0.047374233181457344\n",
      "Gradient Descent(645/9999): loss=0.04736887085684942\n",
      "Gradient Descent(646/9999): loss=0.04736351707689576\n",
      "Gradient Descent(647/9999): loss=0.04735817182590681\n",
      "Gradient Descent(648/9999): loss=0.04735283508826245\n",
      "Gradient Descent(649/9999): loss=0.04734750684841113\n",
      "Gradient Descent(650/9999): loss=0.047342187090868876\n",
      "Gradient Descent(651/9999): loss=0.047336875800218274\n",
      "Gradient Descent(652/9999): loss=0.04733157296110754\n",
      "Gradient Descent(653/9999): loss=0.04732627855824966\n",
      "Gradient Descent(654/9999): loss=0.04732099257642136\n",
      "Gradient Descent(655/9999): loss=0.0473157150004623\n",
      "Gradient Descent(656/9999): loss=0.047310445815274195\n",
      "Gradient Descent(657/9999): loss=0.04730518500581997\n",
      "Gradient Descent(658/9999): loss=0.047299932557122795\n",
      "Gradient Descent(659/9999): loss=0.04729468845426544\n",
      "Gradient Descent(660/9999): loss=0.04728945268238939\n",
      "Gradient Descent(661/9999): loss=0.04728422522669393\n",
      "Gradient Descent(662/9999): loss=0.04727900607243561\n",
      "Gradient Descent(663/9999): loss=0.04727379520492727\n",
      "Gradient Descent(664/9999): loss=0.04726859260953743\n",
      "Gradient Descent(665/9999): loss=0.04726339827168942\n",
      "Gradient Descent(666/9999): loss=0.04725821217686079\n",
      "Gradient Descent(667/9999): loss=0.047253034310582503\n",
      "Gradient Descent(668/9999): loss=0.047247864658438374\n",
      "Gradient Descent(669/9999): loss=0.04724270320606419\n",
      "Gradient Descent(670/9999): loss=0.04723754993914725\n",
      "Gradient Descent(671/9999): loss=0.04723240484342554\n",
      "Gradient Descent(672/9999): loss=0.04722726790468723\n",
      "Gradient Descent(673/9999): loss=0.047222139108769934\n",
      "Gradient Descent(674/9999): loss=0.04721701844156015\n",
      "Gradient Descent(675/9999): loss=0.047211905888992656\n",
      "Gradient Descent(676/9999): loss=0.047206801437049896\n",
      "Gradient Descent(677/9999): loss=0.047201705071761364\n",
      "Gradient Descent(678/9999): loss=0.047196616779203164\n",
      "Gradient Descent(679/9999): loss=0.047191536545497216\n",
      "Gradient Descent(680/9999): loss=0.047186464356810974\n",
      "Gradient Descent(681/9999): loss=0.04718140019935666\n",
      "Gradient Descent(682/9999): loss=0.04717634405939093\n",
      "Gradient Descent(683/9999): loss=0.047171295923214104\n",
      "Gradient Descent(684/9999): loss=0.04716625577716991\n",
      "Gradient Descent(685/9999): loss=0.04716122360764486\n",
      "Gradient Descent(686/9999): loss=0.047156199401067786\n",
      "Gradient Descent(687/9999): loss=0.04715118314390925\n",
      "Gradient Descent(688/9999): loss=0.04714617482268131\n",
      "Gradient Descent(689/9999): loss=0.04714117442393677\n",
      "Gradient Descent(690/9999): loss=0.04713618193426897\n",
      "Gradient Descent(691/9999): loss=0.04713119734031118\n",
      "Gradient Descent(692/9999): loss=0.047126220628736254\n",
      "Gradient Descent(693/9999): loss=0.047121251786256105\n",
      "Gradient Descent(694/9999): loss=0.047116290799621374\n",
      "Gradient Descent(695/9999): loss=0.04711133765562094\n",
      "Gradient Descent(696/9999): loss=0.04710639234108162\n",
      "Gradient Descent(697/9999): loss=0.047101454842867624\n",
      "Gradient Descent(698/9999): loss=0.04709652514788031\n",
      "Gradient Descent(699/9999): loss=0.04709160324305767\n",
      "Gradient Descent(700/9999): loss=0.047086689115374024\n",
      "Gradient Descent(701/9999): loss=0.04708178275183963\n",
      "Gradient Descent(702/9999): loss=0.047076884139500355\n",
      "Gradient Descent(703/9999): loss=0.04707199326543722\n",
      "Gradient Descent(704/9999): loss=0.047067110116766245\n",
      "Gradient Descent(705/9999): loss=0.047062234680637784\n",
      "Gradient Descent(706/9999): loss=0.04705736694423655\n",
      "Gradient Descent(707/9999): loss=0.04705250689478103\n",
      "Gradient Descent(708/9999): loss=0.04704765451952324\n",
      "Gradient Descent(709/9999): loss=0.04704280980574842\n",
      "Gradient Descent(710/9999): loss=0.04703797274077468\n",
      "Gradient Descent(711/9999): loss=0.04703314331195279\n",
      "Gradient Descent(712/9999): loss=0.04702832150666572\n",
      "Gradient Descent(713/9999): loss=0.04702350731232845\n",
      "Gradient Descent(714/9999): loss=0.04701870071638765\n",
      "Gradient Descent(715/9999): loss=0.04701390170632146\n",
      "Gradient Descent(716/9999): loss=0.04700911026963902\n",
      "Gradient Descent(717/9999): loss=0.04700432639388045\n",
      "Gradient Descent(718/9999): loss=0.04699955006661636\n",
      "Gradient Descent(719/9999): loss=0.046994781275447685\n",
      "Gradient Descent(720/9999): loss=0.04699002000800546\n",
      "Gradient Descent(721/9999): loss=0.04698526625195044\n",
      "Gradient Descent(722/9999): loss=0.046980519994972963\n",
      "Gradient Descent(723/9999): loss=0.04697578122479266\n",
      "Gradient Descent(724/9999): loss=0.04697104992915818\n",
      "Gradient Descent(725/9999): loss=0.046966326095846986\n",
      "Gradient Descent(726/9999): loss=0.04696160971266512\n",
      "Gradient Descent(727/9999): loss=0.04695690076744699\n",
      "Gradient Descent(728/9999): loss=0.04695219924805508\n",
      "Gradient Descent(729/9999): loss=0.04694750514237978\n",
      "Gradient Descent(730/9999): loss=0.04694281843833916\n",
      "Gradient Descent(731/9999): loss=0.04693813912387872\n",
      "Gradient Descent(732/9999): loss=0.046933467186971216\n",
      "Gradient Descent(733/9999): loss=0.04692880261561647\n",
      "Gradient Descent(734/9999): loss=0.04692414539784111\n",
      "Gradient Descent(735/9999): loss=0.04691949552169839\n",
      "Gradient Descent(736/9999): loss=0.046914852975268005\n",
      "Gradient Descent(737/9999): loss=0.046910217746655915\n",
      "Gradient Descent(738/9999): loss=0.0469055898239941\n",
      "Gradient Descent(739/9999): loss=0.04690096919544042\n",
      "Gradient Descent(740/9999): loss=0.0468963558491784\n",
      "Gradient Descent(741/9999): loss=0.04689174977341709\n",
      "Gradient Descent(742/9999): loss=0.04688715095639082\n",
      "Gradient Descent(743/9999): loss=0.04688255938635911\n",
      "Gradient Descent(744/9999): loss=0.04687797505160641\n",
      "Gradient Descent(745/9999): loss=0.04687339794044204\n",
      "Gradient Descent(746/9999): loss=0.04686882804119987\n",
      "Gradient Descent(747/9999): loss=0.04686426534223835\n",
      "Gradient Descent(748/9999): loss=0.046859709831940134\n",
      "Gradient Descent(749/9999): loss=0.04685516149871213\n",
      "Gradient Descent(750/9999): loss=0.046850620330985186\n",
      "Gradient Descent(751/9999): loss=0.046846086317214056\n",
      "Gradient Descent(752/9999): loss=0.0468415594458771\n",
      "Gradient Descent(753/9999): loss=0.04683703970547635\n",
      "Gradient Descent(754/9999): loss=0.046832527084537154\n",
      "Gradient Descent(755/9999): loss=0.04682802157160818\n",
      "Gradient Descent(756/9999): loss=0.046823523155261164\n",
      "Gradient Descent(757/9999): loss=0.046819031824090855\n",
      "Gradient Descent(758/9999): loss=0.04681454756671489\n",
      "Gradient Descent(759/9999): loss=0.046810070371773545\n",
      "Gradient Descent(760/9999): loss=0.04680560022792975\n",
      "Gradient Descent(761/9999): loss=0.04680113712386884\n",
      "Gradient Descent(762/9999): loss=0.046796681048298525\n",
      "Gradient Descent(763/9999): loss=0.046792231989948654\n",
      "Gradient Descent(764/9999): loss=0.046787789937571225\n",
      "Gradient Descent(765/9999): loss=0.04678335487994012\n",
      "Gradient Descent(766/9999): loss=0.0467789268058511\n",
      "Gradient Descent(767/9999): loss=0.04677450570412168\n",
      "Gradient Descent(768/9999): loss=0.04677009156359092\n",
      "Gradient Descent(769/9999): loss=0.046765684373119364\n",
      "Gradient Descent(770/9999): loss=0.04676128412158895\n",
      "Gradient Descent(771/9999): loss=0.04675689079790295\n",
      "Gradient Descent(772/9999): loss=0.04675250439098568\n",
      "Gradient Descent(773/9999): loss=0.04674812488978261\n",
      "Gradient Descent(774/9999): loss=0.04674375228326009\n",
      "Gradient Descent(775/9999): loss=0.04673938656040533\n",
      "Gradient Descent(776/9999): loss=0.04673502771022627\n",
      "Gradient Descent(777/9999): loss=0.04673067572175151\n",
      "Gradient Descent(778/9999): loss=0.0467263305840302\n",
      "Gradient Descent(779/9999): loss=0.04672199228613187\n",
      "Gradient Descent(780/9999): loss=0.04671766081714648\n",
      "Gradient Descent(781/9999): loss=0.04671333616618416\n",
      "Gradient Descent(782/9999): loss=0.04670901832237528\n",
      "Gradient Descent(783/9999): loss=0.04670470727487017\n",
      "Gradient Descent(784/9999): loss=0.04670040301283925\n",
      "Gradient Descent(785/9999): loss=0.04669610552547274\n",
      "Gradient Descent(786/9999): loss=0.04669181480198071\n",
      "Gradient Descent(787/9999): loss=0.0466875308315929\n",
      "Gradient Descent(788/9999): loss=0.04668325360355869\n",
      "Gradient Descent(789/9999): loss=0.046678983107147014\n",
      "Gradient Descent(790/9999): loss=0.046674719331646235\n",
      "Gradient Descent(791/9999): loss=0.04667046226636412\n",
      "Gradient Descent(792/9999): loss=0.04666621190062772\n",
      "Gradient Descent(793/9999): loss=0.04666196822378331\n",
      "Gradient Descent(794/9999): loss=0.046657731225196224\n",
      "Gradient Descent(795/9999): loss=0.046653500894251\n",
      "Gradient Descent(796/9999): loss=0.046649277220351025\n",
      "Gradient Descent(797/9999): loss=0.04664506019291866\n",
      "Gradient Descent(798/9999): loss=0.046640849801395066\n",
      "Gradient Descent(799/9999): loss=0.046636646035240176\n",
      "Gradient Descent(800/9999): loss=0.04663244888393264\n",
      "Gradient Descent(801/9999): loss=0.04662825833696965\n",
      "Gradient Descent(802/9999): loss=0.04662407438386702\n",
      "Gradient Descent(803/9999): loss=0.04661989701415901\n",
      "Gradient Descent(804/9999): loss=0.04661572621739823\n",
      "Gradient Descent(805/9999): loss=0.046611561983155705\n",
      "Gradient Descent(806/9999): loss=0.04660740430102072\n",
      "Gradient Descent(807/9999): loss=0.04660325316060072\n",
      "Gradient Descent(808/9999): loss=0.04659910855152134\n",
      "Gradient Descent(809/9999): loss=0.046594970463426255\n",
      "Gradient Descent(810/9999): loss=0.0465908388859772\n",
      "Gradient Descent(811/9999): loss=0.04658671380885376\n",
      "Gradient Descent(812/9999): loss=0.04658259522175352\n",
      "Gradient Descent(813/9999): loss=0.04657848311439186\n",
      "Gradient Descent(814/9999): loss=0.04657437747650186\n",
      "Gradient Descent(815/9999): loss=0.04657027829783442\n",
      "Gradient Descent(816/9999): loss=0.04656618556815799\n",
      "Gradient Descent(817/9999): loss=0.04656209927725862\n",
      "Gradient Descent(818/9999): loss=0.04655801941493995\n",
      "Gradient Descent(819/9999): loss=0.04655394597102304\n",
      "Gradient Descent(820/9999): loss=0.04654987893534643\n",
      "Gradient Descent(821/9999): loss=0.046545818297765926\n",
      "Gradient Descent(822/9999): loss=0.04654176404815476\n",
      "Gradient Descent(823/9999): loss=0.04653771617640328\n",
      "Gradient Descent(824/9999): loss=0.046533674672419194\n",
      "Gradient Descent(825/9999): loss=0.046529639526127174\n",
      "Gradient Descent(826/9999): loss=0.046525610727469184\n",
      "Gradient Descent(827/9999): loss=0.046521588266404096\n",
      "Gradient Descent(828/9999): loss=0.04651757213290779\n",
      "Gradient Descent(829/9999): loss=0.046513562316973116\n",
      "Gradient Descent(830/9999): loss=0.04650955880860981\n",
      "Gradient Descent(831/9999): loss=0.046505561597844476\n",
      "Gradient Descent(832/9999): loss=0.04650157067472046\n",
      "Gradient Descent(833/9999): loss=0.046497586029297856\n",
      "Gradient Descent(834/9999): loss=0.04649360765165349\n",
      "Gradient Descent(835/9999): loss=0.04648963553188083\n",
      "Gradient Descent(836/9999): loss=0.0464856696600899\n",
      "Gradient Descent(837/9999): loss=0.04648171002640734\n",
      "Gradient Descent(838/9999): loss=0.04647775662097623\n",
      "Gradient Descent(839/9999): loss=0.04647380943395621\n",
      "Gradient Descent(840/9999): loss=0.04646986845552325\n",
      "Gradient Descent(841/9999): loss=0.046465933675869736\n",
      "Gradient Descent(842/9999): loss=0.04646200508520436\n",
      "Gradient Descent(843/9999): loss=0.04645808267375212\n",
      "Gradient Descent(844/9999): loss=0.04645416643175425\n",
      "Gradient Descent(845/9999): loss=0.0464502563494682\n",
      "Gradient Descent(846/9999): loss=0.046446352417167526\n",
      "Gradient Descent(847/9999): loss=0.04644245462514195\n",
      "Gradient Descent(848/9999): loss=0.04643856296369725\n",
      "Gradient Descent(849/9999): loss=0.04643467742315524\n",
      "Gradient Descent(850/9999): loss=0.04643079799385372\n",
      "Gradient Descent(851/9999): loss=0.046426924666146424\n",
      "Gradient Descent(852/9999): loss=0.046423057430403036\n",
      "Gradient Descent(853/9999): loss=0.04641919627700907\n",
      "Gradient Descent(854/9999): loss=0.0464153411963659\n",
      "Gradient Descent(855/9999): loss=0.04641149217889068\n",
      "Gradient Descent(856/9999): loss=0.046407649215016306\n",
      "Gradient Descent(857/9999): loss=0.04640381229519142\n",
      "Gradient Descent(858/9999): loss=0.04639998140988031\n",
      "Gradient Descent(859/9999): loss=0.04639615654956291\n",
      "Gradient Descent(860/9999): loss=0.04639233770473475\n",
      "Gradient Descent(861/9999): loss=0.04638852486590697\n",
      "Gradient Descent(862/9999): loss=0.04638471802360619\n",
      "Gradient Descent(863/9999): loss=0.04638091716837451\n",
      "Gradient Descent(864/9999): loss=0.0463771222907695\n",
      "Gradient Descent(865/9999): loss=0.04637333338136419\n",
      "Gradient Descent(866/9999): loss=0.046369550430746954\n",
      "Gradient Descent(867/9999): loss=0.04636577342952149\n",
      "Gradient Descent(868/9999): loss=0.04636200236830686\n",
      "Gradient Descent(869/9999): loss=0.04635823723773738\n",
      "Gradient Descent(870/9999): loss=0.04635447802846262\n",
      "Gradient Descent(871/9999): loss=0.046350724731147326\n",
      "Gradient Descent(872/9999): loss=0.04634697733647146\n",
      "Gradient Descent(873/9999): loss=0.04634323583513012\n",
      "Gradient Descent(874/9999): loss=0.046339500217833486\n",
      "Gradient Descent(875/9999): loss=0.046335770475306834\n",
      "Gradient Descent(876/9999): loss=0.046332046598290474\n",
      "Gradient Descent(877/9999): loss=0.046328328577539726\n",
      "Gradient Descent(878/9999): loss=0.046324616403824875\n",
      "Gradient Descent(879/9999): loss=0.04632091006793119\n",
      "Gradient Descent(880/9999): loss=0.04631720956065881\n",
      "Gradient Descent(881/9999): loss=0.046313514872822766\n",
      "Gradient Descent(882/9999): loss=0.04630982599525295\n",
      "Gradient Descent(883/9999): loss=0.046306142918794035\n",
      "Gradient Descent(884/9999): loss=0.046302465634305526\n",
      "Gradient Descent(885/9999): loss=0.04629879413266165\n",
      "Gradient Descent(886/9999): loss=0.04629512840475139\n",
      "Gradient Descent(887/9999): loss=0.0462914684414784\n",
      "Gradient Descent(888/9999): loss=0.046287814233760965\n",
      "Gradient Descent(889/9999): loss=0.046284165772532096\n",
      "Gradient Descent(890/9999): loss=0.04628052304873929\n",
      "Gradient Descent(891/9999): loss=0.04627688605334472\n",
      "Gradient Descent(892/9999): loss=0.04627325477732502\n",
      "Gradient Descent(893/9999): loss=0.04626962921167137\n",
      "Gradient Descent(894/9999): loss=0.04626600934738948\n",
      "Gradient Descent(895/9999): loss=0.04626239517549943\n",
      "Gradient Descent(896/9999): loss=0.04625878668703583\n",
      "Gradient Descent(897/9999): loss=0.04625518387304755\n",
      "Gradient Descent(898/9999): loss=0.04625158672459796\n",
      "Gradient Descent(899/9999): loss=0.0462479952327647\n",
      "Gradient Descent(900/9999): loss=0.04624440938863976\n",
      "Gradient Descent(901/9999): loss=0.046240829183329335\n",
      "Gradient Descent(902/9999): loss=0.046237254607954005\n",
      "Gradient Descent(903/9999): loss=0.04623368565364844\n",
      "Gradient Descent(904/9999): loss=0.04623012231156163\n",
      "Gradient Descent(905/9999): loss=0.04622656457285665\n",
      "Gradient Descent(906/9999): loss=0.04622301242871078\n",
      "Gradient Descent(907/9999): loss=0.04621946587031539\n",
      "Gradient Descent(908/9999): loss=0.046215924888875944\n",
      "Gradient Descent(909/9999): loss=0.04621238947561193\n",
      "Gradient Descent(910/9999): loss=0.04620885962175698\n",
      "Gradient Descent(911/9999): loss=0.046205335318558645\n",
      "Gradient Descent(912/9999): loss=0.04620181655727848\n",
      "Gradient Descent(913/9999): loss=0.046198303329191975\n",
      "Gradient Descent(914/9999): loss=0.04619479562558863\n",
      "Gradient Descent(915/9999): loss=0.04619129343777182\n",
      "Gradient Descent(916/9999): loss=0.0461877967570587\n",
      "Gradient Descent(917/9999): loss=0.04618430557478041\n",
      "Gradient Descent(918/9999): loss=0.046180819882281884\n",
      "Gradient Descent(919/9999): loss=0.0461773396709218\n",
      "Gradient Descent(920/9999): loss=0.04617386493207268\n",
      "Gradient Descent(921/9999): loss=0.04617039565712075\n",
      "Gradient Descent(922/9999): loss=0.04616693183746601\n",
      "Gradient Descent(923/9999): loss=0.04616347346452208\n",
      "Gradient Descent(924/9999): loss=0.04616002052971638\n",
      "Gradient Descent(925/9999): loss=0.04615657302448985\n",
      "Gradient Descent(926/9999): loss=0.046153130940297125\n",
      "Gradient Descent(927/9999): loss=0.04614969426860644\n",
      "Gradient Descent(928/9999): loss=0.04614626300089958\n",
      "Gradient Descent(929/9999): loss=0.04614283712867193\n",
      "Gradient Descent(930/9999): loss=0.046139416643432345\n",
      "Gradient Descent(931/9999): loss=0.046136001536703185\n",
      "Gradient Descent(932/9999): loss=0.04613259180002033\n",
      "Gradient Descent(933/9999): loss=0.04612918742493312\n",
      "Gradient Descent(934/9999): loss=0.04612578840300427\n",
      "Gradient Descent(935/9999): loss=0.04612239472580994\n",
      "Gradient Descent(936/9999): loss=0.04611900638493964\n",
      "Gradient Descent(937/9999): loss=0.04611562337199633\n",
      "Gradient Descent(938/9999): loss=0.046112245678596134\n",
      "Gradient Descent(939/9999): loss=0.04610887329636868\n",
      "Gradient Descent(940/9999): loss=0.046105506216956756\n",
      "Gradient Descent(941/9999): loss=0.046102144432016455\n",
      "Gradient Descent(942/9999): loss=0.04609878793321708\n",
      "Gradient Descent(943/9999): loss=0.046095436712241235\n",
      "Gradient Descent(944/9999): loss=0.04609209076078465\n",
      "Gradient Descent(945/9999): loss=0.04608875007055621\n",
      "Gradient Descent(946/9999): loss=0.046085414633278005\n",
      "Gradient Descent(947/9999): loss=0.04608208444068523\n",
      "Gradient Descent(948/9999): loss=0.046078759484526166\n",
      "Gradient Descent(949/9999): loss=0.0460754397565622\n",
      "Gradient Descent(950/9999): loss=0.04607212524856772\n",
      "Gradient Descent(951/9999): loss=0.04606881595233024\n",
      "Gradient Descent(952/9999): loss=0.046065511859650236\n",
      "Gradient Descent(953/9999): loss=0.04606221296234114\n",
      "Gradient Descent(954/9999): loss=0.046058919252229436\n",
      "Gradient Descent(955/9999): loss=0.04605563072115448\n",
      "Gradient Descent(956/9999): loss=0.046052347360968554\n",
      "Gradient Descent(957/9999): loss=0.0460490691635369\n",
      "Gradient Descent(958/9999): loss=0.04604579612073758\n",
      "Gradient Descent(959/9999): loss=0.04604252822446155\n",
      "Gradient Descent(960/9999): loss=0.0460392654666126\n",
      "Gradient Descent(961/9999): loss=0.04603600783910729\n",
      "Gradient Descent(962/9999): loss=0.04603275533387495\n",
      "Gradient Descent(963/9999): loss=0.046029507942857865\n",
      "Gradient Descent(964/9999): loss=0.04602626565801083\n",
      "Gradient Descent(965/9999): loss=0.04602302847130147\n",
      "Gradient Descent(966/9999): loss=0.04601979637471017\n",
      "Gradient Descent(967/9999): loss=0.0460165693602299\n",
      "Gradient Descent(968/9999): loss=0.046013347419866325\n",
      "Gradient Descent(969/9999): loss=0.04601013054563778\n",
      "Gradient Descent(970/9999): loss=0.046006918729575214\n",
      "Gradient Descent(971/9999): loss=0.046003711963722146\n",
      "Gradient Descent(972/9999): loss=0.04600051024013467\n",
      "Gradient Descent(973/9999): loss=0.04599731355088146\n",
      "Gradient Descent(974/9999): loss=0.045994121888043714\n",
      "Gradient Descent(975/9999): loss=0.04599093524371512\n",
      "Gradient Descent(976/9999): loss=0.04598775361000194\n",
      "Gradient Descent(977/9999): loss=0.045984576979022806\n",
      "Gradient Descent(978/9999): loss=0.045981405342908836\n",
      "Gradient Descent(979/9999): loss=0.04597823869380363\n",
      "Gradient Descent(980/9999): loss=0.0459750770238631\n",
      "Gradient Descent(981/9999): loss=0.045971920325255676\n",
      "Gradient Descent(982/9999): loss=0.04596876859016205\n",
      "Gradient Descent(983/9999): loss=0.045965621810775265\n",
      "Gradient Descent(984/9999): loss=0.045962479979300745\n",
      "Gradient Descent(985/9999): loss=0.045959343087956225\n",
      "Gradient Descent(986/9999): loss=0.04595621112897168\n",
      "Gradient Descent(987/9999): loss=0.04595308409458934\n",
      "Gradient Descent(988/9999): loss=0.04594996197706375\n",
      "Gradient Descent(989/9999): loss=0.04594684476866163\n",
      "Gradient Descent(990/9999): loss=0.045943732461661925\n",
      "Gradient Descent(991/9999): loss=0.045940625048355735\n",
      "Gradient Descent(992/9999): loss=0.04593752252104638\n",
      "Gradient Descent(993/9999): loss=0.045934424872049275\n",
      "Gradient Descent(994/9999): loss=0.045931332093691976\n",
      "Gradient Descent(995/9999): loss=0.04592824417831416\n",
      "Gradient Descent(996/9999): loss=0.0459251611182676\n",
      "Gradient Descent(997/9999): loss=0.04592208290591608\n",
      "Gradient Descent(998/9999): loss=0.04591900953363546\n",
      "Gradient Descent(999/9999): loss=0.04591594099381368\n",
      "Gradient Descent(1000/9999): loss=0.045912877278850565\n",
      "Gradient Descent(1001/9999): loss=0.04590981838115805\n",
      "Gradient Descent(1002/9999): loss=0.045906764293159964\n",
      "Gradient Descent(1003/9999): loss=0.0459037150072921\n",
      "Gradient Descent(1004/9999): loss=0.04590067051600224\n",
      "Gradient Descent(1005/9999): loss=0.04589763081174993\n",
      "Gradient Descent(1006/9999): loss=0.04589459588700679\n",
      "Gradient Descent(1007/9999): loss=0.04589156573425615\n",
      "Gradient Descent(1008/9999): loss=0.04588854034599328\n",
      "Gradient Descent(1009/9999): loss=0.045885519714725265\n",
      "Gradient Descent(1010/9999): loss=0.04588250383297098\n",
      "Gradient Descent(1011/9999): loss=0.045879492693261144\n",
      "Gradient Descent(1012/9999): loss=0.04587648628813814\n",
      "Gradient Descent(1013/9999): loss=0.04587348461015627\n",
      "Gradient Descent(1014/9999): loss=0.04587048765188145\n",
      "Gradient Descent(1015/9999): loss=0.04586749540589133\n",
      "Gradient Descent(1016/9999): loss=0.04586450786477527\n",
      "Gradient Descent(1017/9999): loss=0.04586152502113433\n",
      "Gradient Descent(1018/9999): loss=0.04585854686758123\n",
      "Gradient Descent(1019/9999): loss=0.04585557339674031\n",
      "Gradient Descent(1020/9999): loss=0.04585260460124749\n",
      "Gradient Descent(1021/9999): loss=0.04584964047375039\n",
      "Gradient Descent(1022/9999): loss=0.045846681006908155\n",
      "Gradient Descent(1023/9999): loss=0.045843726193391506\n",
      "Gradient Descent(1024/9999): loss=0.04584077602588268\n",
      "Gradient Descent(1025/9999): loss=0.04583783049707552\n",
      "Gradient Descent(1026/9999): loss=0.04583488959967532\n",
      "Gradient Descent(1027/9999): loss=0.04583195332639887\n",
      "Gradient Descent(1028/9999): loss=0.045829021669974435\n",
      "Gradient Descent(1029/9999): loss=0.04582609462314177\n",
      "Gradient Descent(1030/9999): loss=0.04582317217865201\n",
      "Gradient Descent(1031/9999): loss=0.04582025432926776\n",
      "Gradient Descent(1032/9999): loss=0.045817341067763\n",
      "Gradient Descent(1033/9999): loss=0.045814432386923086\n",
      "Gradient Descent(1034/9999): loss=0.045811528279544725\n",
      "Gradient Descent(1035/9999): loss=0.04580862873843601\n",
      "Gradient Descent(1036/9999): loss=0.04580573375641635\n",
      "Gradient Descent(1037/9999): loss=0.045802843326316414\n",
      "Gradient Descent(1038/9999): loss=0.04579995744097823\n",
      "Gradient Descent(1039/9999): loss=0.045797076093255035\n",
      "Gradient Descent(1040/9999): loss=0.04579419927601138\n",
      "Gradient Descent(1041/9999): loss=0.045791326982123015\n",
      "Gradient Descent(1042/9999): loss=0.04578845920447692\n",
      "Gradient Descent(1043/9999): loss=0.04578559593597124\n",
      "Gradient Descent(1044/9999): loss=0.04578273716951536\n",
      "Gradient Descent(1045/9999): loss=0.04577988289802983\n",
      "Gradient Descent(1046/9999): loss=0.04577703311444626\n",
      "Gradient Descent(1047/9999): loss=0.04577418781170743\n",
      "Gradient Descent(1048/9999): loss=0.04577134698276735\n",
      "Gradient Descent(1049/9999): loss=0.04576851062059093\n",
      "Gradient Descent(1050/9999): loss=0.045765678718154304\n",
      "Gradient Descent(1051/9999): loss=0.045762851268444545\n",
      "Gradient Descent(1052/9999): loss=0.045760028264459886\n",
      "Gradient Descent(1053/9999): loss=0.045757209699209496\n",
      "Gradient Descent(1054/9999): loss=0.045754395565713556\n",
      "Gradient Descent(1055/9999): loss=0.04575158585700327\n",
      "Gradient Descent(1056/9999): loss=0.045748780566120806\n",
      "Gradient Descent(1057/9999): loss=0.045745979686119244\n",
      "Gradient Descent(1058/9999): loss=0.045743183210062675\n",
      "Gradient Descent(1059/9999): loss=0.04574039113102601\n",
      "Gradient Descent(1060/9999): loss=0.04573760344209514\n",
      "Gradient Descent(1061/9999): loss=0.0457348201363668\n",
      "Gradient Descent(1062/9999): loss=0.04573204120694859\n",
      "Gradient Descent(1063/9999): loss=0.045729266646958984\n",
      "Gradient Descent(1064/9999): loss=0.045726496449527254\n",
      "Gradient Descent(1065/9999): loss=0.04572373060779349\n",
      "Gradient Descent(1066/9999): loss=0.04572096911490863\n",
      "Gradient Descent(1067/9999): loss=0.045718211964034305\n",
      "Gradient Descent(1068/9999): loss=0.04571545914834298\n",
      "Gradient Descent(1069/9999): loss=0.045712710661017814\n",
      "Gradient Descent(1070/9999): loss=0.045709966495252725\n",
      "Gradient Descent(1071/9999): loss=0.04570722664425235\n",
      "Gradient Descent(1072/9999): loss=0.045704491101231974\n",
      "Gradient Descent(1073/9999): loss=0.04570175985941761\n",
      "Gradient Descent(1074/9999): loss=0.04569903291204588\n",
      "Gradient Descent(1075/9999): loss=0.045696310252364106\n",
      "Gradient Descent(1076/9999): loss=0.045693591873630195\n",
      "Gradient Descent(1077/9999): loss=0.045690877769112674\n",
      "Gradient Descent(1078/9999): loss=0.045688167932090656\n",
      "Gradient Descent(1079/9999): loss=0.045685462355853836\n",
      "Gradient Descent(1080/9999): loss=0.04568276103370247\n",
      "Gradient Descent(1081/9999): loss=0.045680063958947345\n",
      "Gradient Descent(1082/9999): loss=0.0456773711249098\n",
      "Gradient Descent(1083/9999): loss=0.04567468252492162\n",
      "Gradient Descent(1084/9999): loss=0.045671998152325165\n",
      "Gradient Descent(1085/9999): loss=0.045669318000473226\n",
      "Gradient Descent(1086/9999): loss=0.04566664206272904\n",
      "Gradient Descent(1087/9999): loss=0.045663970332466296\n",
      "Gradient Descent(1088/9999): loss=0.04566130280306913\n",
      "Gradient Descent(1089/9999): loss=0.04565863946793204\n",
      "Gradient Descent(1090/9999): loss=0.04565598032045999\n",
      "Gradient Descent(1091/9999): loss=0.04565332535406824\n",
      "Gradient Descent(1092/9999): loss=0.04565067456218247\n",
      "Gradient Descent(1093/9999): loss=0.04564802793823866\n",
      "Gradient Descent(1094/9999): loss=0.04564538547568315\n",
      "Gradient Descent(1095/9999): loss=0.04564274716797256\n",
      "Gradient Descent(1096/9999): loss=0.04564011300857382\n",
      "Gradient Descent(1097/9999): loss=0.04563748299096416\n",
      "Gradient Descent(1098/9999): loss=0.045634857108630994\n",
      "Gradient Descent(1099/9999): loss=0.04563223535507208\n",
      "Gradient Descent(1100/9999): loss=0.04562961772379537\n",
      "Gradient Descent(1101/9999): loss=0.04562700420831896\n",
      "Gradient Descent(1102/9999): loss=0.04562439480217126\n",
      "Gradient Descent(1103/9999): loss=0.04562178949889078\n",
      "Gradient Descent(1104/9999): loss=0.04561918829202621\n",
      "Gradient Descent(1105/9999): loss=0.04561659117513637\n",
      "Gradient Descent(1106/9999): loss=0.04561399814179028\n",
      "Gradient Descent(1107/9999): loss=0.04561140918556701\n",
      "Gradient Descent(1108/9999): loss=0.04560882430005577\n",
      "Gradient Descent(1109/9999): loss=0.04560624347885581\n",
      "Gradient Descent(1110/9999): loss=0.04560366671557647\n",
      "Gradient Descent(1111/9999): loss=0.04560109400383717\n",
      "Gradient Descent(1112/9999): loss=0.04559852533726734\n",
      "Gradient Descent(1113/9999): loss=0.04559596070950644\n",
      "Gradient Descent(1114/9999): loss=0.04559340011420389\n",
      "Gradient Descent(1115/9999): loss=0.04559084354501918\n",
      "Gradient Descent(1116/9999): loss=0.045588290995621686\n",
      "Gradient Descent(1117/9999): loss=0.045585742459690826\n",
      "Gradient Descent(1118/9999): loss=0.04558319793091593\n",
      "Gradient Descent(1119/9999): loss=0.045580657402996175\n",
      "Gradient Descent(1120/9999): loss=0.0455781208696408\n",
      "Gradient Descent(1121/9999): loss=0.045575588324568796\n",
      "Gradient Descent(1122/9999): loss=0.04557305976150912\n",
      "Gradient Descent(1123/9999): loss=0.04557053517420055\n",
      "Gradient Descent(1124/9999): loss=0.04556801455639173\n",
      "Gradient Descent(1125/9999): loss=0.04556549790184114\n",
      "Gradient Descent(1126/9999): loss=0.04556298520431708\n",
      "Gradient Descent(1127/9999): loss=0.0455604764575976\n",
      "Gradient Descent(1128/9999): loss=0.04555797165547064\n",
      "Gradient Descent(1129/9999): loss=0.04555547079173376\n",
      "Gradient Descent(1130/9999): loss=0.045552973860194426\n",
      "Gradient Descent(1131/9999): loss=0.0455504808546698\n",
      "Gradient Descent(1132/9999): loss=0.04554799176898668\n",
      "Gradient Descent(1133/9999): loss=0.045545506596981686\n",
      "Gradient Descent(1134/9999): loss=0.045543025332501055\n",
      "Gradient Descent(1135/9999): loss=0.04554054796940078\n",
      "Gradient Descent(1136/9999): loss=0.045538074501546434\n",
      "Gradient Descent(1137/9999): loss=0.04553560492281326\n",
      "Gradient Descent(1138/9999): loss=0.0455331392270862\n",
      "Gradient Descent(1139/9999): loss=0.04553067740825971\n",
      "Gradient Descent(1140/9999): loss=0.045528219460237926\n",
      "Gradient Descent(1141/9999): loss=0.045525765376934535\n",
      "Gradient Descent(1142/9999): loss=0.04552331515227281\n",
      "Gradient Descent(1143/9999): loss=0.04552086878018557\n",
      "Gradient Descent(1144/9999): loss=0.04551842625461518\n",
      "Gradient Descent(1145/9999): loss=0.045515987569513534\n",
      "Gradient Descent(1146/9999): loss=0.04551355271884205\n",
      "Gradient Descent(1147/9999): loss=0.04551112169657161\n",
      "Gradient Descent(1148/9999): loss=0.04550869449668261\n",
      "Gradient Descent(1149/9999): loss=0.045506271113164895\n",
      "Gradient Descent(1150/9999): loss=0.04550385154001778\n",
      "Gradient Descent(1151/9999): loss=0.04550143577124998\n",
      "Gradient Descent(1152/9999): loss=0.04549902380087964\n",
      "Gradient Descent(1153/9999): loss=0.04549661562293439\n",
      "Gradient Descent(1154/9999): loss=0.04549421123145114\n",
      "Gradient Descent(1155/9999): loss=0.04549181062047625\n",
      "Gradient Descent(1156/9999): loss=0.04548941378406538\n",
      "Gradient Descent(1157/9999): loss=0.045487020716283626\n",
      "Gradient Descent(1158/9999): loss=0.04548463141120537\n",
      "Gradient Descent(1159/9999): loss=0.0454822458629143\n",
      "Gradient Descent(1160/9999): loss=0.04547986406550339\n",
      "Gradient Descent(1161/9999): loss=0.04547748601307497\n",
      "Gradient Descent(1162/9999): loss=0.0454751116997406\n",
      "Gradient Descent(1163/9999): loss=0.0454727411196211\n",
      "Gradient Descent(1164/9999): loss=0.04547037426684656\n",
      "Gradient Descent(1165/9999): loss=0.045468011135556284\n",
      "Gradient Descent(1166/9999): loss=0.04546565171989877\n",
      "Gradient Descent(1167/9999): loss=0.045463296014031734\n",
      "Gradient Descent(1168/9999): loss=0.04546094401212215\n",
      "Gradient Descent(1169/9999): loss=0.045458595708346025\n",
      "Gradient Descent(1170/9999): loss=0.045456251096888646\n",
      "Gradient Descent(1171/9999): loss=0.04545391017194436\n",
      "Gradient Descent(1172/9999): loss=0.04545157292771671\n",
      "Gradient Descent(1173/9999): loss=0.04544923935841835\n",
      "Gradient Descent(1174/9999): loss=0.045446909458270957\n",
      "Gradient Descent(1175/9999): loss=0.04544458322150538\n",
      "Gradient Descent(1176/9999): loss=0.04544226064236149\n",
      "Gradient Descent(1177/9999): loss=0.04543994171508824\n",
      "Gradient Descent(1178/9999): loss=0.045437626433943654\n",
      "Gradient Descent(1179/9999): loss=0.04543531479319473\n",
      "Gradient Descent(1180/9999): loss=0.04543300678711746\n",
      "Gradient Descent(1181/9999): loss=0.04543070240999694\n",
      "Gradient Descent(1182/9999): loss=0.04542840165612717\n",
      "Gradient Descent(1183/9999): loss=0.04542610451981112\n",
      "Gradient Descent(1184/9999): loss=0.04542381099536079\n",
      "Gradient Descent(1185/9999): loss=0.04542152107709708\n",
      "Gradient Descent(1186/9999): loss=0.045419234759349766\n",
      "Gradient Descent(1187/9999): loss=0.04541695203645763\n",
      "Gradient Descent(1188/9999): loss=0.04541467290276833\n",
      "Gradient Descent(1189/9999): loss=0.04541239735263838\n",
      "Gradient Descent(1190/9999): loss=0.0454101253804332\n",
      "Gradient Descent(1191/9999): loss=0.045407856980527045\n",
      "Gradient Descent(1192/9999): loss=0.04540559214730307\n",
      "Gradient Descent(1193/9999): loss=0.04540333087515318\n",
      "Gradient Descent(1194/9999): loss=0.045401073158478154\n",
      "Gradient Descent(1195/9999): loss=0.0453988189916876\n",
      "Gradient Descent(1196/9999): loss=0.04539656836919982\n",
      "Gradient Descent(1197/9999): loss=0.045394321285442\n",
      "Gradient Descent(1198/9999): loss=0.045392077734850045\n",
      "Gradient Descent(1199/9999): loss=0.04538983771186856\n",
      "Gradient Descent(1200/9999): loss=0.04538760121095096\n",
      "Gradient Descent(1201/9999): loss=0.045385368226559385\n",
      "Gradient Descent(1202/9999): loss=0.04538313875316462\n",
      "Gradient Descent(1203/9999): loss=0.04538091278524618\n",
      "Gradient Descent(1204/9999): loss=0.04537869031729223\n",
      "Gradient Descent(1205/9999): loss=0.04537647134379972\n",
      "Gradient Descent(1206/9999): loss=0.04537425585927407\n",
      "Gradient Descent(1207/9999): loss=0.04537204385822946\n",
      "Gradient Descent(1208/9999): loss=0.04536983533518869\n",
      "Gradient Descent(1209/9999): loss=0.045367630284683116\n",
      "Gradient Descent(1210/9999): loss=0.045365428701252763\n",
      "Gradient Descent(1211/9999): loss=0.045363230579446204\n",
      "Gradient Descent(1212/9999): loss=0.04536103591382055\n",
      "Gradient Descent(1213/9999): loss=0.04535884469894156\n",
      "Gradient Descent(1214/9999): loss=0.04535665692938345\n",
      "Gradient Descent(1215/9999): loss=0.045354472599729016\n",
      "Gradient Descent(1216/9999): loss=0.04535229170456955\n",
      "Gradient Descent(1217/9999): loss=0.04535011423850485\n",
      "Gradient Descent(1218/9999): loss=0.04534794019614325\n",
      "Gradient Descent(1219/9999): loss=0.045345769572101485\n",
      "Gradient Descent(1220/9999): loss=0.045343602361004834\n",
      "Gradient Descent(1221/9999): loss=0.04534143855748695\n",
      "Gradient Descent(1222/9999): loss=0.045339278156189995\n",
      "Gradient Descent(1223/9999): loss=0.0453371211517645\n",
      "Gradient Descent(1224/9999): loss=0.045334967538869425\n",
      "Gradient Descent(1225/9999): loss=0.045332817312172174\n",
      "Gradient Descent(1226/9999): loss=0.04533067046634847\n",
      "Gradient Descent(1227/9999): loss=0.045328526996082405\n",
      "Gradient Descent(1228/9999): loss=0.045326386896066516\n",
      "Gradient Descent(1229/9999): loss=0.045324250161001604\n",
      "Gradient Descent(1230/9999): loss=0.045322116785596805\n",
      "Gradient Descent(1231/9999): loss=0.045319986764569636\n",
      "Gradient Descent(1232/9999): loss=0.045317860092645844\n",
      "Gradient Descent(1233/9999): loss=0.045315736764559515\n",
      "Gradient Descent(1234/9999): loss=0.04531361677505303\n",
      "Gradient Descent(1235/9999): loss=0.04531150011887701\n",
      "Gradient Descent(1236/9999): loss=0.04530938679079029\n",
      "Gradient Descent(1237/9999): loss=0.045307276785560034\n",
      "Gradient Descent(1238/9999): loss=0.04530517009796155\n",
      "Gradient Descent(1239/9999): loss=0.045303066722778465\n",
      "Gradient Descent(1240/9999): loss=0.04530096665480246\n",
      "Gradient Descent(1241/9999): loss=0.045298869888833554\n",
      "Gradient Descent(1242/9999): loss=0.04529677641967986\n",
      "Gradient Descent(1243/9999): loss=0.04529468624215763\n",
      "Gradient Descent(1244/9999): loss=0.04529259935109138\n",
      "Gradient Descent(1245/9999): loss=0.045290515741313646\n",
      "Gradient Descent(1246/9999): loss=0.045288435407665154\n",
      "Gradient Descent(1247/9999): loss=0.04528635834499473\n",
      "Gradient Descent(1248/9999): loss=0.045284284548159276\n",
      "Gradient Descent(1249/9999): loss=0.04528221401202381\n",
      "Gradient Descent(1250/9999): loss=0.04528014673146139\n",
      "Gradient Descent(1251/9999): loss=0.04527808270135321\n",
      "Gradient Descent(1252/9999): loss=0.04527602191658842\n",
      "Gradient Descent(1253/9999): loss=0.045273964372064254\n",
      "Gradient Descent(1254/9999): loss=0.04527191006268597\n",
      "Gradient Descent(1255/9999): loss=0.04526985898336682\n",
      "Gradient Descent(1256/9999): loss=0.04526781112902808\n",
      "Gradient Descent(1257/9999): loss=0.04526576649459898\n",
      "Gradient Descent(1258/9999): loss=0.04526372507501674\n",
      "Gradient Descent(1259/9999): loss=0.04526168686522657\n",
      "Gradient Descent(1260/9999): loss=0.04525965186018158\n",
      "Gradient Descent(1261/9999): loss=0.0452576200548428\n",
      "Gradient Descent(1262/9999): loss=0.04525559144417924\n",
      "Gradient Descent(1263/9999): loss=0.04525356602316784\n",
      "Gradient Descent(1264/9999): loss=0.04525154378679334\n",
      "Gradient Descent(1265/9999): loss=0.0452495247300484\n",
      "Gradient Descent(1266/9999): loss=0.04524750884793363\n",
      "Gradient Descent(1267/9999): loss=0.04524549613545747\n",
      "Gradient Descent(1268/9999): loss=0.045243486587636106\n",
      "Gradient Descent(1269/9999): loss=0.04524148019949368\n",
      "Gradient Descent(1270/9999): loss=0.0452394769660621\n",
      "Gradient Descent(1271/9999): loss=0.045237476882381104\n",
      "Gradient Descent(1272/9999): loss=0.045235479943498275\n",
      "Gradient Descent(1273/9999): loss=0.04523348614446885\n",
      "Gradient Descent(1274/9999): loss=0.045231495480355995\n",
      "Gradient Descent(1275/9999): loss=0.04522950794623053\n",
      "Gradient Descent(1276/9999): loss=0.04522752353717105\n",
      "Gradient Descent(1277/9999): loss=0.04522554224826398\n",
      "Gradient Descent(1278/9999): loss=0.04522356407460333\n",
      "Gradient Descent(1279/9999): loss=0.045221589011290905\n",
      "Gradient Descent(1280/9999): loss=0.045219617053436204\n",
      "Gradient Descent(1281/9999): loss=0.04521764819615637\n",
      "Gradient Descent(1282/9999): loss=0.04521568243457632\n",
      "Gradient Descent(1283/9999): loss=0.04521371976382854\n",
      "Gradient Descent(1284/9999): loss=0.04521176017905318\n",
      "Gradient Descent(1285/9999): loss=0.0452098036753981\n",
      "Gradient Descent(1286/9999): loss=0.04520785024801876\n",
      "Gradient Descent(1287/9999): loss=0.04520589989207818\n",
      "Gradient Descent(1288/9999): loss=0.045203952602747045\n",
      "Gradient Descent(1289/9999): loss=0.04520200837520361\n",
      "Gradient Descent(1290/9999): loss=0.04520006720463374\n",
      "Gradient Descent(1291/9999): loss=0.045198129086230845\n",
      "Gradient Descent(1292/9999): loss=0.04519619401519589\n",
      "Gradient Descent(1293/9999): loss=0.04519426198673739\n",
      "Gradient Descent(1294/9999): loss=0.04519233299607139\n",
      "Gradient Descent(1295/9999): loss=0.045190407038421476\n",
      "Gradient Descent(1296/9999): loss=0.04518848410901872\n",
      "Gradient Descent(1297/9999): loss=0.04518656420310174\n",
      "Gradient Descent(1298/9999): loss=0.045184647315916565\n",
      "Gradient Descent(1299/9999): loss=0.04518273344271675\n",
      "Gradient Descent(1300/9999): loss=0.0451808225787633\n",
      "Gradient Descent(1301/9999): loss=0.04517891471932465\n",
      "Gradient Descent(1302/9999): loss=0.045177009859676744\n",
      "Gradient Descent(1303/9999): loss=0.04517510799510288\n",
      "Gradient Descent(1304/9999): loss=0.04517320912089381\n",
      "Gradient Descent(1305/9999): loss=0.04517131323234769\n",
      "Gradient Descent(1306/9999): loss=0.045169420324770046\n",
      "Gradient Descent(1307/9999): loss=0.045167530393473786\n",
      "Gradient Descent(1308/9999): loss=0.045165643433779205\n",
      "Gradient Descent(1309/9999): loss=0.04516375944101397\n",
      "Gradient Descent(1310/9999): loss=0.04516187841051306\n",
      "Gradient Descent(1311/9999): loss=0.04516000033761882\n",
      "Gradient Descent(1312/9999): loss=0.045158125217680894\n",
      "Gradient Descent(1313/9999): loss=0.04515625304605622\n",
      "Gradient Descent(1314/9999): loss=0.04515438381810914\n",
      "Gradient Descent(1315/9999): loss=0.04515251752921112\n",
      "Gradient Descent(1316/9999): loss=0.045150654174741074\n",
      "Gradient Descent(1317/9999): loss=0.04514879375008502\n",
      "Gradient Descent(1318/9999): loss=0.045146936250636385\n",
      "Gradient Descent(1319/9999): loss=0.04514508167179572\n",
      "Gradient Descent(1320/9999): loss=0.045143230008970844\n",
      "Gradient Descent(1321/9999): loss=0.04514138125757686\n",
      "Gradient Descent(1322/9999): loss=0.04513953541303599\n",
      "Gradient Descent(1323/9999): loss=0.04513769247077768\n",
      "Gradient Descent(1324/9999): loss=0.045135852426238604\n",
      "Gradient Descent(1325/9999): loss=0.04513401527486255\n",
      "Gradient Descent(1326/9999): loss=0.04513218101210052\n",
      "Gradient Descent(1327/9999): loss=0.04513034963341063\n",
      "Gradient Descent(1328/9999): loss=0.04512852113425815\n",
      "Gradient Descent(1329/9999): loss=0.04512669551011553\n",
      "Gradient Descent(1330/9999): loss=0.04512487275646225\n",
      "Gradient Descent(1331/9999): loss=0.04512305286878496\n",
      "Gradient Descent(1332/9999): loss=0.04512123584257738\n",
      "Gradient Descent(1333/9999): loss=0.04511942167334034\n",
      "Gradient Descent(1334/9999): loss=0.0451176103565817\n",
      "Gradient Descent(1335/9999): loss=0.04511580188781645\n",
      "Gradient Descent(1336/9999): loss=0.04511399626256657\n",
      "Gradient Descent(1337/9999): loss=0.0451121934763611\n",
      "Gradient Descent(1338/9999): loss=0.045110393524736114\n",
      "Gradient Descent(1339/9999): loss=0.04510859640323473\n",
      "Gradient Descent(1340/9999): loss=0.04510680210740702\n",
      "Gradient Descent(1341/9999): loss=0.04510501063281012\n",
      "Gradient Descent(1342/9999): loss=0.04510322197500807\n",
      "Gradient Descent(1343/9999): loss=0.04510143612957193\n",
      "Gradient Descent(1344/9999): loss=0.04509965309207978\n",
      "Gradient Descent(1345/9999): loss=0.04509787285811652\n",
      "Gradient Descent(1346/9999): loss=0.0450960954232741\n",
      "Gradient Descent(1347/9999): loss=0.0450943207831514\n",
      "Gradient Descent(1348/9999): loss=0.04509254893335412\n",
      "Gradient Descent(1349/9999): loss=0.045090779869495\n",
      "Gradient Descent(1350/9999): loss=0.04508901358719358\n",
      "Gradient Descent(1351/9999): loss=0.04508725008207634\n",
      "Gradient Descent(1352/9999): loss=0.04508548934977659\n",
      "Gradient Descent(1353/9999): loss=0.045083731385934585\n",
      "Gradient Descent(1354/9999): loss=0.04508197618619735\n",
      "Gradient Descent(1355/9999): loss=0.0450802237462188\n",
      "Gradient Descent(1356/9999): loss=0.04507847406165966\n",
      "Gradient Descent(1357/9999): loss=0.045076727128187534\n",
      "Gradient Descent(1358/9999): loss=0.045074982941476736\n",
      "Gradient Descent(1359/9999): loss=0.0450732414972085\n",
      "Gradient Descent(1360/9999): loss=0.04507150279107072\n",
      "Gradient Descent(1361/9999): loss=0.0450697668187582\n",
      "Gradient Descent(1362/9999): loss=0.04506803357597246\n",
      "Gradient Descent(1363/9999): loss=0.04506630305842174\n",
      "Gradient Descent(1364/9999): loss=0.04506457526182107\n",
      "Gradient Descent(1365/9999): loss=0.04506285018189222\n",
      "Gradient Descent(1366/9999): loss=0.04506112781436368\n",
      "Gradient Descent(1367/9999): loss=0.04505940815497061\n",
      "Gradient Descent(1368/9999): loss=0.04505769119945497\n",
      "Gradient Descent(1369/9999): loss=0.04505597694356532\n",
      "Gradient Descent(1370/9999): loss=0.04505426538305702\n",
      "Gradient Descent(1371/9999): loss=0.04505255651369192\n",
      "Gradient Descent(1372/9999): loss=0.04505085033123875\n",
      "Gradient Descent(1373/9999): loss=0.04504914683147274\n",
      "Gradient Descent(1374/9999): loss=0.045047446010175796\n",
      "Gradient Descent(1375/9999): loss=0.045045747863136504\n",
      "Gradient Descent(1376/9999): loss=0.04504405238615001\n",
      "Gradient Descent(1377/9999): loss=0.045042359575018134\n",
      "Gradient Descent(1378/9999): loss=0.045040669425549254\n",
      "Gradient Descent(1379/9999): loss=0.045038981933558334\n",
      "Gradient Descent(1380/9999): loss=0.045037297094866924\n",
      "Gradient Descent(1381/9999): loss=0.04503561490530319\n",
      "Gradient Descent(1382/9999): loss=0.045033935360701785\n",
      "Gradient Descent(1383/9999): loss=0.04503225845690398\n",
      "Gradient Descent(1384/9999): loss=0.0450305841897575\n",
      "Gradient Descent(1385/9999): loss=0.045028912555116714\n",
      "Gradient Descent(1386/9999): loss=0.04502724354884241\n",
      "Gradient Descent(1387/9999): loss=0.04502557716680188\n",
      "Gradient Descent(1388/9999): loss=0.04502391340486905\n",
      "Gradient Descent(1389/9999): loss=0.04502225225892415\n",
      "Gradient Descent(1390/9999): loss=0.045020593724854015\n",
      "Gradient Descent(1391/9999): loss=0.04501893779855192\n",
      "Gradient Descent(1392/9999): loss=0.045017284475917546\n",
      "Gradient Descent(1393/9999): loss=0.045015633752857075\n",
      "Gradient Descent(1394/9999): loss=0.04501398562528313\n",
      "Gradient Descent(1395/9999): loss=0.04501234008911473\n",
      "Gradient Descent(1396/9999): loss=0.04501069714027727\n",
      "Gradient Descent(1397/9999): loss=0.045009056774702697\n",
      "Gradient Descent(1398/9999): loss=0.04500741898832918\n",
      "Gradient Descent(1399/9999): loss=0.04500578377710141\n",
      "Gradient Descent(1400/9999): loss=0.04500415113697038\n",
      "Gradient Descent(1401/9999): loss=0.04500252106389345\n",
      "Gradient Descent(1402/9999): loss=0.04500089355383438\n",
      "Gradient Descent(1403/9999): loss=0.04499926860276327\n",
      "Gradient Descent(1404/9999): loss=0.04499764620665648\n",
      "Gradient Descent(1405/9999): loss=0.0449960263614968\n",
      "Gradient Descent(1406/9999): loss=0.04499440906327326\n",
      "Gradient Descent(1407/9999): loss=0.044992794307981254\n",
      "Gradient Descent(1408/9999): loss=0.04499118209162245\n",
      "Gradient Descent(1409/9999): loss=0.044989572410204764\n",
      "Gradient Descent(1410/9999): loss=0.044987965259742475\n",
      "Gradient Descent(1411/9999): loss=0.044986360636256\n",
      "Gradient Descent(1412/9999): loss=0.04498475853577221\n",
      "Gradient Descent(1413/9999): loss=0.04498315895432398\n",
      "Gradient Descent(1414/9999): loss=0.04498156188795061\n",
      "Gradient Descent(1415/9999): loss=0.044979967332697574\n",
      "Gradient Descent(1416/9999): loss=0.04497837528461652\n",
      "Gradient Descent(1417/9999): loss=0.044976785739765324\n",
      "Gradient Descent(1418/9999): loss=0.04497519869420814\n",
      "Gradient Descent(1419/9999): loss=0.04497361414401519\n",
      "Gradient Descent(1420/9999): loss=0.04497203208526294\n",
      "Gradient Descent(1421/9999): loss=0.044970452514034046\n",
      "Gradient Descent(1422/9999): loss=0.04496887542641728\n",
      "Gradient Descent(1423/9999): loss=0.044967300818507534\n",
      "Gradient Descent(1424/9999): loss=0.044965728686405894\n",
      "Gradient Descent(1425/9999): loss=0.04496415902621962\n",
      "Gradient Descent(1426/9999): loss=0.044962591834061996\n",
      "Gradient Descent(1427/9999): loss=0.044961027106052445\n",
      "Gradient Descent(1428/9999): loss=0.044959464838316546\n",
      "Gradient Descent(1429/9999): loss=0.0449579050269859\n",
      "Gradient Descent(1430/9999): loss=0.04495634766819822\n",
      "Gradient Descent(1431/9999): loss=0.0449547927580973\n",
      "Gradient Descent(1432/9999): loss=0.044953240292833006\n",
      "Gradient Descent(1433/9999): loss=0.04495169026856118\n",
      "Gradient Descent(1434/9999): loss=0.04495014268144382\n",
      "Gradient Descent(1435/9999): loss=0.044948597527648924\n",
      "Gradient Descent(1436/9999): loss=0.044947054803350435\n",
      "Gradient Descent(1437/9999): loss=0.044945514504728425\n",
      "Gradient Descent(1438/9999): loss=0.044943976627968917\n",
      "Gradient Descent(1439/9999): loss=0.044942441169263915\n",
      "Gradient Descent(1440/9999): loss=0.04494090812481145\n",
      "Gradient Descent(1441/9999): loss=0.04493937749081549\n",
      "Gradient Descent(1442/9999): loss=0.04493784926348604\n",
      "Gradient Descent(1443/9999): loss=0.04493632343903895\n",
      "Gradient Descent(1444/9999): loss=0.044934800013696166\n",
      "Gradient Descent(1445/9999): loss=0.04493327898368541\n",
      "Gradient Descent(1446/9999): loss=0.044931760345240504\n",
      "Gradient Descent(1447/9999): loss=0.04493024409460108\n",
      "Gradient Descent(1448/9999): loss=0.0449287302280127\n",
      "Gradient Descent(1449/9999): loss=0.04492721874172683\n",
      "Gradient Descent(1450/9999): loss=0.04492570963200086\n",
      "Gradient Descent(1451/9999): loss=0.04492420289509808\n",
      "Gradient Descent(1452/9999): loss=0.044922698527287565\n",
      "Gradient Descent(1453/9999): loss=0.044921196524844324\n",
      "Gradient Descent(1454/9999): loss=0.04491969688404925\n",
      "Gradient Descent(1455/9999): loss=0.044918199601189014\n",
      "Gradient Descent(1456/9999): loss=0.044916704672556144\n",
      "Gradient Descent(1457/9999): loss=0.04491521209444902\n",
      "Gradient Descent(1458/9999): loss=0.044913721863171864\n",
      "Gradient Descent(1459/9999): loss=0.04491223397503464\n",
      "Gradient Descent(1460/9999): loss=0.044910748426353175\n",
      "Gradient Descent(1461/9999): loss=0.04490926521344904\n",
      "Gradient Descent(1462/9999): loss=0.04490778433264963\n",
      "Gradient Descent(1463/9999): loss=0.04490630578028812\n",
      "Gradient Descent(1464/9999): loss=0.04490482955270339\n",
      "Gradient Descent(1465/9999): loss=0.044903355646240144\n",
      "Gradient Descent(1466/9999): loss=0.044901884057248785\n",
      "Gradient Descent(1467/9999): loss=0.044900414782085486\n",
      "Gradient Descent(1468/9999): loss=0.04489894781711214\n",
      "Gradient Descent(1469/9999): loss=0.04489748315869638\n",
      "Gradient Descent(1470/9999): loss=0.04489602080321149\n",
      "Gradient Descent(1471/9999): loss=0.044894560747036516\n",
      "Gradient Descent(1472/9999): loss=0.04489310298655618\n",
      "Gradient Descent(1473/9999): loss=0.04489164751816086\n",
      "Gradient Descent(1474/9999): loss=0.04489019433824667\n",
      "Gradient Descent(1475/9999): loss=0.044888743443215334\n",
      "Gradient Descent(1476/9999): loss=0.04488729482947425\n",
      "Gradient Descent(1477/9999): loss=0.0448858484934365\n",
      "Gradient Descent(1478/9999): loss=0.04488440443152073\n",
      "Gradient Descent(1479/9999): loss=0.0448829626401513\n",
      "Gradient Descent(1480/9999): loss=0.044881523115758146\n",
      "Gradient Descent(1481/9999): loss=0.04488008585477682\n",
      "Gradient Descent(1482/9999): loss=0.044878650853648484\n",
      "Gradient Descent(1483/9999): loss=0.04487721810881989\n",
      "Gradient Descent(1484/9999): loss=0.04487578761674342\n",
      "Gradient Descent(1485/9999): loss=0.04487435937387697\n",
      "Gradient Descent(1486/9999): loss=0.04487293337668401\n",
      "Gradient Descent(1487/9999): loss=0.04487150962163364\n",
      "Gradient Descent(1488/9999): loss=0.044870088105200406\n",
      "Gradient Descent(1489/9999): loss=0.04486866882386451\n",
      "Gradient Descent(1490/9999): loss=0.04486725177411159\n",
      "Gradient Descent(1491/9999): loss=0.04486583695243293\n",
      "Gradient Descent(1492/9999): loss=0.04486442435532517\n",
      "Gradient Descent(1493/9999): loss=0.04486301397929057\n",
      "Gradient Descent(1494/9999): loss=0.0448616058208369\n",
      "Gradient Descent(1495/9999): loss=0.04486019987647734\n",
      "Gradient Descent(1496/9999): loss=0.044858796142730605\n",
      "Gradient Descent(1497/9999): loss=0.044857394616120885\n",
      "Gradient Descent(1498/9999): loss=0.044855995293177825\n",
      "Gradient Descent(1499/9999): loss=0.04485459817043652\n",
      "Gradient Descent(1500/9999): loss=0.04485320324443753\n",
      "Gradient Descent(1501/9999): loss=0.04485181051172684\n",
      "Gradient Descent(1502/9999): loss=0.04485041996885585\n",
      "Gradient Descent(1503/9999): loss=0.04484903161238145\n",
      "Gradient Descent(1504/9999): loss=0.04484764543886584\n",
      "Gradient Descent(1505/9999): loss=0.04484626144487672\n",
      "Gradient Descent(1506/9999): loss=0.04484487962698713\n",
      "Gradient Descent(1507/9999): loss=0.04484349998177553\n",
      "Gradient Descent(1508/9999): loss=0.04484212250582576\n",
      "Gradient Descent(1509/9999): loss=0.04484074719572696\n",
      "Gradient Descent(1510/9999): loss=0.04483937404807377\n",
      "Gradient Descent(1511/9999): loss=0.044838003059466044\n",
      "Gradient Descent(1512/9999): loss=0.04483663422650905\n",
      "Gradient Descent(1513/9999): loss=0.04483526754581339\n",
      "Gradient Descent(1514/9999): loss=0.044833903013995036\n",
      "Gradient Descent(1515/9999): loss=0.04483254062767518\n",
      "Gradient Descent(1516/9999): loss=0.04483118038348042\n",
      "Gradient Descent(1517/9999): loss=0.04482982227804261\n",
      "Gradient Descent(1518/9999): loss=0.04482846630799891\n",
      "Gradient Descent(1519/9999): loss=0.04482711246999175\n",
      "Gradient Descent(1520/9999): loss=0.044825760760668916\n",
      "Gradient Descent(1521/9999): loss=0.04482441117668334\n",
      "Gradient Descent(1522/9999): loss=0.04482306371469333\n",
      "Gradient Descent(1523/9999): loss=0.044821718371362354\n",
      "Gradient Descent(1524/9999): loss=0.04482037514335925\n",
      "Gradient Descent(1525/9999): loss=0.04481903402735792\n",
      "Gradient Descent(1526/9999): loss=0.04481769502003769\n",
      "Gradient Descent(1527/9999): loss=0.04481635811808297\n",
      "Gradient Descent(1528/9999): loss=0.04481502331818341\n",
      "Gradient Descent(1529/9999): loss=0.04481369061703388\n",
      "Gradient Descent(1530/9999): loss=0.044812360011334464\n",
      "Gradient Descent(1531/9999): loss=0.04481103149779043\n",
      "Gradient Descent(1532/9999): loss=0.044809705073112174\n",
      "Gradient Descent(1533/9999): loss=0.044808380734015295\n",
      "Gradient Descent(1534/9999): loss=0.044807058477220635\n",
      "Gradient Descent(1535/9999): loss=0.04480573829945404\n",
      "Gradient Descent(1536/9999): loss=0.044804420197446654\n",
      "Gradient Descent(1537/9999): loss=0.04480310416793465\n",
      "Gradient Descent(1538/9999): loss=0.04480179020765937\n",
      "Gradient Descent(1539/9999): loss=0.04480047831336734\n",
      "Gradient Descent(1540/9999): loss=0.04479916848181005\n",
      "Gradient Descent(1541/9999): loss=0.04479786070974431\n",
      "Gradient Descent(1542/9999): loss=0.04479655499393183\n",
      "Gradient Descent(1543/9999): loss=0.04479525133113949\n",
      "Gradient Descent(1544/9999): loss=0.04479394971813933\n",
      "Gradient Descent(1545/9999): loss=0.04479265015170832\n",
      "Gradient Descent(1546/9999): loss=0.04479135262862863\n",
      "Gradient Descent(1547/9999): loss=0.04479005714568737\n",
      "Gradient Descent(1548/9999): loss=0.04478876369967682\n",
      "Gradient Descent(1549/9999): loss=0.04478747228739422\n",
      "Gradient Descent(1550/9999): loss=0.044786182905641875\n",
      "Gradient Descent(1551/9999): loss=0.044784895551227104\n",
      "Gradient Descent(1552/9999): loss=0.044783610220962264\n",
      "Gradient Descent(1553/9999): loss=0.04478232691166475\n",
      "Gradient Descent(1554/9999): loss=0.04478104562015688\n",
      "Gradient Descent(1555/9999): loss=0.04477976634326602\n",
      "Gradient Descent(1556/9999): loss=0.044778489077824564\n",
      "Gradient Descent(1557/9999): loss=0.044777213820669806\n",
      "Gradient Descent(1558/9999): loss=0.04477594056864406\n",
      "Gradient Descent(1559/9999): loss=0.044774669318594605\n",
      "Gradient Descent(1560/9999): loss=0.04477340006737365\n",
      "Gradient Descent(1561/9999): loss=0.04477213281183838\n",
      "Gradient Descent(1562/9999): loss=0.044770867548850936\n",
      "Gradient Descent(1563/9999): loss=0.0447696042752783\n",
      "Gradient Descent(1564/9999): loss=0.044768342987992524\n",
      "Gradient Descent(1565/9999): loss=0.044767083683870466\n",
      "Gradient Descent(1566/9999): loss=0.04476582635979393\n",
      "Gradient Descent(1567/9999): loss=0.04476457101264964\n",
      "Gradient Descent(1568/9999): loss=0.044763317639329205\n",
      "Gradient Descent(1569/9999): loss=0.0447620662367291\n",
      "Gradient Descent(1570/9999): loss=0.04476081680175071\n",
      "Gradient Descent(1571/9999): loss=0.04475956933130026\n",
      "Gradient Descent(1572/9999): loss=0.04475832382228887\n",
      "Gradient Descent(1573/9999): loss=0.04475708027163249\n",
      "Gradient Descent(1574/9999): loss=0.04475583867625198\n",
      "Gradient Descent(1575/9999): loss=0.04475459903307294\n",
      "Gradient Descent(1576/9999): loss=0.04475336133902588\n",
      "Gradient Descent(1577/9999): loss=0.04475212559104611\n",
      "Gradient Descent(1578/9999): loss=0.04475089178607376\n",
      "Gradient Descent(1579/9999): loss=0.04474965992105382\n",
      "Gradient Descent(1580/9999): loss=0.044748429992935976\n",
      "Gradient Descent(1581/9999): loss=0.044747201998674824\n",
      "Gradient Descent(1582/9999): loss=0.04474597593522964\n",
      "Gradient Descent(1583/9999): loss=0.044744751799564594\n",
      "Gradient Descent(1584/9999): loss=0.04474352958864854\n",
      "Gradient Descent(1585/9999): loss=0.04474230929945515\n",
      "Gradient Descent(1586/9999): loss=0.04474109092896284\n",
      "Gradient Descent(1587/9999): loss=0.04473987447415475\n",
      "Gradient Descent(1588/9999): loss=0.04473865993201878\n",
      "Gradient Descent(1589/9999): loss=0.044737447299547595\n",
      "Gradient Descent(1590/9999): loss=0.04473623657373859\n",
      "Gradient Descent(1591/9999): loss=0.04473502775159381\n",
      "Gradient Descent(1592/9999): loss=0.04473382083012007\n",
      "Gradient Descent(1593/9999): loss=0.044732615806328896\n",
      "Gradient Descent(1594/9999): loss=0.04473141267723649\n",
      "Gradient Descent(1595/9999): loss=0.04473021143986375\n",
      "Gradient Descent(1596/9999): loss=0.044729012091236274\n",
      "Gradient Descent(1597/9999): loss=0.04472781462838435\n",
      "Gradient Descent(1598/9999): loss=0.044726619048342846\n",
      "Gradient Descent(1599/9999): loss=0.044725425348151424\n",
      "Gradient Descent(1600/9999): loss=0.04472423352485427\n",
      "Gradient Descent(1601/9999): loss=0.04472304357550033\n",
      "Gradient Descent(1602/9999): loss=0.04472185549714314\n",
      "Gradient Descent(1603/9999): loss=0.04472066928684083\n",
      "Gradient Descent(1604/9999): loss=0.04471948494165624\n",
      "Gradient Descent(1605/9999): loss=0.04471830245865674\n",
      "Gradient Descent(1606/9999): loss=0.04471712183491441\n",
      "Gradient Descent(1607/9999): loss=0.04471594306750582\n",
      "Gradient Descent(1608/9999): loss=0.04471476615351224\n",
      "Gradient Descent(1609/9999): loss=0.04471359109001946\n",
      "Gradient Descent(1610/9999): loss=0.04471241787411788\n",
      "Gradient Descent(1611/9999): loss=0.04471124650290248\n",
      "Gradient Descent(1612/9999): loss=0.044710076973472776\n",
      "Gradient Descent(1613/9999): loss=0.04470890928293291\n",
      "Gradient Descent(1614/9999): loss=0.044707743428391504\n",
      "Gradient Descent(1615/9999): loss=0.044706579406961745\n",
      "Gradient Descent(1616/9999): loss=0.044705417215761385\n",
      "Gradient Descent(1617/9999): loss=0.04470425685191271\n",
      "Gradient Descent(1618/9999): loss=0.044703098312542495\n",
      "Gradient Descent(1619/9999): loss=0.04470194159478206\n",
      "Gradient Descent(1620/9999): loss=0.04470078669576724\n",
      "Gradient Descent(1621/9999): loss=0.04469963361263833\n",
      "Gradient Descent(1622/9999): loss=0.044698482342540174\n",
      "Gradient Descent(1623/9999): loss=0.04469733288262208\n",
      "Gradient Descent(1624/9999): loss=0.044696185230037856\n",
      "Gradient Descent(1625/9999): loss=0.044695039381945724\n",
      "Gradient Descent(1626/9999): loss=0.04469389533550849\n",
      "Gradient Descent(1627/9999): loss=0.04469275308789329\n",
      "Gradient Descent(1628/9999): loss=0.044691612636271805\n",
      "Gradient Descent(1629/9999): loss=0.044690473977820125\n",
      "Gradient Descent(1630/9999): loss=0.04468933710971882\n",
      "Gradient Descent(1631/9999): loss=0.044688202029152836\n",
      "Gradient Descent(1632/9999): loss=0.04468706873331156\n",
      "Gradient Descent(1633/9999): loss=0.04468593721938884\n",
      "Gradient Descent(1634/9999): loss=0.04468480748458288\n",
      "Gradient Descent(1635/9999): loss=0.044683679526096344\n",
      "Gradient Descent(1636/9999): loss=0.04468255334113624\n",
      "Gradient Descent(1637/9999): loss=0.044681428926914\n",
      "Gradient Descent(1638/9999): loss=0.04468030628064548\n",
      "Gradient Descent(1639/9999): loss=0.044679185399550764\n",
      "Gradient Descent(1640/9999): loss=0.044678066280854516\n",
      "Gradient Descent(1641/9999): loss=0.0446769489217856\n",
      "Gradient Descent(1642/9999): loss=0.04467583331957732\n",
      "Gradient Descent(1643/9999): loss=0.04467471947146729\n",
      "Gradient Descent(1644/9999): loss=0.04467360737469751\n",
      "Gradient Descent(1645/9999): loss=0.04467249702651425\n",
      "Gradient Descent(1646/9999): loss=0.04467138842416817\n",
      "Gradient Descent(1647/9999): loss=0.04467028156491425\n",
      "Gradient Descent(1648/9999): loss=0.044669176446011746\n",
      "Gradient Descent(1649/9999): loss=0.044668073064724235\n",
      "Gradient Descent(1650/9999): loss=0.044666971418319656\n",
      "Gradient Descent(1651/9999): loss=0.044665871504070136\n",
      "Gradient Descent(1652/9999): loss=0.0446647733192522\n",
      "Gradient Descent(1653/9999): loss=0.04466367686114655\n",
      "Gradient Descent(1654/9999): loss=0.04466258212703826\n",
      "Gradient Descent(1655/9999): loss=0.04466148911421665\n",
      "Gradient Descent(1656/9999): loss=0.04466039781997523\n",
      "Gradient Descent(1657/9999): loss=0.044659308241611864\n",
      "Gradient Descent(1658/9999): loss=0.04465822037642861\n",
      "Gradient Descent(1659/9999): loss=0.044657134221731745\n",
      "Gradient Descent(1660/9999): loss=0.04465604977483187\n",
      "Gradient Descent(1661/9999): loss=0.044654967033043697\n",
      "Gradient Descent(1662/9999): loss=0.04465388599368629\n",
      "Gradient Descent(1663/9999): loss=0.04465280665408281\n",
      "Gradient Descent(1664/9999): loss=0.044651729011560705\n",
      "Gradient Descent(1665/9999): loss=0.04465065306345158\n",
      "Gradient Descent(1666/9999): loss=0.044649578807091285\n",
      "Gradient Descent(1667/9999): loss=0.044648506239819745\n",
      "Gradient Descent(1668/9999): loss=0.04464743535898126\n",
      "Gradient Descent(1669/9999): loss=0.04464636616192411\n",
      "Gradient Descent(1670/9999): loss=0.0446452986460009\n",
      "Gradient Descent(1671/9999): loss=0.04464423280856826\n",
      "Gradient Descent(1672/9999): loss=0.0446431686469871\n",
      "Gradient Descent(1673/9999): loss=0.044642106158622366\n",
      "Gradient Descent(1674/9999): loss=0.044641045340843254\n",
      "Gradient Descent(1675/9999): loss=0.044639986191023044\n",
      "Gradient Descent(1676/9999): loss=0.04463892870653911\n",
      "Gradient Descent(1677/9999): loss=0.044637872884773014\n",
      "Gradient Descent(1678/9999): loss=0.044636818723110434\n",
      "Gradient Descent(1679/9999): loss=0.04463576621894108\n",
      "Gradient Descent(1680/9999): loss=0.04463471536965888\n",
      "Gradient Descent(1681/9999): loss=0.04463366617266174\n",
      "Gradient Descent(1682/9999): loss=0.04463261862535177\n",
      "Gradient Descent(1683/9999): loss=0.04463157272513508\n",
      "Gradient Descent(1684/9999): loss=0.04463052846942192\n",
      "Gradient Descent(1685/9999): loss=0.044629485855626524\n",
      "Gradient Descent(1686/9999): loss=0.04462844488116732\n",
      "Gradient Descent(1687/9999): loss=0.04462740554346668\n",
      "Gradient Descent(1688/9999): loss=0.0446263678399511\n",
      "Gradient Descent(1689/9999): loss=0.04462533176805108\n",
      "Gradient Descent(1690/9999): loss=0.044624297325201186\n",
      "Gradient Descent(1691/9999): loss=0.04462326450884\n",
      "Gradient Descent(1692/9999): loss=0.04462223331641015\n",
      "Gradient Descent(1693/9999): loss=0.04462120374535828\n",
      "Gradient Descent(1694/9999): loss=0.04462017579313502\n",
      "Gradient Descent(1695/9999): loss=0.044619149457195056\n",
      "Gradient Descent(1696/9999): loss=0.04461812473499704\n",
      "Gradient Descent(1697/9999): loss=0.04461710162400363\n",
      "Gradient Descent(1698/9999): loss=0.04461608012168148\n",
      "Gradient Descent(1699/9999): loss=0.044615060225501234\n",
      "Gradient Descent(1700/9999): loss=0.04461404193293749\n",
      "Gradient Descent(1701/9999): loss=0.04461302524146881\n",
      "Gradient Descent(1702/9999): loss=0.04461201014857779\n",
      "Gradient Descent(1703/9999): loss=0.04461099665175089\n",
      "Gradient Descent(1704/9999): loss=0.04460998474847857\n",
      "Gradient Descent(1705/9999): loss=0.04460897443625525\n",
      "Gradient Descent(1706/9999): loss=0.04460796571257928\n",
      "Gradient Descent(1707/9999): loss=0.044606958574952886\n",
      "Gradient Descent(1708/9999): loss=0.04460595302088231\n",
      "Gradient Descent(1709/9999): loss=0.04460494904787767\n",
      "Gradient Descent(1710/9999): loss=0.04460394665345298\n",
      "Gradient Descent(1711/9999): loss=0.04460294583512622\n",
      "Gradient Descent(1712/9999): loss=0.04460194659041921\n",
      "Gradient Descent(1713/9999): loss=0.04460094891685774\n",
      "Gradient Descent(1714/9999): loss=0.0445999528119714\n",
      "Gradient Descent(1715/9999): loss=0.044598958273293715\n",
      "Gradient Descent(1716/9999): loss=0.044597965298362074\n",
      "Gradient Descent(1717/9999): loss=0.04459697388471782\n",
      "Gradient Descent(1718/9999): loss=0.044595984029906\n",
      "Gradient Descent(1719/9999): loss=0.04459499573147564\n",
      "Gradient Descent(1720/9999): loss=0.04459400898697963\n",
      "Gradient Descent(1721/9999): loss=0.04459302379397459\n",
      "Gradient Descent(1722/9999): loss=0.044592040150021155\n",
      "Gradient Descent(1723/9999): loss=0.04459105805268362\n",
      "Gradient Descent(1724/9999): loss=0.044590077499530194\n",
      "Gradient Descent(1725/9999): loss=0.04458909848813297\n",
      "Gradient Descent(1726/9999): loss=0.04458812101606774\n",
      "Gradient Descent(1727/9999): loss=0.04458714508091417\n",
      "Gradient Descent(1728/9999): loss=0.04458617068025573\n",
      "Gradient Descent(1729/9999): loss=0.044585197811679675\n",
      "Gradient Descent(1730/9999): loss=0.044584226472777065\n",
      "Gradient Descent(1731/9999): loss=0.04458325666114275\n",
      "Gradient Descent(1732/9999): loss=0.044582288374375355\n",
      "Gradient Descent(1733/9999): loss=0.044581321610077274\n",
      "Gradient Descent(1734/9999): loss=0.044580356365854674\n",
      "Gradient Descent(1735/9999): loss=0.0445793926393175\n",
      "Gradient Descent(1736/9999): loss=0.04457843042807944\n",
      "Gradient Descent(1737/9999): loss=0.04457746972975794\n",
      "Gradient Descent(1738/9999): loss=0.0445765105419742\n",
      "Gradient Descent(1739/9999): loss=0.044575552862353124\n",
      "Gradient Descent(1740/9999): loss=0.04457459668852343\n",
      "Gradient Descent(1741/9999): loss=0.04457364201811745\n",
      "Gradient Descent(1742/9999): loss=0.04457268884877135\n",
      "Gradient Descent(1743/9999): loss=0.04457173717812493\n",
      "Gradient Descent(1744/9999): loss=0.04457078700382176\n",
      "Gradient Descent(1745/9999): loss=0.044569838323509074\n",
      "Gradient Descent(1746/9999): loss=0.044568891134837856\n",
      "Gradient Descent(1747/9999): loss=0.04456794543546269\n",
      "Gradient Descent(1748/9999): loss=0.04456700122304192\n",
      "Gradient Descent(1749/9999): loss=0.04456605849523762\n",
      "Gradient Descent(1750/9999): loss=0.044565117249715396\n",
      "Gradient Descent(1751/9999): loss=0.04456417748414466\n",
      "Gradient Descent(1752/9999): loss=0.04456323919619843\n",
      "Gradient Descent(1753/9999): loss=0.044562302383553354\n",
      "Gradient Descent(1754/9999): loss=0.04456136704388982\n",
      "Gradient Descent(1755/9999): loss=0.04456043317489175\n",
      "Gradient Descent(1756/9999): loss=0.044559500774246835\n",
      "Gradient Descent(1757/9999): loss=0.04455856983964626\n",
      "Gradient Descent(1758/9999): loss=0.04455764036878496\n",
      "Gradient Descent(1759/9999): loss=0.044556712359361456\n",
      "Gradient Descent(1760/9999): loss=0.044555785809077875\n",
      "Gradient Descent(1761/9999): loss=0.04455486071563995\n",
      "Gradient Descent(1762/9999): loss=0.044553937076757026\n",
      "Gradient Descent(1763/9999): loss=0.044553014890142094\n",
      "Gradient Descent(1764/9999): loss=0.0445520941535117\n",
      "Gradient Descent(1765/9999): loss=0.04455117486458593\n",
      "Gradient Descent(1766/9999): loss=0.04455025702108856\n",
      "Gradient Descent(1767/9999): loss=0.04454934062074689\n",
      "Gradient Descent(1768/9999): loss=0.044548425661291795\n",
      "Gradient Descent(1769/9999): loss=0.04454751214045772\n",
      "Gradient Descent(1770/9999): loss=0.044546600055982676\n",
      "Gradient Descent(1771/9999): loss=0.044545689405608206\n",
      "Gradient Descent(1772/9999): loss=0.04454478018707946\n",
      "Gradient Descent(1773/9999): loss=0.044543872398145085\n",
      "Gradient Descent(1774/9999): loss=0.044542966036557295\n",
      "Gradient Descent(1775/9999): loss=0.04454206110007178\n",
      "Gradient Descent(1776/9999): loss=0.04454115758644786\n",
      "Gradient Descent(1777/9999): loss=0.0445402554934483\n",
      "Gradient Descent(1778/9999): loss=0.0445393548188394\n",
      "Gradient Descent(1779/9999): loss=0.044538455560390974\n",
      "Gradient Descent(1780/9999): loss=0.044537557715876364\n",
      "Gradient Descent(1781/9999): loss=0.0445366612830724\n",
      "Gradient Descent(1782/9999): loss=0.04453576625975937\n",
      "Gradient Descent(1783/9999): loss=0.04453487264372111\n",
      "Gradient Descent(1784/9999): loss=0.044533980432744896\n",
      "Gradient Descent(1785/9999): loss=0.04453308962462154\n",
      "Gradient Descent(1786/9999): loss=0.044532200217145254\n",
      "Gradient Descent(1787/9999): loss=0.044531312208113766\n",
      "Gradient Descent(1788/9999): loss=0.04453042559532827\n",
      "Gradient Descent(1789/9999): loss=0.04452954037659336\n",
      "Gradient Descent(1790/9999): loss=0.044528656549717154\n",
      "Gradient Descent(1791/9999): loss=0.044527774112511194\n",
      "Gradient Descent(1792/9999): loss=0.044526893062790444\n",
      "Gradient Descent(1793/9999): loss=0.044526013398373324\n",
      "Gradient Descent(1794/9999): loss=0.04452513511708165\n",
      "Gradient Descent(1795/9999): loss=0.044524258216740734\n",
      "Gradient Descent(1796/9999): loss=0.0445233826951792\n",
      "Gradient Descent(1797/9999): loss=0.04452250855022919\n",
      "Gradient Descent(1798/9999): loss=0.04452163577972621\n",
      "Gradient Descent(1799/9999): loss=0.044520764381509176\n",
      "Gradient Descent(1800/9999): loss=0.044519894353420374\n",
      "Gradient Descent(1801/9999): loss=0.044519025693305514\n",
      "Gradient Descent(1802/9999): loss=0.044518158399013685\n",
      "Gradient Descent(1803/9999): loss=0.04451729246839738\n",
      "Gradient Descent(1804/9999): loss=0.04451642789931241\n",
      "Gradient Descent(1805/9999): loss=0.04451556468961801\n",
      "Gradient Descent(1806/9999): loss=0.044514702837176776\n",
      "Gradient Descent(1807/9999): loss=0.044513842339854656\n",
      "Gradient Descent(1808/9999): loss=0.04451298319552094\n",
      "Gradient Descent(1809/9999): loss=0.04451212540204828\n",
      "Gradient Descent(1810/9999): loss=0.04451126895731267\n",
      "Gradient Descent(1811/9999): loss=0.044510413859193465\n",
      "Gradient Descent(1812/9999): loss=0.0445095601055733\n",
      "Gradient Descent(1813/9999): loss=0.04450870769433819\n",
      "Gradient Descent(1814/9999): loss=0.04450785662337747\n",
      "Gradient Descent(1815/9999): loss=0.044507006890583796\n",
      "Gradient Descent(1816/9999): loss=0.04450615849385308\n",
      "Gradient Descent(1817/9999): loss=0.044505311431084625\n",
      "Gradient Descent(1818/9999): loss=0.04450446570018099\n",
      "Gradient Descent(1819/9999): loss=0.044503621299048\n",
      "Gradient Descent(1820/9999): loss=0.04450277822559486\n",
      "Gradient Descent(1821/9999): loss=0.044501936477734025\n",
      "Gradient Descent(1822/9999): loss=0.04450109605338116\n",
      "Gradient Descent(1823/9999): loss=0.044500256950455334\n",
      "Gradient Descent(1824/9999): loss=0.0444994191668788\n",
      "Gradient Descent(1825/9999): loss=0.04449858270057708\n",
      "Gradient Descent(1826/9999): loss=0.04449774754947902\n",
      "Gradient Descent(1827/9999): loss=0.04449691371151663\n",
      "Gradient Descent(1828/9999): loss=0.044496081184625266\n",
      "Gradient Descent(1829/9999): loss=0.04449524996674347\n",
      "Gradient Descent(1830/9999): loss=0.04449442005581306\n",
      "Gradient Descent(1831/9999): loss=0.04449359144977905\n",
      "Gradient Descent(1832/9999): loss=0.0444927641465897\n",
      "Gradient Descent(1833/9999): loss=0.04449193814419657\n",
      "Gradient Descent(1834/9999): loss=0.0444911134405543\n",
      "Gradient Descent(1835/9999): loss=0.044490290033620844\n",
      "Gradient Descent(1836/9999): loss=0.044489467921357356\n",
      "Gradient Descent(1837/9999): loss=0.04448864710172819\n",
      "Gradient Descent(1838/9999): loss=0.04448782757270085\n",
      "Gradient Descent(1839/9999): loss=0.044487009332246155\n",
      "Gradient Descent(1840/9999): loss=0.04448619237833795\n",
      "Gradient Descent(1841/9999): loss=0.04448537670895343\n",
      "Gradient Descent(1842/9999): loss=0.044484562322072846\n",
      "Gradient Descent(1843/9999): loss=0.04448374921567971\n",
      "Gradient Descent(1844/9999): loss=0.04448293738776066\n",
      "Gradient Descent(1845/9999): loss=0.044482126836305494\n",
      "Gradient Descent(1846/9999): loss=0.04448131755930719\n",
      "Gradient Descent(1847/9999): loss=0.04448050955476189\n",
      "Gradient Descent(1848/9999): loss=0.04447970282066885\n",
      "Gradient Descent(1849/9999): loss=0.04447889735503053\n",
      "Gradient Descent(1850/9999): loss=0.04447809315585247\n",
      "Gradient Descent(1851/9999): loss=0.0444772902211434\n",
      "Gradient Descent(1852/9999): loss=0.044476488548915116\n",
      "Gradient Descent(1853/9999): loss=0.04447568813718261\n",
      "Gradient Descent(1854/9999): loss=0.044474888983963935\n",
      "Gradient Descent(1855/9999): loss=0.04447409108728031\n",
      "Gradient Descent(1856/9999): loss=0.04447329444515605\n",
      "Gradient Descent(1857/9999): loss=0.04447249905561855\n",
      "Gradient Descent(1858/9999): loss=0.044471704916698294\n",
      "Gradient Descent(1859/9999): loss=0.044470912026428965\n",
      "Gradient Descent(1860/9999): loss=0.044470120382847235\n",
      "Gradient Descent(1861/9999): loss=0.04446932998399291\n",
      "Gradient Descent(1862/9999): loss=0.044468540827908826\n",
      "Gradient Descent(1863/9999): loss=0.044467752912640976\n",
      "Gradient Descent(1864/9999): loss=0.04446696623623837\n",
      "Gradient Descent(1865/9999): loss=0.04446618079675308\n",
      "Gradient Descent(1866/9999): loss=0.044465396592240296\n",
      "Gradient Descent(1867/9999): loss=0.0444646136207582\n",
      "Gradient Descent(1868/9999): loss=0.044463831880368065\n",
      "Gradient Descent(1869/9999): loss=0.04446305136913422\n",
      "Gradient Descent(1870/9999): loss=0.04446227208512399\n",
      "Gradient Descent(1871/9999): loss=0.0444614940264078\n",
      "Gradient Descent(1872/9999): loss=0.04446071719105906\n",
      "Gradient Descent(1873/9999): loss=0.04445994157715423\n",
      "Gradient Descent(1874/9999): loss=0.04445916718277283\n",
      "Gradient Descent(1875/9999): loss=0.04445839400599731\n",
      "Gradient Descent(1876/9999): loss=0.04445762204491321\n",
      "Gradient Descent(1877/9999): loss=0.04445685129760906\n",
      "Gradient Descent(1878/9999): loss=0.04445608176217641\n",
      "Gradient Descent(1879/9999): loss=0.04445531343670976\n",
      "Gradient Descent(1880/9999): loss=0.04445454631930667\n",
      "Gradient Descent(1881/9999): loss=0.04445378040806765\n",
      "Gradient Descent(1882/9999): loss=0.04445301570109622\n",
      "Gradient Descent(1883/9999): loss=0.04445225219649883\n",
      "Gradient Descent(1884/9999): loss=0.044451489892384996\n",
      "Gradient Descent(1885/9999): loss=0.04445072878686712\n",
      "Gradient Descent(1886/9999): loss=0.04444996887806065\n",
      "Gradient Descent(1887/9999): loss=0.04444921016408393\n",
      "Gradient Descent(1888/9999): loss=0.044448452643058294\n",
      "Gradient Descent(1889/9999): loss=0.04444769631310802\n",
      "Gradient Descent(1890/9999): loss=0.04444694117236036\n",
      "Gradient Descent(1891/9999): loss=0.04444618721894547\n",
      "Gradient Descent(1892/9999): loss=0.04444543445099646\n",
      "Gradient Descent(1893/9999): loss=0.04444468286664943\n",
      "Gradient Descent(1894/9999): loss=0.04444393246404333\n",
      "Gradient Descent(1895/9999): loss=0.044443183241320044\n",
      "Gradient Descent(1896/9999): loss=0.04444243519662445\n",
      "Gradient Descent(1897/9999): loss=0.044441688328104266\n",
      "Gradient Descent(1898/9999): loss=0.04444094263391017\n",
      "Gradient Descent(1899/9999): loss=0.04444019811219574\n",
      "Gradient Descent(1900/9999): loss=0.04443945476111741\n",
      "Gradient Descent(1901/9999): loss=0.044438712578834604\n",
      "Gradient Descent(1902/9999): loss=0.0444379715635095\n",
      "Gradient Descent(1903/9999): loss=0.04443723171330734\n",
      "Gradient Descent(1904/9999): loss=0.04443649302639611\n",
      "Gradient Descent(1905/9999): loss=0.04443575550094675\n",
      "Gradient Descent(1906/9999): loss=0.044435019135133064\n",
      "Gradient Descent(1907/9999): loss=0.04443428392713171\n",
      "Gradient Descent(1908/9999): loss=0.044433549875122214\n",
      "Gradient Descent(1909/9999): loss=0.04443281697728699\n",
      "Gradient Descent(1910/9999): loss=0.04443208523181125\n",
      "Gradient Descent(1911/9999): loss=0.044431354636883136\n",
      "Gradient Descent(1912/9999): loss=0.04443062519069361\n",
      "Gradient Descent(1913/9999): loss=0.04442989689143644\n",
      "Gradient Descent(1914/9999): loss=0.04442916973730829\n",
      "Gradient Descent(1915/9999): loss=0.04442844372650864\n",
      "Gradient Descent(1916/9999): loss=0.04442771885723979\n",
      "Gradient Descent(1917/9999): loss=0.04442699512770686\n",
      "Gradient Descent(1918/9999): loss=0.044426272536117806\n",
      "Gradient Descent(1919/9999): loss=0.04442555108068343\n",
      "Gradient Descent(1920/9999): loss=0.04442483075961732\n",
      "Gradient Descent(1921/9999): loss=0.04442411157113581\n",
      "Gradient Descent(1922/9999): loss=0.04442339351345817\n",
      "Gradient Descent(1923/9999): loss=0.04442267658480634\n",
      "Gradient Descent(1924/9999): loss=0.04442196078340517\n",
      "Gradient Descent(1925/9999): loss=0.04442124610748222\n",
      "Gradient Descent(1926/9999): loss=0.04442053255526786\n",
      "Gradient Descent(1927/9999): loss=0.04441982012499526\n",
      "Gradient Descent(1928/9999): loss=0.044419108814900354\n",
      "Gradient Descent(1929/9999): loss=0.04441839862322183\n",
      "Gradient Descent(1930/9999): loss=0.044417689548201175\n",
      "Gradient Descent(1931/9999): loss=0.04441698158808263\n",
      "Gradient Descent(1932/9999): loss=0.04441627474111321\n",
      "Gradient Descent(1933/9999): loss=0.044415569005542645\n",
      "Gradient Descent(1934/9999): loss=0.044414864379623484\n",
      "Gradient Descent(1935/9999): loss=0.044414160861610937\n",
      "Gradient Descent(1936/9999): loss=0.04441345844976307\n",
      "Gradient Descent(1937/9999): loss=0.04441275714234058\n",
      "Gradient Descent(1938/9999): loss=0.04441205693760694\n",
      "Gradient Descent(1939/9999): loss=0.044411357833828366\n",
      "Gradient Descent(1940/9999): loss=0.04441065982927378\n",
      "Gradient Descent(1941/9999): loss=0.04440996292221488\n",
      "Gradient Descent(1942/9999): loss=0.04440926711092599\n",
      "Gradient Descent(1943/9999): loss=0.0444085723936842\n",
      "Gradient Descent(1944/9999): loss=0.044407878768769296\n",
      "Gradient Descent(1945/9999): loss=0.044407186234463804\n",
      "Gradient Descent(1946/9999): loss=0.044406494789052925\n",
      "Gradient Descent(1947/9999): loss=0.04440580443082453\n",
      "Gradient Descent(1948/9999): loss=0.04440511515806921\n",
      "Gradient Descent(1949/9999): loss=0.04440442696908024\n",
      "Gradient Descent(1950/9999): loss=0.044403739862153596\n",
      "Gradient Descent(1951/9999): loss=0.044403053835587916\n",
      "Gradient Descent(1952/9999): loss=0.04440236888768449\n",
      "Gradient Descent(1953/9999): loss=0.0444016850167473\n",
      "Gradient Descent(1954/9999): loss=0.044401002221083015\n",
      "Gradient Descent(1955/9999): loss=0.044400320499000945\n",
      "Gradient Descent(1956/9999): loss=0.04439963984881303\n",
      "Gradient Descent(1957/9999): loss=0.04439896026883397\n",
      "Gradient Descent(1958/9999): loss=0.04439828175738094\n",
      "Gradient Descent(1959/9999): loss=0.044397604312773974\n",
      "Gradient Descent(1960/9999): loss=0.04439692793333552\n",
      "Gradient Descent(1961/9999): loss=0.04439625261739086\n",
      "Gradient Descent(1962/9999): loss=0.04439557836326778\n",
      "Gradient Descent(1963/9999): loss=0.04439490516929679\n",
      "Gradient Descent(1964/9999): loss=0.04439423303381093\n",
      "Gradient Descent(1965/9999): loss=0.04439356195514594\n",
      "Gradient Descent(1966/9999): loss=0.04439289193164013\n",
      "Gradient Descent(1967/9999): loss=0.044392222961634434\n",
      "Gradient Descent(1968/9999): loss=0.044391555043472426\n",
      "Gradient Descent(1969/9999): loss=0.04439088817550022\n",
      "Gradient Descent(1970/9999): loss=0.04439022235606662\n",
      "Gradient Descent(1971/9999): loss=0.044389557583522914\n",
      "Gradient Descent(1972/9999): loss=0.04438889385622307\n",
      "Gradient Descent(1973/9999): loss=0.044388231172523655\n",
      "Gradient Descent(1974/9999): loss=0.0443875695307837\n",
      "Gradient Descent(1975/9999): loss=0.04438690892936494\n",
      "Gradient Descent(1976/9999): loss=0.04438624936663167\n",
      "Gradient Descent(1977/9999): loss=0.044385590840950694\n",
      "Gradient Descent(1978/9999): loss=0.04438493335069142\n",
      "Gradient Descent(1979/9999): loss=0.044384276894225846\n",
      "Gradient Descent(1980/9999): loss=0.044383621469928476\n",
      "Gradient Descent(1981/9999): loss=0.04438296707617643\n",
      "Gradient Descent(1982/9999): loss=0.044382313711349326\n",
      "Gradient Descent(1983/9999): loss=0.04438166137382933\n",
      "Gradient Descent(1984/9999): loss=0.04438101006200124\n",
      "Gradient Descent(1985/9999): loss=0.04438035977425225\n",
      "Gradient Descent(1986/9999): loss=0.04437971050897219\n",
      "Gradient Descent(1987/9999): loss=0.04437906226455344\n",
      "Gradient Descent(1988/9999): loss=0.0443784150393908\n",
      "Gradient Descent(1989/9999): loss=0.04437776883188172\n",
      "Gradient Descent(1990/9999): loss=0.04437712364042609\n",
      "Gradient Descent(1991/9999): loss=0.04437647946342629\n",
      "Gradient Descent(1992/9999): loss=0.04437583629928735\n",
      "Gradient Descent(1993/9999): loss=0.04437519414641662\n",
      "Gradient Descent(1994/9999): loss=0.04437455300322411\n",
      "Gradient Descent(1995/9999): loss=0.04437391286812224\n",
      "Gradient Descent(1996/9999): loss=0.04437327373952599\n",
      "Gradient Descent(1997/9999): loss=0.04437263561585274\n",
      "Gradient Descent(1998/9999): loss=0.044371998495522494\n",
      "Gradient Descent(1999/9999): loss=0.044371362376957595\n",
      "Gradient Descent(2000/9999): loss=0.04437072725858297\n",
      "Gradient Descent(2001/9999): loss=0.044370093138825964\n",
      "Gradient Descent(2002/9999): loss=0.04436946001611642\n",
      "Gradient Descent(2003/9999): loss=0.0443688278888867\n",
      "Gradient Descent(2004/9999): loss=0.04436819675557149\n",
      "Gradient Descent(2005/9999): loss=0.044367566614608084\n",
      "Gradient Descent(2006/9999): loss=0.04436693746443615\n",
      "Gradient Descent(2007/9999): loss=0.044366309303497886\n",
      "Gradient Descent(2008/9999): loss=0.04436568213023783\n",
      "Gradient Descent(2009/9999): loss=0.044365055943103006\n",
      "Gradient Descent(2010/9999): loss=0.04436443074054296\n",
      "Gradient Descent(2011/9999): loss=0.04436380652100957\n",
      "Gradient Descent(2012/9999): loss=0.04436318328295718\n",
      "Gradient Descent(2013/9999): loss=0.04436256102484263\n",
      "Gradient Descent(2014/9999): loss=0.04436193974512509\n",
      "Gradient Descent(2015/9999): loss=0.0443613194422662\n",
      "Gradient Descent(2016/9999): loss=0.04436070011473002\n",
      "Gradient Descent(2017/9999): loss=0.044360081760983015\n",
      "Gradient Descent(2018/9999): loss=0.04435946437949407\n",
      "Gradient Descent(2019/9999): loss=0.04435884796873444\n",
      "Gradient Descent(2020/9999): loss=0.044358232527177854\n",
      "Gradient Descent(2021/9999): loss=0.044357618053300386\n",
      "Gradient Descent(2022/9999): loss=0.04435700454558053\n",
      "Gradient Descent(2023/9999): loss=0.04435639200249913\n",
      "Gradient Descent(2024/9999): loss=0.0443557804225395\n",
      "Gradient Descent(2025/9999): loss=0.04435516980418728\n",
      "Gradient Descent(2026/9999): loss=0.04435456014593049\n",
      "Gradient Descent(2027/9999): loss=0.044353951446259525\n",
      "Gradient Descent(2028/9999): loss=0.04435334370366723\n",
      "Gradient Descent(2029/9999): loss=0.04435273691664869\n",
      "Gradient Descent(2030/9999): loss=0.04435213108370145\n",
      "Gradient Descent(2031/9999): loss=0.04435152620332541\n",
      "Gradient Descent(2032/9999): loss=0.04435092227402279\n",
      "Gradient Descent(2033/9999): loss=0.04435031929429819\n",
      "Gradient Descent(2034/9999): loss=0.044349717262658565\n",
      "Gradient Descent(2035/9999): loss=0.04434911617761319\n",
      "Gradient Descent(2036/9999): loss=0.04434851603767372\n",
      "Gradient Descent(2037/9999): loss=0.04434791684135412\n",
      "Gradient Descent(2038/9999): loss=0.044347318587170725\n",
      "Gradient Descent(2039/9999): loss=0.04434672127364218\n",
      "Gradient Descent(2040/9999): loss=0.044346124899289456\n",
      "Gradient Descent(2041/9999): loss=0.04434552946263585\n",
      "Gradient Descent(2042/9999): loss=0.04434493496220697\n",
      "Gradient Descent(2043/9999): loss=0.044344341396530824\n",
      "Gradient Descent(2044/9999): loss=0.044343748764137596\n",
      "Gradient Descent(2045/9999): loss=0.044343157063559915\n",
      "Gradient Descent(2046/9999): loss=0.04434256629333261\n",
      "Gradient Descent(2047/9999): loss=0.04434197645199287\n",
      "Gradient Descent(2048/9999): loss=0.044341387538080174\n",
      "Gradient Descent(2049/9999): loss=0.04434079955013633\n",
      "Gradient Descent(2050/9999): loss=0.044340212486705353\n",
      "Gradient Descent(2051/9999): loss=0.04433962634633365\n",
      "Gradient Descent(2052/9999): loss=0.04433904112756983\n",
      "Gradient Descent(2053/9999): loss=0.044338456828964846\n",
      "Gradient Descent(2054/9999): loss=0.0443378734490719\n",
      "Gradient Descent(2055/9999): loss=0.04433729098644643\n",
      "Gradient Descent(2056/9999): loss=0.044336709439646224\n",
      "Gradient Descent(2057/9999): loss=0.044336128807231284\n",
      "Gradient Descent(2058/9999): loss=0.04433554908776391\n",
      "Gradient Descent(2059/9999): loss=0.044334970279808636\n",
      "Gradient Descent(2060/9999): loss=0.04433439238193224\n",
      "Gradient Descent(2061/9999): loss=0.0443338153927038\n",
      "Gradient Descent(2062/9999): loss=0.04433323931069463\n",
      "Gradient Descent(2063/9999): loss=0.044332664134478256\n",
      "Gradient Descent(2064/9999): loss=0.04433208986263043\n",
      "Gradient Descent(2065/9999): loss=0.044331516493729274\n",
      "Gradient Descent(2066/9999): loss=0.044330944026355\n",
      "Gradient Descent(2067/9999): loss=0.04433037245909011\n",
      "Gradient Descent(2068/9999): loss=0.04432980179051934\n",
      "Gradient Descent(2069/9999): loss=0.04432923201922966\n",
      "Gradient Descent(2070/9999): loss=0.04432866314381021\n",
      "Gradient Descent(2071/9999): loss=0.04432809516285241\n",
      "Gradient Descent(2072/9999): loss=0.044327528074949836\n",
      "Gradient Descent(2073/9999): loss=0.04432696187869836\n",
      "Gradient Descent(2074/9999): loss=0.04432639657269597\n",
      "Gradient Descent(2075/9999): loss=0.044325832155542894\n",
      "Gradient Descent(2076/9999): loss=0.04432526862584158\n",
      "Gradient Descent(2077/9999): loss=0.0443247059821967\n",
      "Gradient Descent(2078/9999): loss=0.04432414422321499\n",
      "Gradient Descent(2079/9999): loss=0.044323583347505524\n",
      "Gradient Descent(2080/9999): loss=0.04432302335367949\n",
      "Gradient Descent(2081/9999): loss=0.044322464240350294\n",
      "Gradient Descent(2082/9999): loss=0.04432190600613346\n",
      "Gradient Descent(2083/9999): loss=0.044321348649646757\n",
      "Gradient Descent(2084/9999): loss=0.04432079216951009\n",
      "Gradient Descent(2085/9999): loss=0.04432023656434555\n",
      "Gradient Descent(2086/9999): loss=0.04431968183277743\n",
      "Gradient Descent(2087/9999): loss=0.04431912797343207\n",
      "Gradient Descent(2088/9999): loss=0.04431857498493807\n",
      "Gradient Descent(2089/9999): loss=0.04431802286592621\n",
      "Gradient Descent(2090/9999): loss=0.04431747161502928\n",
      "Gradient Descent(2091/9999): loss=0.04431692123088242\n",
      "Gradient Descent(2092/9999): loss=0.04431637171212273\n",
      "Gradient Descent(2093/9999): loss=0.0443158230573896\n",
      "Gradient Descent(2094/9999): loss=0.04431527526532442\n",
      "Gradient Descent(2095/9999): loss=0.04431472833457086\n",
      "Gradient Descent(2096/9999): loss=0.04431418226377461\n",
      "Gradient Descent(2097/9999): loss=0.04431363705158353\n",
      "Gradient Descent(2098/9999): loss=0.04431309269664761\n",
      "Gradient Descent(2099/9999): loss=0.044312549197618976\n",
      "Gradient Descent(2100/9999): loss=0.04431200655315181\n",
      "Gradient Descent(2101/9999): loss=0.044311464761902536\n",
      "Gradient Descent(2102/9999): loss=0.04431092382252953\n",
      "Gradient Descent(2103/9999): loss=0.044310383733693434\n",
      "Gradient Descent(2104/9999): loss=0.04430984449405686\n",
      "Gradient Descent(2105/9999): loss=0.04430930610228462\n",
      "Gradient Descent(2106/9999): loss=0.04430876855704354\n",
      "Gradient Descent(2107/9999): loss=0.04430823185700264\n",
      "Gradient Descent(2108/9999): loss=0.04430769600083298\n",
      "Gradient Descent(2109/9999): loss=0.044307160987207696\n",
      "Gradient Descent(2110/9999): loss=0.04430662681480205\n",
      "Gradient Descent(2111/9999): loss=0.044306093482293335\n",
      "Gradient Descent(2112/9999): loss=0.044305560988360956\n",
      "Gradient Descent(2113/9999): loss=0.04430502933168642\n",
      "Gradient Descent(2114/9999): loss=0.04430449851095325\n",
      "Gradient Descent(2115/9999): loss=0.044303968524847075\n",
      "Gradient Descent(2116/9999): loss=0.0443034393720556\n",
      "Gradient Descent(2117/9999): loss=0.04430291105126855\n",
      "Gradient Descent(2118/9999): loss=0.044302383561177765\n",
      "Gradient Descent(2119/9999): loss=0.044301856900477096\n",
      "Gradient Descent(2120/9999): loss=0.04430133106786247\n",
      "Gradient Descent(2121/9999): loss=0.044300806062031836\n",
      "Gradient Descent(2122/9999): loss=0.04430028188168527\n",
      "Gradient Descent(2123/9999): loss=0.04429975852552479\n",
      "Gradient Descent(2124/9999): loss=0.04429923599225452\n",
      "Gradient Descent(2125/9999): loss=0.0442987142805806\n",
      "Gradient Descent(2126/9999): loss=0.0442981933892112\n",
      "Gradient Descent(2127/9999): loss=0.04429767331685652\n",
      "Gradient Descent(2128/9999): loss=0.044297154062228825\n",
      "Gradient Descent(2129/9999): loss=0.04429663562404234\n",
      "Gradient Descent(2130/9999): loss=0.0442961180010134\n",
      "Gradient Descent(2131/9999): loss=0.04429560119186023\n",
      "Gradient Descent(2132/9999): loss=0.04429508519530322\n",
      "Gradient Descent(2133/9999): loss=0.04429457001006464\n",
      "Gradient Descent(2134/9999): loss=0.04429405563486887\n",
      "Gradient Descent(2135/9999): loss=0.04429354206844224\n",
      "Gradient Descent(2136/9999): loss=0.04429302930951308\n",
      "Gradient Descent(2137/9999): loss=0.044292517356811764\n",
      "Gradient Descent(2138/9999): loss=0.044292006209070596\n",
      "Gradient Descent(2139/9999): loss=0.04429149586502395\n",
      "Gradient Descent(2140/9999): loss=0.0442909863234081\n",
      "Gradient Descent(2141/9999): loss=0.04429047758296143\n",
      "Gradient Descent(2142/9999): loss=0.04428996964242416\n",
      "Gradient Descent(2143/9999): loss=0.04428946250053864\n",
      "Gradient Descent(2144/9999): loss=0.044288956156049046\n",
      "Gradient Descent(2145/9999): loss=0.044288450607701646\n",
      "Gradient Descent(2146/9999): loss=0.04428794585424465\n",
      "Gradient Descent(2147/9999): loss=0.04428744189442822\n",
      "Gradient Descent(2148/9999): loss=0.044286938727004455\n",
      "Gradient Descent(2149/9999): loss=0.0442864363507275\n",
      "Gradient Descent(2150/9999): loss=0.044285934764353385\n",
      "Gradient Descent(2151/9999): loss=0.044285433966640114\n",
      "Gradient Descent(2152/9999): loss=0.04428493395634764\n",
      "Gradient Descent(2153/9999): loss=0.04428443473223789\n",
      "Gradient Descent(2154/9999): loss=0.044283936293074734\n",
      "Gradient Descent(2155/9999): loss=0.044283438637623955\n",
      "Gradient Descent(2156/9999): loss=0.044282941764653326\n",
      "Gradient Descent(2157/9999): loss=0.044282445672932484\n",
      "Gradient Descent(2158/9999): loss=0.04428195036123308\n",
      "Gradient Descent(2159/9999): loss=0.044281455828328625\n",
      "Gradient Descent(2160/9999): loss=0.04428096207299464\n",
      "Gradient Descent(2161/9999): loss=0.044280469094008505\n",
      "Gradient Descent(2162/9999): loss=0.044279976890149515\n",
      "Gradient Descent(2163/9999): loss=0.04427948546019895\n",
      "Gradient Descent(2164/9999): loss=0.04427899480293993\n",
      "Gradient Descent(2165/9999): loss=0.04427850491715756\n",
      "Gradient Descent(2166/9999): loss=0.044278015801638836\n",
      "Gradient Descent(2167/9999): loss=0.04427752745517261\n",
      "Gradient Descent(2168/9999): loss=0.044277039876549684\n",
      "Gradient Descent(2169/9999): loss=0.04427655306456277\n",
      "Gradient Descent(2170/9999): loss=0.044276067018006425\n",
      "Gradient Descent(2171/9999): loss=0.04427558173567716\n",
      "Gradient Descent(2172/9999): loss=0.04427509721637336\n",
      "Gradient Descent(2173/9999): loss=0.0442746134588953\n",
      "Gradient Descent(2174/9999): loss=0.044274130462045126\n",
      "Gradient Descent(2175/9999): loss=0.04427364822462687\n",
      "Gradient Descent(2176/9999): loss=0.044273166745446464\n",
      "Gradient Descent(2177/9999): loss=0.044272686023311685\n",
      "Gradient Descent(2178/9999): loss=0.04427220605703225\n",
      "Gradient Descent(2179/9999): loss=0.04427172684541966\n",
      "Gradient Descent(2180/9999): loss=0.044271248387287326\n",
      "Gradient Descent(2181/9999): loss=0.04427077068145056\n",
      "Gradient Descent(2182/9999): loss=0.04427029372672649\n",
      "Gradient Descent(2183/9999): loss=0.04426981752193408\n",
      "Gradient Descent(2184/9999): loss=0.04426934206589425\n",
      "Gradient Descent(2185/9999): loss=0.044268867357429686\n",
      "Gradient Descent(2186/9999): loss=0.044268393395364924\n",
      "Gradient Descent(2187/9999): loss=0.04426792017852639\n",
      "Gradient Descent(2188/9999): loss=0.044267447705742355\n",
      "Gradient Descent(2189/9999): loss=0.04426697597584293\n",
      "Gradient Descent(2190/9999): loss=0.04426650498766001\n",
      "Gradient Descent(2191/9999): loss=0.0442660347400274\n",
      "Gradient Descent(2192/9999): loss=0.04426556523178067\n",
      "Gradient Descent(2193/9999): loss=0.04426509646175731\n",
      "Gradient Descent(2194/9999): loss=0.044264628428796575\n",
      "Gradient Descent(2195/9999): loss=0.04426416113173953\n",
      "Gradient Descent(2196/9999): loss=0.04426369456942911\n",
      "Gradient Descent(2197/9999): loss=0.044263228740710064\n",
      "Gradient Descent(2198/9999): loss=0.04426276364442889\n",
      "Gradient Descent(2199/9999): loss=0.044262299279434016\n",
      "Gradient Descent(2200/9999): loss=0.04426183564457556\n",
      "Gradient Descent(2201/9999): loss=0.044261372738705555\n",
      "Gradient Descent(2202/9999): loss=0.04426091056067775\n",
      "Gradient Descent(2203/9999): loss=0.04426044910934776\n",
      "Gradient Descent(2204/9999): loss=0.044259988383572944\n",
      "Gradient Descent(2205/9999): loss=0.044259528382212504\n",
      "Gradient Descent(2206/9999): loss=0.04425906910412743\n",
      "Gradient Descent(2207/9999): loss=0.044258610548180456\n",
      "Gradient Descent(2208/9999): loss=0.04425815271323621\n",
      "Gradient Descent(2209/9999): loss=0.04425769559816097\n",
      "Gradient Descent(2210/9999): loss=0.04425723920182289\n",
      "Gradient Descent(2211/9999): loss=0.044256783523091876\n",
      "Gradient Descent(2212/9999): loss=0.04425632856083958\n",
      "Gradient Descent(2213/9999): loss=0.04425587431393953\n",
      "Gradient Descent(2214/9999): loss=0.04425542078126687\n",
      "Gradient Descent(2215/9999): loss=0.044254967961698644\n",
      "Gradient Descent(2216/9999): loss=0.0442545158541136\n",
      "Gradient Descent(2217/9999): loss=0.04425406445739227\n",
      "Gradient Descent(2218/9999): loss=0.044253613770416936\n",
      "Gradient Descent(2219/9999): loss=0.04425316379207165\n",
      "Gradient Descent(2220/9999): loss=0.044252714521242215\n",
      "Gradient Descent(2221/9999): loss=0.044252265956816146\n",
      "Gradient Descent(2222/9999): loss=0.04425181809768275\n",
      "Gradient Descent(2223/9999): loss=0.04425137094273313\n",
      "Gradient Descent(2224/9999): loss=0.044250924490859996\n",
      "Gradient Descent(2225/9999): loss=0.04425047874095792\n",
      "Gradient Descent(2226/9999): loss=0.04425003369192316\n",
      "Gradient Descent(2227/9999): loss=0.04424958934265372\n",
      "Gradient Descent(2228/9999): loss=0.04424914569204935\n",
      "Gradient Descent(2229/9999): loss=0.04424870273901149\n",
      "Gradient Descent(2230/9999): loss=0.04424826048244333\n",
      "Gradient Descent(2231/9999): loss=0.04424781892124982\n",
      "Gradient Descent(2232/9999): loss=0.04424737805433757\n",
      "Gradient Descent(2233/9999): loss=0.044246937880614956\n",
      "Gradient Descent(2234/9999): loss=0.044246498398992064\n",
      "Gradient Descent(2235/9999): loss=0.04424605960838064\n",
      "Gradient Descent(2236/9999): loss=0.04424562150769423\n",
      "Gradient Descent(2237/9999): loss=0.04424518409584803\n",
      "Gradient Descent(2238/9999): loss=0.04424474737175894\n",
      "Gradient Descent(2239/9999): loss=0.04424431133434555\n",
      "Gradient Descent(2240/9999): loss=0.04424387598252824\n",
      "Gradient Descent(2241/9999): loss=0.044243441315229\n",
      "Gradient Descent(2242/9999): loss=0.04424300733137151\n",
      "Gradient Descent(2243/9999): loss=0.04424257402988121\n",
      "Gradient Descent(2244/9999): loss=0.04424214140968519\n",
      "Gradient Descent(2245/9999): loss=0.04424170946971219\n",
      "Gradient Descent(2246/9999): loss=0.04424127820889272\n",
      "Gradient Descent(2247/9999): loss=0.04424084762615888\n",
      "Gradient Descent(2248/9999): loss=0.04424041772044452\n",
      "Gradient Descent(2249/9999): loss=0.04423998849068514\n",
      "Gradient Descent(2250/9999): loss=0.044239559935817935\n",
      "Gradient Descent(2251/9999): loss=0.044239132054781694\n",
      "Gradient Descent(2252/9999): loss=0.044238704846516985\n",
      "Gradient Descent(2253/9999): loss=0.044238278309965955\n",
      "Gradient Descent(2254/9999): loss=0.04423785244407244\n",
      "Gradient Descent(2255/9999): loss=0.04423742724778197\n",
      "Gradient Descent(2256/9999): loss=0.044237002720041704\n",
      "Gradient Descent(2257/9999): loss=0.04423657885980044\n",
      "Gradient Descent(2258/9999): loss=0.04423615566600865\n",
      "Gradient Descent(2259/9999): loss=0.04423573313761845\n",
      "Gradient Descent(2260/9999): loss=0.04423531127358363\n",
      "Gradient Descent(2261/9999): loss=0.04423489007285958\n",
      "Gradient Descent(2262/9999): loss=0.04423446953440337\n",
      "Gradient Descent(2263/9999): loss=0.04423404965717368\n",
      "Gradient Descent(2264/9999): loss=0.04423363044013084\n",
      "Gradient Descent(2265/9999): loss=0.04423321188223686\n",
      "Gradient Descent(2266/9999): loss=0.044232793982455255\n",
      "Gradient Descent(2267/9999): loss=0.044232376739751354\n",
      "Gradient Descent(2268/9999): loss=0.04423196015309192\n",
      "Gradient Descent(2269/9999): loss=0.04423154422144547\n",
      "Gradient Descent(2270/9999): loss=0.04423112894378213\n",
      "Gradient Descent(2271/9999): loss=0.04423071431907359\n",
      "Gradient Descent(2272/9999): loss=0.04423030034629319\n",
      "Gradient Descent(2273/9999): loss=0.04422988702441589\n",
      "Gradient Descent(2274/9999): loss=0.04422947435241825\n",
      "Gradient Descent(2275/9999): loss=0.044229062329278424\n",
      "Gradient Descent(2276/9999): loss=0.04422865095397627\n",
      "Gradient Descent(2277/9999): loss=0.04422824022549308\n",
      "Gradient Descent(2278/9999): loss=0.04422783014281185\n",
      "Gradient Descent(2279/9999): loss=0.044227420704917204\n",
      "Gradient Descent(2280/9999): loss=0.0442270119107953\n",
      "Gradient Descent(2281/9999): loss=0.04422660375943392\n",
      "Gradient Descent(2282/9999): loss=0.04422619624982242\n",
      "Gradient Descent(2283/9999): loss=0.04422578938095178\n",
      "Gradient Descent(2284/9999): loss=0.0442253831518145\n",
      "Gradient Descent(2285/9999): loss=0.044224977561404755\n",
      "Gradient Descent(2286/9999): loss=0.044224572608718205\n",
      "Gradient Descent(2287/9999): loss=0.04422416829275219\n",
      "Gradient Descent(2288/9999): loss=0.04422376461250554\n",
      "Gradient Descent(2289/9999): loss=0.044223361566978696\n",
      "Gradient Descent(2290/9999): loss=0.044222959155173644\n",
      "Gradient Descent(2291/9999): loss=0.044222557376093985\n",
      "Gradient Descent(2292/9999): loss=0.044222156228744904\n",
      "Gradient Descent(2293/9999): loss=0.04422175571213303\n",
      "Gradient Descent(2294/9999): loss=0.04422135582526668\n",
      "Gradient Descent(2295/9999): loss=0.04422095656715569\n",
      "Gradient Descent(2296/9999): loss=0.04422055793681144\n",
      "Gradient Descent(2297/9999): loss=0.04422015993324686\n",
      "Gradient Descent(2298/9999): loss=0.04421976255547645\n",
      "Gradient Descent(2299/9999): loss=0.044219365802516265\n",
      "Gradient Descent(2300/9999): loss=0.044218969673383866\n",
      "Gradient Descent(2301/9999): loss=0.04421857416709841\n",
      "Gradient Descent(2302/9999): loss=0.04421817928268059\n",
      "Gradient Descent(2303/9999): loss=0.044217785019152585\n",
      "Gradient Descent(2304/9999): loss=0.04421739137553816\n",
      "Gradient Descent(2305/9999): loss=0.04421699835086264\n",
      "Gradient Descent(2306/9999): loss=0.0442166059441528\n",
      "Gradient Descent(2307/9999): loss=0.04421621415443702\n",
      "Gradient Descent(2308/9999): loss=0.044215822980745154\n",
      "Gradient Descent(2309/9999): loss=0.04421543242210862\n",
      "Gradient Descent(2310/9999): loss=0.044215042477560404\n",
      "Gradient Descent(2311/9999): loss=0.04421465314613484\n",
      "Gradient Descent(2312/9999): loss=0.04421426442686799\n",
      "Gradient Descent(2313/9999): loss=0.04421387631879729\n",
      "Gradient Descent(2314/9999): loss=0.044213488820961745\n",
      "Gradient Descent(2315/9999): loss=0.04421310193240187\n",
      "Gradient Descent(2316/9999): loss=0.04421271565215968\n",
      "Gradient Descent(2317/9999): loss=0.04421232997927869\n",
      "Gradient Descent(2318/9999): loss=0.04421194491280394\n",
      "Gradient Descent(2319/9999): loss=0.044211560451781935\n",
      "Gradient Descent(2320/9999): loss=0.04421117659526073\n",
      "Gradient Descent(2321/9999): loss=0.044210793342289806\n",
      "Gradient Descent(2322/9999): loss=0.044210410691920214\n",
      "Gradient Descent(2323/9999): loss=0.04421002864320449\n",
      "Gradient Descent(2324/9999): loss=0.04420964719519655\n",
      "Gradient Descent(2325/9999): loss=0.04420926634695193\n",
      "Gradient Descent(2326/9999): loss=0.04420888609752764\n",
      "Gradient Descent(2327/9999): loss=0.04420850644598209\n",
      "Gradient Descent(2328/9999): loss=0.04420812739137522\n",
      "Gradient Descent(2329/9999): loss=0.04420774893276846\n",
      "Gradient Descent(2330/9999): loss=0.04420737106922467\n",
      "Gradient Descent(2331/9999): loss=0.044206993799808234\n",
      "Gradient Descent(2332/9999): loss=0.044206617123584986\n",
      "Gradient Descent(2333/9999): loss=0.04420624103962222\n",
      "Gradient Descent(2334/9999): loss=0.04420586554698871\n",
      "Gradient Descent(2335/9999): loss=0.044205490644754715\n",
      "Gradient Descent(2336/9999): loss=0.044205116331991874\n",
      "Gradient Descent(2337/9999): loss=0.04420474260777338\n",
      "Gradient Descent(2338/9999): loss=0.04420436947117383\n",
      "Gradient Descent(2339/9999): loss=0.044203996921269306\n",
      "Gradient Descent(2340/9999): loss=0.04420362495713734\n",
      "Gradient Descent(2341/9999): loss=0.044203253577856864\n",
      "Gradient Descent(2342/9999): loss=0.04420288278250832\n",
      "Gradient Descent(2343/9999): loss=0.044202512570173574\n",
      "Gradient Descent(2344/9999): loss=0.04420214293993591\n",
      "Gradient Descent(2345/9999): loss=0.04420177389088012\n",
      "Gradient Descent(2346/9999): loss=0.04420140542209237\n",
      "Gradient Descent(2347/9999): loss=0.04420103753266027\n",
      "Gradient Descent(2348/9999): loss=0.04420067022167288\n",
      "Gradient Descent(2349/9999): loss=0.04420030348822073\n",
      "Gradient Descent(2350/9999): loss=0.04419993733139569\n",
      "Gradient Descent(2351/9999): loss=0.04419957175029118\n",
      "Gradient Descent(2352/9999): loss=0.04419920674400193\n",
      "Gradient Descent(2353/9999): loss=0.04419884231162412\n",
      "Gradient Descent(2354/9999): loss=0.0441984784522554\n",
      "Gradient Descent(2355/9999): loss=0.044198115164994826\n",
      "Gradient Descent(2356/9999): loss=0.04419775244894279\n",
      "Gradient Descent(2357/9999): loss=0.044197390303201224\n",
      "Gradient Descent(2358/9999): loss=0.0441970287268734\n",
      "Gradient Descent(2359/9999): loss=0.04419666771906397\n",
      "Gradient Descent(2360/9999): loss=0.04419630727887908\n",
      "Gradient Descent(2361/9999): loss=0.04419594740542621\n",
      "Gradient Descent(2362/9999): loss=0.04419558809781428\n",
      "Gradient Descent(2363/9999): loss=0.04419522935515358\n",
      "Gradient Descent(2364/9999): loss=0.04419487117655585\n",
      "Gradient Descent(2365/9999): loss=0.044194513561134185\n",
      "Gradient Descent(2366/9999): loss=0.04419415650800306\n",
      "Gradient Descent(2367/9999): loss=0.044193800016278394\n",
      "Gradient Descent(2368/9999): loss=0.044193444085077443\n",
      "Gradient Descent(2369/9999): loss=0.0441930887135189\n",
      "Gradient Descent(2370/9999): loss=0.04419273390072285\n",
      "Gradient Descent(2371/9999): loss=0.04419237964581066\n",
      "Gradient Descent(2372/9999): loss=0.044192025947905195\n",
      "Gradient Descent(2373/9999): loss=0.04419167280613066\n",
      "Gradient Descent(2374/9999): loss=0.04419132021961259\n",
      "Gradient Descent(2375/9999): loss=0.04419096818747799\n",
      "Gradient Descent(2376/9999): loss=0.04419061670885514\n",
      "Gradient Descent(2377/9999): loss=0.04419026578287378\n",
      "Gradient Descent(2378/9999): loss=0.04418991540866496\n",
      "Gradient Descent(2379/9999): loss=0.04418956558536108\n",
      "Gradient Descent(2380/9999): loss=0.04418921631209592\n",
      "Gradient Descent(2381/9999): loss=0.04418886758800471\n",
      "Gradient Descent(2382/9999): loss=0.04418851941222393\n",
      "Gradient Descent(2383/9999): loss=0.04418817178389144\n",
      "Gradient Descent(2384/9999): loss=0.044187824702146467\n",
      "Gradient Descent(2385/9999): loss=0.044187478166129626\n",
      "Gradient Descent(2386/9999): loss=0.04418713217498281\n",
      "Gradient Descent(2387/9999): loss=0.04418678672784931\n",
      "Gradient Descent(2388/9999): loss=0.04418644182387379\n",
      "Gradient Descent(2389/9999): loss=0.04418609746220217\n",
      "Gradient Descent(2390/9999): loss=0.04418575364198182\n",
      "Gradient Descent(2391/9999): loss=0.04418541036236138\n",
      "Gradient Descent(2392/9999): loss=0.04418506762249084\n",
      "Gradient Descent(2393/9999): loss=0.04418472542152152\n",
      "Gradient Descent(2394/9999): loss=0.04418438375860614\n",
      "Gradient Descent(2395/9999): loss=0.044184042632898636\n",
      "Gradient Descent(2396/9999): loss=0.04418370204355437\n",
      "Gradient Descent(2397/9999): loss=0.04418336198972999\n",
      "Gradient Descent(2398/9999): loss=0.044183022470583505\n",
      "Gradient Descent(2399/9999): loss=0.044182683485274196\n",
      "Gradient Descent(2400/9999): loss=0.04418234503296272\n",
      "Gradient Descent(2401/9999): loss=0.04418200711281099\n",
      "Gradient Descent(2402/9999): loss=0.04418166972398228\n",
      "Gradient Descent(2403/9999): loss=0.04418133286564123\n",
      "Gradient Descent(2404/9999): loss=0.04418099653695366\n",
      "Gradient Descent(2405/9999): loss=0.044180660737086826\n",
      "Gradient Descent(2406/9999): loss=0.04418032546520921\n",
      "Gradient Descent(2407/9999): loss=0.04417999072049068\n",
      "Gradient Descent(2408/9999): loss=0.044179656502102344\n",
      "Gradient Descent(2409/9999): loss=0.04417932280921662\n",
      "Gradient Descent(2410/9999): loss=0.04417898964100729\n",
      "Gradient Descent(2411/9999): loss=0.04417865699664933\n",
      "Gradient Descent(2412/9999): loss=0.04417832487531911\n",
      "Gradient Descent(2413/9999): loss=0.04417799327619422\n",
      "Gradient Descent(2414/9999): loss=0.04417766219845363\n",
      "Gradient Descent(2415/9999): loss=0.044177331641277524\n",
      "Gradient Descent(2416/9999): loss=0.0441770016038474\n",
      "Gradient Descent(2417/9999): loss=0.04417667208534606\n",
      "Gradient Descent(2418/9999): loss=0.04417634308495752\n",
      "Gradient Descent(2419/9999): loss=0.0441760146018672\n",
      "Gradient Descent(2420/9999): loss=0.0441756866352617\n",
      "Gradient Descent(2421/9999): loss=0.04417535918432895\n",
      "Gradient Descent(2422/9999): loss=0.04417503224825813\n",
      "Gradient Descent(2423/9999): loss=0.04417470582623971\n",
      "Gradient Descent(2424/9999): loss=0.044174379917465444\n",
      "Gradient Descent(2425/9999): loss=0.04417405452112832\n",
      "Gradient Descent(2426/9999): loss=0.044173729636422604\n",
      "Gradient Descent(2427/9999): loss=0.04417340526254387\n",
      "Gradient Descent(2428/9999): loss=0.04417308139868892\n",
      "Gradient Descent(2429/9999): loss=0.044172758044055814\n",
      "Gradient Descent(2430/9999): loss=0.04417243519784387\n",
      "Gradient Descent(2431/9999): loss=0.04417211285925375\n",
      "Gradient Descent(2432/9999): loss=0.04417179102748723\n",
      "Gradient Descent(2433/9999): loss=0.04417146970174745\n",
      "Gradient Descent(2434/9999): loss=0.0441711488812388\n",
      "Gradient Descent(2435/9999): loss=0.0441708285651668\n",
      "Gradient Descent(2436/9999): loss=0.04417050875273837\n",
      "Gradient Descent(2437/9999): loss=0.04417018944316161\n",
      "Gradient Descent(2438/9999): loss=0.04416987063564586\n",
      "Gradient Descent(2439/9999): loss=0.04416955232940168\n",
      "Gradient Descent(2440/9999): loss=0.04416923452364098\n",
      "Gradient Descent(2441/9999): loss=0.044168917217576785\n",
      "Gradient Descent(2442/9999): loss=0.0441686004104234\n",
      "Gradient Descent(2443/9999): loss=0.04416828410139639\n",
      "Gradient Descent(2444/9999): loss=0.04416796828971252\n",
      "Gradient Descent(2445/9999): loss=0.04416765297458978\n",
      "Gradient Descent(2446/9999): loss=0.04416733815524748\n",
      "Gradient Descent(2447/9999): loss=0.044167023830906045\n",
      "Gradient Descent(2448/9999): loss=0.04416671000078716\n",
      "Gradient Descent(2449/9999): loss=0.04416639666411375\n",
      "Gradient Descent(2450/9999): loss=0.044166083820109954\n",
      "Gradient Descent(2451/9999): loss=0.04416577146800113\n",
      "Gradient Descent(2452/9999): loss=0.04416545960701388\n",
      "Gradient Descent(2453/9999): loss=0.044165148236375955\n",
      "Gradient Descent(2454/9999): loss=0.04416483735531639\n",
      "Gradient Descent(2455/9999): loss=0.04416452696306541\n",
      "Gradient Descent(2456/9999): loss=0.04416421705885444\n",
      "Gradient Descent(2457/9999): loss=0.044163907641916106\n",
      "Gradient Descent(2458/9999): loss=0.04416359871148426\n",
      "Gradient Descent(2459/9999): loss=0.04416329026679394\n",
      "Gradient Descent(2460/9999): loss=0.04416298230708143\n",
      "Gradient Descent(2461/9999): loss=0.04416267483158415\n",
      "Gradient Descent(2462/9999): loss=0.04416236783954078\n",
      "Gradient Descent(2463/9999): loss=0.04416206133019113\n",
      "Gradient Descent(2464/9999): loss=0.04416175530277629\n",
      "Gradient Descent(2465/9999): loss=0.04416144975653847\n",
      "Gradient Descent(2466/9999): loss=0.04416114469072111\n",
      "Gradient Descent(2467/9999): loss=0.04416084010456882\n",
      "Gradient Descent(2468/9999): loss=0.04416053599732743\n",
      "Gradient Descent(2469/9999): loss=0.044160232368243915\n",
      "Gradient Descent(2470/9999): loss=0.04415992921656644\n",
      "Gradient Descent(2471/9999): loss=0.04415962654154438\n",
      "Gradient Descent(2472/9999): loss=0.044159324342428266\n",
      "Gradient Descent(2473/9999): loss=0.044159022618469844\n",
      "Gradient Descent(2474/9999): loss=0.044158721368921953\n",
      "Gradient Descent(2475/9999): loss=0.04415842059303868\n",
      "Gradient Descent(2476/9999): loss=0.044158120290075306\n",
      "Gradient Descent(2477/9999): loss=0.0441578204592882\n",
      "Gradient Descent(2478/9999): loss=0.04415752109993498\n",
      "Gradient Descent(2479/9999): loss=0.044157222211274355\n",
      "Gradient Descent(2480/9999): loss=0.04415692379256628\n",
      "Gradient Descent(2481/9999): loss=0.044156625843071826\n",
      "Gradient Descent(2482/9999): loss=0.04415632836205318\n",
      "Gradient Descent(2483/9999): loss=0.044156031348773786\n",
      "Gradient Descent(2484/9999): loss=0.04415573480249821\n",
      "Gradient Descent(2485/9999): loss=0.04415543872249214\n",
      "Gradient Descent(2486/9999): loss=0.04415514310802246\n",
      "Gradient Descent(2487/9999): loss=0.044154847958357196\n",
      "Gradient Descent(2488/9999): loss=0.04415455327276551\n",
      "Gradient Descent(2489/9999): loss=0.04415425905051772\n",
      "Gradient Descent(2490/9999): loss=0.044153965290885326\n",
      "Gradient Descent(2491/9999): loss=0.044153671993140894\n",
      "Gradient Descent(2492/9999): loss=0.04415337915655823\n",
      "Gradient Descent(2493/9999): loss=0.044153086780412215\n",
      "Gradient Descent(2494/9999): loss=0.04415279486397886\n",
      "Gradient Descent(2495/9999): loss=0.04415250340653541\n",
      "Gradient Descent(2496/9999): loss=0.04415221240736012\n",
      "Gradient Descent(2497/9999): loss=0.04415192186573245\n",
      "Gradient Descent(2498/9999): loss=0.04415163178093299\n",
      "Gradient Descent(2499/9999): loss=0.04415134215224347\n",
      "Gradient Descent(2500/9999): loss=0.04415105297894672\n",
      "Gradient Descent(2501/9999): loss=0.04415076426032669\n",
      "Gradient Descent(2502/9999): loss=0.04415047599566854\n",
      "Gradient Descent(2503/9999): loss=0.04415018818425841\n",
      "Gradient Descent(2504/9999): loss=0.0441499008253837\n",
      "Gradient Descent(2505/9999): loss=0.044149613918332865\n",
      "Gradient Descent(2506/9999): loss=0.04414932746239546\n",
      "Gradient Descent(2507/9999): loss=0.04414904145686218\n",
      "Gradient Descent(2508/9999): loss=0.044148755901024905\n",
      "Gradient Descent(2509/9999): loss=0.044148470794176484\n",
      "Gradient Descent(2510/9999): loss=0.04414818613561102\n",
      "Gradient Descent(2511/9999): loss=0.04414790192462362\n",
      "Gradient Descent(2512/9999): loss=0.04414761816051056\n",
      "Gradient Descent(2513/9999): loss=0.044147334842569184\n",
      "Gradient Descent(2514/9999): loss=0.04414705197009802\n",
      "Gradient Descent(2515/9999): loss=0.04414676954239656\n",
      "Gradient Descent(2516/9999): loss=0.04414648755876553\n",
      "Gradient Descent(2517/9999): loss=0.044146206018506724\n",
      "Gradient Descent(2518/9999): loss=0.044145924920922945\n",
      "Gradient Descent(2519/9999): loss=0.04414564426531821\n",
      "Gradient Descent(2520/9999): loss=0.044145364050997574\n",
      "Gradient Descent(2521/9999): loss=0.04414508427726716\n",
      "Gradient Descent(2522/9999): loss=0.04414480494343426\n",
      "Gradient Descent(2523/9999): loss=0.04414452604880717\n",
      "Gradient Descent(2524/9999): loss=0.04414424759269535\n",
      "Gradient Descent(2525/9999): loss=0.04414396957440925\n",
      "Gradient Descent(2526/9999): loss=0.044143691993260535\n",
      "Gradient Descent(2527/9999): loss=0.044143414848561824\n",
      "Gradient Descent(2528/9999): loss=0.04414313813962689\n",
      "Gradient Descent(2529/9999): loss=0.044142861865770565\n",
      "Gradient Descent(2530/9999): loss=0.04414258602630878\n",
      "Gradient Descent(2531/9999): loss=0.04414231062055851\n",
      "Gradient Descent(2532/9999): loss=0.04414203564783779\n",
      "Gradient Descent(2533/9999): loss=0.044141761107465774\n",
      "Gradient Descent(2534/9999): loss=0.0441414869987627\n",
      "Gradient Descent(2535/9999): loss=0.04414121332104976\n",
      "Gradient Descent(2536/9999): loss=0.044140940073649396\n",
      "Gradient Descent(2537/9999): loss=0.044140667255884905\n",
      "Gradient Descent(2538/9999): loss=0.044140394867080844\n",
      "Gradient Descent(2539/9999): loss=0.044140122906562704\n",
      "Gradient Descent(2540/9999): loss=0.044139851373657085\n",
      "Gradient Descent(2541/9999): loss=0.04413958026769163\n",
      "Gradient Descent(2542/9999): loss=0.04413930958799505\n",
      "Gradient Descent(2543/9999): loss=0.04413903933389712\n",
      "Gradient Descent(2544/9999): loss=0.044138769504728635\n",
      "Gradient Descent(2545/9999): loss=0.044138500099821495\n",
      "Gradient Descent(2546/9999): loss=0.0441382311185086\n",
      "Gradient Descent(2547/9999): loss=0.0441379625601239\n",
      "Gradient Descent(2548/9999): loss=0.04413769442400248\n",
      "Gradient Descent(2549/9999): loss=0.04413742670948032\n",
      "Gradient Descent(2550/9999): loss=0.04413715941589456\n",
      "Gradient Descent(2551/9999): loss=0.04413689254258335\n",
      "Gradient Descent(2552/9999): loss=0.04413662608888588\n",
      "Gradient Descent(2553/9999): loss=0.04413636005414237\n",
      "Gradient Descent(2554/9999): loss=0.04413609443769409\n",
      "Gradient Descent(2555/9999): loss=0.04413582923888334\n",
      "Gradient Descent(2556/9999): loss=0.04413556445705345\n",
      "Gradient Descent(2557/9999): loss=0.044135300091548785\n",
      "Gradient Descent(2558/9999): loss=0.04413503614171476\n",
      "Gradient Descent(2559/9999): loss=0.04413477260689779\n",
      "Gradient Descent(2560/9999): loss=0.04413450948644532\n",
      "Gradient Descent(2561/9999): loss=0.044134246779705816\n",
      "Gradient Descent(2562/9999): loss=0.04413398448602887\n",
      "Gradient Descent(2563/9999): loss=0.044133722604764926\n",
      "Gradient Descent(2564/9999): loss=0.04413346113526554\n",
      "Gradient Descent(2565/9999): loss=0.0441332000768833\n",
      "Gradient Descent(2566/9999): loss=0.04413293942897179\n",
      "Gradient Descent(2567/9999): loss=0.044132679190885635\n",
      "Gradient Descent(2568/9999): loss=0.044132419361980445\n",
      "Gradient Descent(2569/9999): loss=0.04413215994161281\n",
      "Gradient Descent(2570/9999): loss=0.04413190092914039\n",
      "Gradient Descent(2571/9999): loss=0.044131642323921874\n",
      "Gradient Descent(2572/9999): loss=0.04413138412531689\n",
      "Gradient Descent(2573/9999): loss=0.0441311263326861\n",
      "Gradient Descent(2574/9999): loss=0.04413086894539119\n",
      "Gradient Descent(2575/9999): loss=0.04413061196279488\n",
      "Gradient Descent(2576/9999): loss=0.04413035538426076\n",
      "Gradient Descent(2577/9999): loss=0.04413009920915357\n",
      "Gradient Descent(2578/9999): loss=0.044129843436839\n",
      "Gradient Descent(2579/9999): loss=0.04412958806668366\n",
      "Gradient Descent(2580/9999): loss=0.04412933309805531\n",
      "Gradient Descent(2581/9999): loss=0.044129078530322524\n",
      "Gradient Descent(2582/9999): loss=0.044128824362855035\n",
      "Gradient Descent(2583/9999): loss=0.04412857059502345\n",
      "Gradient Descent(2584/9999): loss=0.04412831722619944\n",
      "Gradient Descent(2585/9999): loss=0.044128064255755585\n",
      "Gradient Descent(2586/9999): loss=0.04412781168306556\n",
      "Gradient Descent(2587/9999): loss=0.044127559507503904\n",
      "Gradient Descent(2588/9999): loss=0.044127307728446245\n",
      "Gradient Descent(2589/9999): loss=0.0441270563452691\n",
      "Gradient Descent(2590/9999): loss=0.044126805357350085\n",
      "Gradient Descent(2591/9999): loss=0.04412655476406766\n",
      "Gradient Descent(2592/9999): loss=0.04412630456480132\n",
      "Gradient Descent(2593/9999): loss=0.04412605475893157\n",
      "Gradient Descent(2594/9999): loss=0.04412580534583988\n",
      "Gradient Descent(2595/9999): loss=0.04412555632490861\n",
      "Gradient Descent(2596/9999): loss=0.04412530769552122\n",
      "Gradient Descent(2597/9999): loss=0.04412505945706201\n",
      "Gradient Descent(2598/9999): loss=0.04412481160891637\n",
      "Gradient Descent(2599/9999): loss=0.04412456415047053\n",
      "Gradient Descent(2600/9999): loss=0.04412431708111183\n",
      "Gradient Descent(2601/9999): loss=0.044124070400228435\n",
      "Gradient Descent(2602/9999): loss=0.04412382410720956\n",
      "Gradient Descent(2603/9999): loss=0.04412357820144536\n",
      "Gradient Descent(2604/9999): loss=0.044123332682326905\n",
      "Gradient Descent(2605/9999): loss=0.04412308754924629\n",
      "Gradient Descent(2606/9999): loss=0.04412284280159653\n",
      "Gradient Descent(2607/9999): loss=0.0441225984387716\n",
      "Gradient Descent(2608/9999): loss=0.04412235446016641\n",
      "Gradient Descent(2609/9999): loss=0.04412211086517687\n",
      "Gradient Descent(2610/9999): loss=0.044121867653199774\n",
      "Gradient Descent(2611/9999): loss=0.04412162482363293\n",
      "Gradient Descent(2612/9999): loss=0.04412138237587504\n",
      "Gradient Descent(2613/9999): loss=0.04412114030932577\n",
      "Gradient Descent(2614/9999): loss=0.04412089862338576\n",
      "Gradient Descent(2615/9999): loss=0.044120657317456534\n",
      "Gradient Descent(2616/9999): loss=0.044120416390940616\n",
      "Gradient Descent(2617/9999): loss=0.04412017584324143\n",
      "Gradient Descent(2618/9999): loss=0.04411993567376334\n",
      "Gradient Descent(2619/9999): loss=0.04411969588191165\n",
      "Gradient Descent(2620/9999): loss=0.04411945646709266\n",
      "Gradient Descent(2621/9999): loss=0.04411921742871347\n",
      "Gradient Descent(2622/9999): loss=0.044118978766182274\n",
      "Gradient Descent(2623/9999): loss=0.04411874047890805\n",
      "Gradient Descent(2624/9999): loss=0.04411850256630078\n",
      "Gradient Descent(2625/9999): loss=0.044118265027771365\n",
      "Gradient Descent(2626/9999): loss=0.044118027862731675\n",
      "Gradient Descent(2627/9999): loss=0.04411779107059439\n",
      "Gradient Descent(2628/9999): loss=0.044117554650773216\n",
      "Gradient Descent(2629/9999): loss=0.044117318602682795\n",
      "Gradient Descent(2630/9999): loss=0.04411708292573855\n",
      "Gradient Descent(2631/9999): loss=0.04411684761935696\n",
      "Gradient Descent(2632/9999): loss=0.04411661268295537\n",
      "Gradient Descent(2633/9999): loss=0.044116378115952114\n",
      "Gradient Descent(2634/9999): loss=0.044116143917766276\n",
      "Gradient Descent(2635/9999): loss=0.04411591008781797\n",
      "Gradient Descent(2636/9999): loss=0.04411567662552827\n",
      "Gradient Descent(2637/9999): loss=0.044115443530319036\n",
      "Gradient Descent(2638/9999): loss=0.04411521080161309\n",
      "Gradient Descent(2639/9999): loss=0.0441149784388342\n",
      "Gradient Descent(2640/9999): loss=0.04411474644140701\n",
      "Gradient Descent(2641/9999): loss=0.04411451480875705\n",
      "Gradient Descent(2642/9999): loss=0.04411428354031075\n",
      "Gradient Descent(2643/9999): loss=0.04411405263549548\n",
      "Gradient Descent(2644/9999): loss=0.04411382209373949\n",
      "Gradient Descent(2645/9999): loss=0.04411359191447195\n",
      "Gradient Descent(2646/9999): loss=0.044113362097122855\n",
      "Gradient Descent(2647/9999): loss=0.04411313264112321\n",
      "Gradient Descent(2648/9999): loss=0.04411290354590482\n",
      "Gradient Descent(2649/9999): loss=0.04411267481090039\n",
      "Gradient Descent(2650/9999): loss=0.04411244643554361\n",
      "Gradient Descent(2651/9999): loss=0.04411221841926895\n",
      "Gradient Descent(2652/9999): loss=0.04411199076151183\n",
      "Gradient Descent(2653/9999): loss=0.04411176346170851\n",
      "Gradient Descent(2654/9999): loss=0.04411153651929621\n",
      "Gradient Descent(2655/9999): loss=0.04411130993371295\n",
      "Gradient Descent(2656/9999): loss=0.044111083704397716\n",
      "Gradient Descent(2657/9999): loss=0.0441108578307903\n",
      "Gradient Descent(2658/9999): loss=0.04411063231233145\n",
      "Gradient Descent(2659/9999): loss=0.04411040714846272\n",
      "Gradient Descent(2660/9999): loss=0.0441101823386266\n",
      "Gradient Descent(2661/9999): loss=0.04410995788226641\n",
      "Gradient Descent(2662/9999): loss=0.044109733778826384\n",
      "Gradient Descent(2663/9999): loss=0.044109510027751625\n",
      "Gradient Descent(2664/9999): loss=0.04410928662848808\n",
      "Gradient Descent(2665/9999): loss=0.04410906358048258\n",
      "Gradient Descent(2666/9999): loss=0.044108840883182836\n",
      "Gradient Descent(2667/9999): loss=0.04410861853603742\n",
      "Gradient Descent(2668/9999): loss=0.044108396538495784\n",
      "Gradient Descent(2669/9999): loss=0.04410817489000822\n",
      "Gradient Descent(2670/9999): loss=0.044107953590025915\n",
      "Gradient Descent(2671/9999): loss=0.04410773263800089\n",
      "Gradient Descent(2672/9999): loss=0.04410751203338604\n",
      "Gradient Descent(2673/9999): loss=0.04410729177563513\n",
      "Gradient Descent(2674/9999): loss=0.04410707186420275\n",
      "Gradient Descent(2675/9999): loss=0.04410685229854439\n",
      "Gradient Descent(2676/9999): loss=0.044106633078116395\n",
      "Gradient Descent(2677/9999): loss=0.04410641420237596\n",
      "Gradient Descent(2678/9999): loss=0.04410619567078102\n",
      "Gradient Descent(2679/9999): loss=0.04410597748279058\n",
      "Gradient Descent(2680/9999): loss=0.044105759637864346\n",
      "Gradient Descent(2681/9999): loss=0.04410554213546287\n",
      "Gradient Descent(2682/9999): loss=0.04410532497504762\n",
      "Gradient Descent(2683/9999): loss=0.04410510815608089\n",
      "Gradient Descent(2684/9999): loss=0.04410489167802579\n",
      "Gradient Descent(2685/9999): loss=0.044104675540346285\n",
      "Gradient Descent(2686/9999): loss=0.04410445974250722\n",
      "Gradient Descent(2687/9999): loss=0.04410424428397421\n",
      "Gradient Descent(2688/9999): loss=0.04410402916421381\n",
      "Gradient Descent(2689/9999): loss=0.04410381438269331\n",
      "Gradient Descent(2690/9999): loss=0.044103599938880904\n",
      "Gradient Descent(2691/9999): loss=0.044103385832245594\n",
      "Gradient Descent(2692/9999): loss=0.044103172062257216\n",
      "Gradient Descent(2693/9999): loss=0.0441029586283865\n",
      "Gradient Descent(2694/9999): loss=0.044102745530104875\n",
      "Gradient Descent(2695/9999): loss=0.044102532766884764\n",
      "Gradient Descent(2696/9999): loss=0.044102320338199295\n",
      "Gradient Descent(2697/9999): loss=0.04410210824352245\n",
      "Gradient Descent(2698/9999): loss=0.044101896482329116\n",
      "Gradient Descent(2699/9999): loss=0.0441016850540949\n",
      "Gradient Descent(2700/9999): loss=0.04410147395829629\n",
      "Gradient Descent(2701/9999): loss=0.04410126319441057\n",
      "Gradient Descent(2702/9999): loss=0.044101052761915914\n",
      "Gradient Descent(2703/9999): loss=0.0441008426602912\n",
      "Gradient Descent(2704/9999): loss=0.04410063288901624\n",
      "Gradient Descent(2705/9999): loss=0.044100423447571584\n",
      "Gradient Descent(2706/9999): loss=0.04410021433543864\n",
      "Gradient Descent(2707/9999): loss=0.04410000555209964\n",
      "Gradient Descent(2708/9999): loss=0.04409979709703756\n",
      "Gradient Descent(2709/9999): loss=0.044099588969736286\n",
      "Gradient Descent(2710/9999): loss=0.044099381169680464\n",
      "Gradient Descent(2711/9999): loss=0.04409917369635551\n",
      "Gradient Descent(2712/9999): loss=0.044098966549247755\n",
      "Gradient Descent(2713/9999): loss=0.044098759727844244\n",
      "Gradient Descent(2714/9999): loss=0.044098553231632885\n",
      "Gradient Descent(2715/9999): loss=0.04409834706010232\n",
      "Gradient Descent(2716/9999): loss=0.0440981412127421\n",
      "Gradient Descent(2717/9999): loss=0.044097935689042495\n",
      "Gradient Descent(2718/9999): loss=0.04409773048849458\n",
      "Gradient Descent(2719/9999): loss=0.044097525610590285\n",
      "Gradient Descent(2720/9999): loss=0.04409732105482228\n",
      "Gradient Descent(2721/9999): loss=0.044097116820684096\n",
      "Gradient Descent(2722/9999): loss=0.04409691290767001\n",
      "Gradient Descent(2723/9999): loss=0.04409670931527509\n",
      "Gradient Descent(2724/9999): loss=0.04409650604299523\n",
      "Gradient Descent(2725/9999): loss=0.0440963030903271\n",
      "Gradient Descent(2726/9999): loss=0.04409610045676815\n",
      "Gradient Descent(2727/9999): loss=0.044095898141816664\n",
      "Gradient Descent(2728/9999): loss=0.04409569614497167\n",
      "Gradient Descent(2729/9999): loss=0.044095494465733\n",
      "Gradient Descent(2730/9999): loss=0.04409529310360126\n",
      "Gradient Descent(2731/9999): loss=0.04409509205807785\n",
      "Gradient Descent(2732/9999): loss=0.04409489132866499\n",
      "Gradient Descent(2733/9999): loss=0.04409469091486561\n",
      "Gradient Descent(2734/9999): loss=0.04409449081618349\n",
      "Gradient Descent(2735/9999): loss=0.04409429103212314\n",
      "Gradient Descent(2736/9999): loss=0.044094091562189866\n",
      "Gradient Descent(2737/9999): loss=0.04409389240588979\n",
      "Gradient Descent(2738/9999): loss=0.044093693562729734\n",
      "Gradient Descent(2739/9999): loss=0.04409349503221737\n",
      "Gradient Descent(2740/9999): loss=0.0440932968138611\n",
      "Gradient Descent(2741/9999): loss=0.044093098907170114\n",
      "Gradient Descent(2742/9999): loss=0.04409290131165434\n",
      "Gradient Descent(2743/9999): loss=0.04409270402682454\n",
      "Gradient Descent(2744/9999): loss=0.04409250705219222\n",
      "Gradient Descent(2745/9999): loss=0.044092310387269615\n",
      "Gradient Descent(2746/9999): loss=0.04409211403156978\n",
      "Gradient Descent(2747/9999): loss=0.044091917984606495\n",
      "Gradient Descent(2748/9999): loss=0.044091722245894346\n",
      "Gradient Descent(2749/9999): loss=0.04409152681494866\n",
      "Gradient Descent(2750/9999): loss=0.04409133169128549\n",
      "Gradient Descent(2751/9999): loss=0.04409113687442174\n",
      "Gradient Descent(2752/9999): loss=0.04409094236387498\n",
      "Gradient Descent(2753/9999): loss=0.04409074815916358\n",
      "Gradient Descent(2754/9999): loss=0.04409055425980666\n",
      "Gradient Descent(2755/9999): loss=0.044090360665324145\n",
      "Gradient Descent(2756/9999): loss=0.04409016737523662\n",
      "Gradient Descent(2757/9999): loss=0.04408997438906551\n",
      "Gradient Descent(2758/9999): loss=0.04408978170633292\n",
      "Gradient Descent(2759/9999): loss=0.04408958932656176\n",
      "Gradient Descent(2760/9999): loss=0.0440893972492757\n",
      "Gradient Descent(2761/9999): loss=0.044089205473999096\n",
      "Gradient Descent(2762/9999): loss=0.04408901400025713\n",
      "Gradient Descent(2763/9999): loss=0.044088822827575605\n",
      "Gradient Descent(2764/9999): loss=0.04408863195548127\n",
      "Gradient Descent(2765/9999): loss=0.044088441383501424\n",
      "Gradient Descent(2766/9999): loss=0.04408825111116419\n",
      "Gradient Descent(2767/9999): loss=0.04408806113799847\n",
      "Gradient Descent(2768/9999): loss=0.04408787146353384\n",
      "Gradient Descent(2769/9999): loss=0.044087682087300635\n",
      "Gradient Descent(2770/9999): loss=0.04408749300882996\n",
      "Gradient Descent(2771/9999): loss=0.044087304227653605\n",
      "Gradient Descent(2772/9999): loss=0.04408711574330414\n",
      "Gradient Descent(2773/9999): loss=0.04408692755531487\n",
      "Gradient Descent(2774/9999): loss=0.04408673966321977\n",
      "Gradient Descent(2775/9999): loss=0.044086552066553646\n",
      "Gradient Descent(2776/9999): loss=0.044086364764851955\n",
      "Gradient Descent(2777/9999): loss=0.04408617775765095\n",
      "Gradient Descent(2778/9999): loss=0.044085991044487544\n",
      "Gradient Descent(2779/9999): loss=0.044085804624899395\n",
      "Gradient Descent(2780/9999): loss=0.04408561849842497\n",
      "Gradient Descent(2781/9999): loss=0.044085432664603345\n",
      "Gradient Descent(2782/9999): loss=0.04408524712297437\n",
      "Gradient Descent(2783/9999): loss=0.04408506187307865\n",
      "Gradient Descent(2784/9999): loss=0.04408487691445747\n",
      "Gradient Descent(2785/9999): loss=0.044084692246652846\n",
      "Gradient Descent(2786/9999): loss=0.0440845078692075\n",
      "Gradient Descent(2787/9999): loss=0.04408432378166494\n",
      "Gradient Descent(2788/9999): loss=0.04408413998356932\n",
      "Gradient Descent(2789/9999): loss=0.0440839564744655\n",
      "Gradient Descent(2790/9999): loss=0.044083773253899124\n",
      "Gradient Descent(2791/9999): loss=0.04408359032141651\n",
      "Gradient Descent(2792/9999): loss=0.04408340767656469\n",
      "Gradient Descent(2793/9999): loss=0.044083225318891396\n",
      "Gradient Descent(2794/9999): loss=0.04408304324794513\n",
      "Gradient Descent(2795/9999): loss=0.04408286146327501\n",
      "Gradient Descent(2796/9999): loss=0.04408267996443097\n",
      "Gradient Descent(2797/9999): loss=0.04408249875096354\n",
      "Gradient Descent(2798/9999): loss=0.04408231782242404\n",
      "Gradient Descent(2799/9999): loss=0.04408213717836448\n",
      "Gradient Descent(2800/9999): loss=0.04408195681833755\n",
      "Gradient Descent(2801/9999): loss=0.04408177674189665\n",
      "Gradient Descent(2802/9999): loss=0.044081596948595904\n",
      "Gradient Descent(2803/9999): loss=0.04408141743799011\n",
      "Gradient Descent(2804/9999): loss=0.044081238209634784\n",
      "Gradient Descent(2805/9999): loss=0.04408105926308613\n",
      "Gradient Descent(2806/9999): loss=0.04408088059790106\n",
      "Gradient Descent(2807/9999): loss=0.044080702213637156\n",
      "Gradient Descent(2808/9999): loss=0.04408052410985272\n",
      "Gradient Descent(2809/9999): loss=0.04408034628610679\n",
      "Gradient Descent(2810/9999): loss=0.044080168741959\n",
      "Gradient Descent(2811/9999): loss=0.04407999147696973\n",
      "Gradient Descent(2812/9999): loss=0.04407981449070008\n",
      "Gradient Descent(2813/9999): loss=0.0440796377827118\n",
      "Gradient Descent(2814/9999): loss=0.04407946135256734\n",
      "Gradient Descent(2815/9999): loss=0.044079285199829836\n",
      "Gradient Descent(2816/9999): loss=0.044079109324063094\n",
      "Gradient Descent(2817/9999): loss=0.044078933724831654\n",
      "Gradient Descent(2818/9999): loss=0.044078758401700696\n",
      "Gradient Descent(2819/9999): loss=0.04407858335423607\n",
      "Gradient Descent(2820/9999): loss=0.044078408582004396\n",
      "Gradient Descent(2821/9999): loss=0.04407823408457287\n",
      "Gradient Descent(2822/9999): loss=0.04407805986150942\n",
      "Gradient Descent(2823/9999): loss=0.04407788591238267\n",
      "Gradient Descent(2824/9999): loss=0.04407771223676185\n",
      "Gradient Descent(2825/9999): loss=0.04407753883421701\n",
      "Gradient Descent(2826/9999): loss=0.04407736570431871\n",
      "Gradient Descent(2827/9999): loss=0.044077192846638254\n",
      "Gradient Descent(2828/9999): loss=0.04407702026074765\n",
      "Gradient Descent(2829/9999): loss=0.04407684794621956\n",
      "Gradient Descent(2830/9999): loss=0.04407667590262731\n",
      "Gradient Descent(2831/9999): loss=0.0440765041295449\n",
      "Gradient Descent(2832/9999): loss=0.044076332626546956\n",
      "Gradient Descent(2833/9999): loss=0.04407616139320887\n",
      "Gradient Descent(2834/9999): loss=0.044075990429106635\n",
      "Gradient Descent(2835/9999): loss=0.04407581973381689\n",
      "Gradient Descent(2836/9999): loss=0.04407564930691703\n",
      "Gradient Descent(2837/9999): loss=0.04407547914798499\n",
      "Gradient Descent(2838/9999): loss=0.0440753092565995\n",
      "Gradient Descent(2839/9999): loss=0.04407513963233982\n",
      "Gradient Descent(2840/9999): loss=0.04407497027478601\n",
      "Gradient Descent(2841/9999): loss=0.04407480118351867\n",
      "Gradient Descent(2842/9999): loss=0.04407463235811909\n",
      "Gradient Descent(2843/9999): loss=0.04407446379816932\n",
      "Gradient Descent(2844/9999): loss=0.04407429550325191\n",
      "Gradient Descent(2845/9999): loss=0.04407412747295018\n",
      "Gradient Descent(2846/9999): loss=0.04407395970684802\n",
      "Gradient Descent(2847/9999): loss=0.04407379220453007\n",
      "Gradient Descent(2848/9999): loss=0.04407362496558158\n",
      "Gradient Descent(2849/9999): loss=0.04407345798958839\n",
      "Gradient Descent(2850/9999): loss=0.044073291276137086\n",
      "Gradient Descent(2851/9999): loss=0.04407312482481487\n",
      "Gradient Descent(2852/9999): loss=0.04407295863520958\n",
      "Gradient Descent(2853/9999): loss=0.044072792706909676\n",
      "Gradient Descent(2854/9999): loss=0.04407262703950434\n",
      "Gradient Descent(2855/9999): loss=0.044072461632583354\n",
      "Gradient Descent(2856/9999): loss=0.04407229648573712\n",
      "Gradient Descent(2857/9999): loss=0.044072131598556774\n",
      "Gradient Descent(2858/9999): loss=0.04407196697063396\n",
      "Gradient Descent(2859/9999): loss=0.04407180260156109\n",
      "Gradient Descent(2860/9999): loss=0.044071638490931146\n",
      "Gradient Descent(2861/9999): loss=0.044071474638337785\n",
      "Gradient Descent(2862/9999): loss=0.04407131104337525\n",
      "Gradient Descent(2863/9999): loss=0.04407114770563852\n",
      "Gradient Descent(2864/9999): loss=0.044070984624723086\n",
      "Gradient Descent(2865/9999): loss=0.044070821800225195\n",
      "Gradient Descent(2866/9999): loss=0.04407065923174165\n",
      "Gradient Descent(2867/9999): loss=0.04407049691886989\n",
      "Gradient Descent(2868/9999): loss=0.044070334861208045\n",
      "Gradient Descent(2869/9999): loss=0.04407017305835481\n",
      "Gradient Descent(2870/9999): loss=0.04407001150990955\n",
      "Gradient Descent(2871/9999): loss=0.044069850215472266\n",
      "Gradient Descent(2872/9999): loss=0.04406968917464356\n",
      "Gradient Descent(2873/9999): loss=0.04406952838702468\n",
      "Gradient Descent(2874/9999): loss=0.04406936785221748\n",
      "Gradient Descent(2875/9999): loss=0.04406920756982448\n",
      "Gradient Descent(2876/9999): loss=0.04406904753944878\n",
      "Gradient Descent(2877/9999): loss=0.04406888776069412\n",
      "Gradient Descent(2878/9999): loss=0.04406872823316492\n",
      "Gradient Descent(2879/9999): loss=0.0440685689564661\n",
      "Gradient Descent(2880/9999): loss=0.044068409930203334\n",
      "Gradient Descent(2881/9999): loss=0.0440682511539828\n",
      "Gradient Descent(2882/9999): loss=0.044068092627411376\n",
      "Gradient Descent(2883/9999): loss=0.04406793435009652\n",
      "Gradient Descent(2884/9999): loss=0.04406777632164631\n",
      "Gradient Descent(2885/9999): loss=0.044067618541669476\n",
      "Gradient Descent(2886/9999): loss=0.04406746100977532\n",
      "Gradient Descent(2887/9999): loss=0.044067303725573756\n",
      "Gradient Descent(2888/9999): loss=0.04406714668867537\n",
      "Gradient Descent(2889/9999): loss=0.044066989898691275\n",
      "Gradient Descent(2890/9999): loss=0.04406683335523326\n",
      "Gradient Descent(2891/9999): loss=0.04406667705791373\n",
      "Gradient Descent(2892/9999): loss=0.04406652100634563\n",
      "Gradient Descent(2893/9999): loss=0.04406636520014255\n",
      "Gradient Descent(2894/9999): loss=0.04406620963891873\n",
      "Gradient Descent(2895/9999): loss=0.04406605432228897\n",
      "Gradient Descent(2896/9999): loss=0.04406589924986869\n",
      "Gradient Descent(2897/9999): loss=0.04406574442127391\n",
      "Gradient Descent(2898/9999): loss=0.04406558983612124\n",
      "Gradient Descent(2899/9999): loss=0.04406543549402793\n",
      "Gradient Descent(2900/9999): loss=0.04406528139461179\n",
      "Gradient Descent(2901/9999): loss=0.04406512753749126\n",
      "Gradient Descent(2902/9999): loss=0.04406497392228537\n",
      "Gradient Descent(2903/9999): loss=0.04406482054861377\n",
      "Gradient Descent(2904/9999): loss=0.04406466741609665\n",
      "Gradient Descent(2905/9999): loss=0.04406451452435487\n",
      "Gradient Descent(2906/9999): loss=0.0440643618730098\n",
      "Gradient Descent(2907/9999): loss=0.044064209461683504\n",
      "Gradient Descent(2908/9999): loss=0.04406405728999859\n",
      "Gradient Descent(2909/9999): loss=0.04406390535757826\n",
      "Gradient Descent(2910/9999): loss=0.044063753664046246\n",
      "Gradient Descent(2911/9999): loss=0.044063602209027046\n",
      "Gradient Descent(2912/9999): loss=0.044063450992145564\n",
      "Gradient Descent(2913/9999): loss=0.0440633000130274\n",
      "Gradient Descent(2914/9999): loss=0.0440631492712987\n",
      "Gradient Descent(2915/9999): loss=0.04406299876658617\n",
      "Gradient Descent(2916/9999): loss=0.044062848498517215\n",
      "Gradient Descent(2917/9999): loss=0.04406269846671971\n",
      "Gradient Descent(2918/9999): loss=0.04406254867082213\n",
      "Gradient Descent(2919/9999): loss=0.04406239911045365\n",
      "Gradient Descent(2920/9999): loss=0.04406224978524383\n",
      "Gradient Descent(2921/9999): loss=0.044062100694823005\n",
      "Gradient Descent(2922/9999): loss=0.04406195183882198\n",
      "Gradient Descent(2923/9999): loss=0.04406180321687216\n",
      "Gradient Descent(2924/9999): loss=0.04406165482860556\n",
      "Gradient Descent(2925/9999): loss=0.04406150667365474\n",
      "Gradient Descent(2926/9999): loss=0.04406135875165286\n",
      "Gradient Descent(2927/9999): loss=0.044061211062233624\n",
      "Gradient Descent(2928/9999): loss=0.044061063605031335\n",
      "Gradient Descent(2929/9999): loss=0.04406091637968091\n",
      "Gradient Descent(2930/9999): loss=0.044060769385817745\n",
      "Gradient Descent(2931/9999): loss=0.04406062262307793\n",
      "Gradient Descent(2932/9999): loss=0.044060476091098\n",
      "Gradient Descent(2933/9999): loss=0.04406032978951517\n",
      "Gradient Descent(2934/9999): loss=0.04406018371796714\n",
      "Gradient Descent(2935/9999): loss=0.04406003787609223\n",
      "Gradient Descent(2936/9999): loss=0.044059892263529334\n",
      "Gradient Descent(2937/9999): loss=0.04405974687991789\n",
      "Gradient Descent(2938/9999): loss=0.044059601724897916\n",
      "Gradient Descent(2939/9999): loss=0.04405945679810996\n",
      "Gradient Descent(2940/9999): loss=0.044059312099195204\n",
      "Gradient Descent(2941/9999): loss=0.04405916762779533\n",
      "Gradient Descent(2942/9999): loss=0.0440590233835526\n",
      "Gradient Descent(2943/9999): loss=0.04405887936610992\n",
      "Gradient Descent(2944/9999): loss=0.04405873557511059\n",
      "Gradient Descent(2945/9999): loss=0.044058592010198606\n",
      "Gradient Descent(2946/9999): loss=0.0440584486710185\n",
      "Gradient Descent(2947/9999): loss=0.0440583055572153\n",
      "Gradient Descent(2948/9999): loss=0.04405816266843469\n",
      "Gradient Descent(2949/9999): loss=0.04405802000432283\n",
      "Gradient Descent(2950/9999): loss=0.04405787756452648\n",
      "Gradient Descent(2951/9999): loss=0.04405773534869291\n",
      "Gradient Descent(2952/9999): loss=0.04405759335647001\n",
      "Gradient Descent(2953/9999): loss=0.044057451587506194\n",
      "Gradient Descent(2954/9999): loss=0.0440573100414504\n",
      "Gradient Descent(2955/9999): loss=0.04405716871795216\n",
      "Gradient Descent(2956/9999): loss=0.04405702761666151\n",
      "Gradient Descent(2957/9999): loss=0.04405688673722911\n",
      "Gradient Descent(2958/9999): loss=0.04405674607930609\n",
      "Gradient Descent(2959/9999): loss=0.04405660564254421\n",
      "Gradient Descent(2960/9999): loss=0.04405646542659564\n",
      "Gradient Descent(2961/9999): loss=0.044056325431113284\n",
      "Gradient Descent(2962/9999): loss=0.04405618565575043\n",
      "Gradient Descent(2963/9999): loss=0.04405604610016102\n",
      "Gradient Descent(2964/9999): loss=0.04405590676399946\n",
      "Gradient Descent(2965/9999): loss=0.044055767646920777\n",
      "Gradient Descent(2966/9999): loss=0.04405562874858046\n",
      "Gradient Descent(2967/9999): loss=0.044055490068634635\n",
      "Gradient Descent(2968/9999): loss=0.04405535160673986\n",
      "Gradient Descent(2969/9999): loss=0.044055213362553275\n",
      "Gradient Descent(2970/9999): loss=0.04405507533573263\n",
      "Gradient Descent(2971/9999): loss=0.0440549375259361\n",
      "Gradient Descent(2972/9999): loss=0.04405479993282249\n",
      "Gradient Descent(2973/9999): loss=0.04405466255605107\n",
      "Gradient Descent(2974/9999): loss=0.04405452539528172\n",
      "Gradient Descent(2975/9999): loss=0.04405438845017476\n",
      "Gradient Descent(2976/9999): loss=0.04405425172039111\n",
      "Gradient Descent(2977/9999): loss=0.04405411520559225\n",
      "Gradient Descent(2978/9999): loss=0.0440539789054401\n",
      "Gradient Descent(2979/9999): loss=0.04405384281959721\n",
      "Gradient Descent(2980/9999): loss=0.044053706947726576\n",
      "Gradient Descent(2981/9999): loss=0.04405357128949179\n",
      "Gradient Descent(2982/9999): loss=0.04405343584455692\n",
      "Gradient Descent(2983/9999): loss=0.04405330061258663\n",
      "Gradient Descent(2984/9999): loss=0.04405316559324603\n",
      "Gradient Descent(2985/9999): loss=0.04405303078620078\n",
      "Gradient Descent(2986/9999): loss=0.04405289619111713\n",
      "Gradient Descent(2987/9999): loss=0.04405276180766177\n",
      "Gradient Descent(2988/9999): loss=0.044052627635501974\n",
      "Gradient Descent(2989/9999): loss=0.044052493674305505\n",
      "Gradient Descent(2990/9999): loss=0.04405235992374066\n",
      "Gradient Descent(2991/9999): loss=0.04405222638347622\n",
      "Gradient Descent(2992/9999): loss=0.044052093053181605\n",
      "Gradient Descent(2993/9999): loss=0.0440519599325266\n",
      "Gradient Descent(2994/9999): loss=0.04405182702118159\n",
      "Gradient Descent(2995/9999): loss=0.044051694318817516\n",
      "Gradient Descent(2996/9999): loss=0.04405156182510571\n",
      "Gradient Descent(2997/9999): loss=0.04405142953971821\n",
      "Gradient Descent(2998/9999): loss=0.04405129746232736\n",
      "Gradient Descent(2999/9999): loss=0.04405116559260617\n",
      "Gradient Descent(3000/9999): loss=0.044051033930228106\n",
      "Gradient Descent(3001/9999): loss=0.04405090247486715\n",
      "Gradient Descent(3002/9999): loss=0.04405077122619781\n",
      "Gradient Descent(3003/9999): loss=0.044050640183895096\n",
      "Gradient Descent(3004/9999): loss=0.04405050934763454\n",
      "Gradient Descent(3005/9999): loss=0.044050378717092144\n",
      "Gradient Descent(3006/9999): loss=0.044050248291944503\n",
      "Gradient Descent(3007/9999): loss=0.044050118071868594\n",
      "Gradient Descent(3008/9999): loss=0.04404998805654206\n",
      "Gradient Descent(3009/9999): loss=0.044049858245642885\n",
      "Gradient Descent(3010/9999): loss=0.044049728638849706\n",
      "Gradient Descent(3011/9999): loss=0.04404959923584158\n",
      "Gradient Descent(3012/9999): loss=0.044049470036298076\n",
      "Gradient Descent(3013/9999): loss=0.0440493410398993\n",
      "Gradient Descent(3014/9999): loss=0.0440492122463258\n",
      "Gradient Descent(3015/9999): loss=0.04404908365525873\n",
      "Gradient Descent(3016/9999): loss=0.04404895526637962\n",
      "Gradient Descent(3017/9999): loss=0.0440488270793706\n",
      "Gradient Descent(3018/9999): loss=0.04404869909391427\n",
      "Gradient Descent(3019/9999): loss=0.044048571309693696\n",
      "Gradient Descent(3020/9999): loss=0.044048443726392474\n",
      "Gradient Descent(3021/9999): loss=0.0440483163436947\n",
      "Gradient Descent(3022/9999): loss=0.044048189161284945\n",
      "Gradient Descent(3023/9999): loss=0.044048062178848345\n",
      "Gradient Descent(3024/9999): loss=0.04404793539607038\n",
      "Gradient Descent(3025/9999): loss=0.044047808812637194\n",
      "Gradient Descent(3026/9999): loss=0.04404768242823533\n",
      "Gradient Descent(3027/9999): loss=0.04404755624255183\n",
      "Gradient Descent(3028/9999): loss=0.04404743025527425\n",
      "Gradient Descent(3029/9999): loss=0.04404730446609064\n",
      "Gradient Descent(3030/9999): loss=0.04404717887468954\n",
      "Gradient Descent(3031/9999): loss=0.044047053480759935\n",
      "Gradient Descent(3032/9999): loss=0.04404692828399136\n",
      "Gradient Descent(3033/9999): loss=0.04404680328407383\n",
      "Gradient Descent(3034/9999): loss=0.04404667848069777\n",
      "Gradient Descent(3035/9999): loss=0.0440465538735542\n",
      "Gradient Descent(3036/9999): loss=0.04404642946233457\n",
      "Gradient Descent(3037/9999): loss=0.04404630524673081\n",
      "Gradient Descent(3038/9999): loss=0.044046181226435346\n",
      "Gradient Descent(3039/9999): loss=0.044046057401141106\n",
      "Gradient Descent(3040/9999): loss=0.04404593377054147\n",
      "Gradient Descent(3041/9999): loss=0.044045810334330325\n",
      "Gradient Descent(3042/9999): loss=0.04404568709220201\n",
      "Gradient Descent(3043/9999): loss=0.04404556404385135\n",
      "Gradient Descent(3044/9999): loss=0.044045441188973695\n",
      "Gradient Descent(3045/9999): loss=0.04404531852726486\n",
      "Gradient Descent(3046/9999): loss=0.04404519605842104\n",
      "Gradient Descent(3047/9999): loss=0.04404507378213908\n",
      "Gradient Descent(3048/9999): loss=0.04404495169811613\n",
      "Gradient Descent(3049/9999): loss=0.04404482980604993\n",
      "Gradient Descent(3050/9999): loss=0.044044708105638666\n",
      "Gradient Descent(3051/9999): loss=0.044044586596581\n",
      "Gradient Descent(3052/9999): loss=0.04404446527857603\n",
      "Gradient Descent(3053/9999): loss=0.044044344151323356\n",
      "Gradient Descent(3054/9999): loss=0.04404422321452308\n",
      "Gradient Descent(3055/9999): loss=0.04404410246787573\n",
      "Gradient Descent(3056/9999): loss=0.044043981911082344\n",
      "Gradient Descent(3057/9999): loss=0.04404386154384441\n",
      "Gradient Descent(3058/9999): loss=0.04404374136586384\n",
      "Gradient Descent(3059/9999): loss=0.04404362137684309\n",
      "Gradient Descent(3060/9999): loss=0.04404350157648503\n",
      "Gradient Descent(3061/9999): loss=0.04404338196449307\n",
      "Gradient Descent(3062/9999): loss=0.04404326254057096\n",
      "Gradient Descent(3063/9999): loss=0.04404314330442307\n",
      "Gradient Descent(3064/9999): loss=0.0440430242557541\n",
      "Gradient Descent(3065/9999): loss=0.044042905394269284\n",
      "Gradient Descent(3066/9999): loss=0.04404278671967433\n",
      "Gradient Descent(3067/9999): loss=0.04404266823167537\n",
      "Gradient Descent(3068/9999): loss=0.044042549929979\n",
      "Gradient Descent(3069/9999): loss=0.04404243181429232\n",
      "Gradient Descent(3070/9999): loss=0.044042313884322795\n",
      "Gradient Descent(3071/9999): loss=0.04404219613977849\n",
      "Gradient Descent(3072/9999): loss=0.04404207858036782\n",
      "Gradient Descent(3073/9999): loss=0.04404196120579975\n",
      "Gradient Descent(3074/9999): loss=0.04404184401578354\n",
      "Gradient Descent(3075/9999): loss=0.04404172701002909\n",
      "Gradient Descent(3076/9999): loss=0.044041610188246635\n",
      "Gradient Descent(3077/9999): loss=0.04404149355014697\n",
      "Gradient Descent(3078/9999): loss=0.044041377095441224\n",
      "Gradient Descent(3079/9999): loss=0.04404126082384109\n",
      "Gradient Descent(3080/9999): loss=0.04404114473505861\n",
      "Gradient Descent(3081/9999): loss=0.04404102882880638\n",
      "Gradient Descent(3082/9999): loss=0.044040913104797366\n",
      "Gradient Descent(3083/9999): loss=0.044040797562745065\n",
      "Gradient Descent(3084/9999): loss=0.044040682202363346\n",
      "Gradient Descent(3085/9999): loss=0.044040567023366554\n",
      "Gradient Descent(3086/9999): loss=0.04404045202546951\n",
      "Gradient Descent(3087/9999): loss=0.04404033720838748\n",
      "Gradient Descent(3088/9999): loss=0.04404022257183613\n",
      "Gradient Descent(3089/9999): loss=0.04404010811553166\n",
      "Gradient Descent(3090/9999): loss=0.04403999383919058\n",
      "Gradient Descent(3091/9999): loss=0.04403987974252999\n",
      "Gradient Descent(3092/9999): loss=0.04403976582526736\n",
      "Gradient Descent(3093/9999): loss=0.0440396520871206\n",
      "Gradient Descent(3094/9999): loss=0.04403953852780809\n",
      "Gradient Descent(3095/9999): loss=0.04403942514704865\n",
      "Gradient Descent(3096/9999): loss=0.044039311944561524\n",
      "Gradient Descent(3097/9999): loss=0.044039198920066414\n",
      "Gradient Descent(3098/9999): loss=0.04403908607328347\n",
      "Gradient Descent(3099/9999): loss=0.044038973403933236\n",
      "Gradient Descent(3100/9999): loss=0.04403886091173677\n",
      "Gradient Descent(3101/9999): loss=0.04403874859641548\n",
      "Gradient Descent(3102/9999): loss=0.04403863645769131\n",
      "Gradient Descent(3103/9999): loss=0.04403852449528656\n",
      "Gradient Descent(3104/9999): loss=0.044038412708924\n",
      "Gradient Descent(3105/9999): loss=0.04403830109832681\n",
      "Gradient Descent(3106/9999): loss=0.04403818966321868\n",
      "Gradient Descent(3107/9999): loss=0.04403807840332366\n",
      "Gradient Descent(3108/9999): loss=0.04403796731836624\n",
      "Gradient Descent(3109/9999): loss=0.04403785640807139\n",
      "Gradient Descent(3110/9999): loss=0.044037745672164466\n",
      "Gradient Descent(3111/9999): loss=0.044037635110371245\n",
      "Gradient Descent(3112/9999): loss=0.04403752472241802\n",
      "Gradient Descent(3113/9999): loss=0.044037414508031406\n",
      "Gradient Descent(3114/9999): loss=0.04403730446693855\n",
      "Gradient Descent(3115/9999): loss=0.044037194598866895\n",
      "Gradient Descent(3116/9999): loss=0.04403708490354448\n",
      "Gradient Descent(3117/9999): loss=0.04403697538069961\n",
      "Gradient Descent(3118/9999): loss=0.04403686603006116\n",
      "Gradient Descent(3119/9999): loss=0.04403675685135834\n",
      "Gradient Descent(3120/9999): loss=0.04403664784432081\n",
      "Gradient Descent(3121/9999): loss=0.044036539008678634\n",
      "Gradient Descent(3122/9999): loss=0.04403643034416236\n",
      "Gradient Descent(3123/9999): loss=0.04403632185050289\n",
      "Gradient Descent(3124/9999): loss=0.044036213527431596\n",
      "Gradient Descent(3125/9999): loss=0.044036105374680216\n",
      "Gradient Descent(3126/9999): loss=0.044035997391981024\n",
      "Gradient Descent(3127/9999): loss=0.04403588957906659\n",
      "Gradient Descent(3128/9999): loss=0.04403578193566997\n",
      "Gradient Descent(3129/9999): loss=0.044035674461524614\n",
      "Gradient Descent(3130/9999): loss=0.04403556715636442\n",
      "Gradient Descent(3131/9999): loss=0.04403546001992367\n",
      "Gradient Descent(3132/9999): loss=0.04403535305193711\n",
      "Gradient Descent(3133/9999): loss=0.044035246252139816\n",
      "Gradient Descent(3134/9999): loss=0.04403513962026742\n",
      "Gradient Descent(3135/9999): loss=0.04403503315605582\n",
      "Gradient Descent(3136/9999): loss=0.04403492685924143\n",
      "Gradient Descent(3137/9999): loss=0.04403482072956104\n",
      "Gradient Descent(3138/9999): loss=0.044034714766751845\n",
      "Gradient Descent(3139/9999): loss=0.04403460897055149\n",
      "Gradient Descent(3140/9999): loss=0.044034503340698\n",
      "Gradient Descent(3141/9999): loss=0.04403439787692981\n",
      "Gradient Descent(3142/9999): loss=0.044034292578985806\n",
      "Gradient Descent(3143/9999): loss=0.044034187446605255\n",
      "Gradient Descent(3144/9999): loss=0.0440340824795278\n",
      "Gradient Descent(3145/9999): loss=0.04403397767749358\n",
      "Gradient Descent(3146/9999): loss=0.04403387304024303\n",
      "Gradient Descent(3147/9999): loss=0.04403376856751711\n",
      "Gradient Descent(3148/9999): loss=0.044033664259057116\n",
      "Gradient Descent(3149/9999): loss=0.044033560114604756\n",
      "Gradient Descent(3150/9999): loss=0.044033456133902166\n",
      "Gradient Descent(3151/9999): loss=0.04403335231669188\n",
      "Gradient Descent(3152/9999): loss=0.04403324866271682\n",
      "Gradient Descent(3153/9999): loss=0.04403314517172034\n",
      "Gradient Descent(3154/9999): loss=0.04403304184344618\n",
      "Gradient Descent(3155/9999): loss=0.044032938677638474\n",
      "Gradient Descent(3156/9999): loss=0.04403283567404178\n",
      "Gradient Descent(3157/9999): loss=0.04403273283240106\n",
      "Gradient Descent(3158/9999): loss=0.04403263015246166\n",
      "Gradient Descent(3159/9999): loss=0.044032527633969316\n",
      "Gradient Descent(3160/9999): loss=0.04403242527667017\n",
      "Gradient Descent(3161/9999): loss=0.044032323080310805\n",
      "Gradient Descent(3162/9999): loss=0.04403222104463818\n",
      "Gradient Descent(3163/9999): loss=0.04403211916939959\n",
      "Gradient Descent(3164/9999): loss=0.044032017454342806\n",
      "Gradient Descent(3165/9999): loss=0.044031915899215984\n",
      "Gradient Descent(3166/9999): loss=0.04403181450376764\n",
      "Gradient Descent(3167/9999): loss=0.044031713267746714\n",
      "Gradient Descent(3168/9999): loss=0.04403161219090255\n",
      "Gradient Descent(3169/9999): loss=0.04403151127298483\n",
      "Gradient Descent(3170/9999): loss=0.044031410513743714\n",
      "Gradient Descent(3171/9999): loss=0.04403130991292966\n",
      "Gradient Descent(3172/9999): loss=0.044031209470293604\n",
      "Gradient Descent(3173/9999): loss=0.04403110918558682\n",
      "Gradient Descent(3174/9999): loss=0.04403100905856101\n",
      "Gradient Descent(3175/9999): loss=0.044030909088968215\n",
      "Gradient Descent(3176/9999): loss=0.04403080927656095\n",
      "Gradient Descent(3177/9999): loss=0.04403070962109202\n",
      "Gradient Descent(3178/9999): loss=0.044030610122314684\n",
      "Gradient Descent(3179/9999): loss=0.044030510779982567\n",
      "Gradient Descent(3180/9999): loss=0.044030411593849676\n",
      "Gradient Descent(3181/9999): loss=0.044030312563670425\n",
      "Gradient Descent(3182/9999): loss=0.04403021368919961\n",
      "Gradient Descent(3183/9999): loss=0.04403011497019239\n",
      "Gradient Descent(3184/9999): loss=0.04403001640640433\n",
      "Gradient Descent(3185/9999): loss=0.04402991799759137\n",
      "Gradient Descent(3186/9999): loss=0.04402981974350986\n",
      "Gradient Descent(3187/9999): loss=0.04402972164391646\n",
      "Gradient Descent(3188/9999): loss=0.04402962369856834\n",
      "Gradient Descent(3189/9999): loss=0.04402952590722289\n",
      "Gradient Descent(3190/9999): loss=0.044029428269638016\n",
      "Gradient Descent(3191/9999): loss=0.04402933078557198\n",
      "Gradient Descent(3192/9999): loss=0.04402923345478332\n",
      "Gradient Descent(3193/9999): loss=0.04402913627703111\n",
      "Gradient Descent(3194/9999): loss=0.04402903925207469\n",
      "Gradient Descent(3195/9999): loss=0.044028942379673815\n",
      "Gradient Descent(3196/9999): loss=0.04402884565958862\n",
      "Gradient Descent(3197/9999): loss=0.04402874909157962\n",
      "Gradient Descent(3198/9999): loss=0.044028652675407676\n",
      "Gradient Descent(3199/9999): loss=0.044028556410834085\n",
      "Gradient Descent(3200/9999): loss=0.04402846029762046\n",
      "Gradient Descent(3201/9999): loss=0.044028364335528834\n",
      "Gradient Descent(3202/9999): loss=0.04402826852432157\n",
      "Gradient Descent(3203/9999): loss=0.04402817286376143\n",
      "Gradient Descent(3204/9999): loss=0.04402807735361157\n",
      "Gradient Descent(3205/9999): loss=0.04402798199363545\n",
      "Gradient Descent(3206/9999): loss=0.044027886783597\n",
      "Gradient Descent(3207/9999): loss=0.04402779172326045\n",
      "Gradient Descent(3208/9999): loss=0.04402769681239039\n",
      "Gradient Descent(3209/9999): loss=0.044027602050751825\n",
      "Gradient Descent(3210/9999): loss=0.04402750743811014\n",
      "Gradient Descent(3211/9999): loss=0.04402741297423103\n",
      "Gradient Descent(3212/9999): loss=0.044027318658880626\n",
      "Gradient Descent(3213/9999): loss=0.04402722449182533\n",
      "Gradient Descent(3214/9999): loss=0.044027130472832035\n",
      "Gradient Descent(3215/9999): loss=0.04402703660166789\n",
      "Gradient Descent(3216/9999): loss=0.044026942878100515\n",
      "Gradient Descent(3217/9999): loss=0.044026849301897764\n",
      "Gradient Descent(3218/9999): loss=0.04402675587282803\n",
      "Gradient Descent(3219/9999): loss=0.04402666259065986\n",
      "Gradient Descent(3220/9999): loss=0.044026569455162375\n",
      "Gradient Descent(3221/9999): loss=0.04402647646610487\n",
      "Gradient Descent(3222/9999): loss=0.044026383623257156\n",
      "Gradient Descent(3223/9999): loss=0.04402629092638932\n",
      "Gradient Descent(3224/9999): loss=0.044026198375271844\n",
      "Gradient Descent(3225/9999): loss=0.04402610596967554\n",
      "Gradient Descent(3226/9999): loss=0.04402601370937162\n",
      "Gradient Descent(3227/9999): loss=0.04402592159413162\n",
      "Gradient Descent(3228/9999): loss=0.044025829623727475\n",
      "Gradient Descent(3229/9999): loss=0.04402573779793142\n",
      "Gradient Descent(3230/9999): loss=0.04402564611651608\n",
      "Gradient Descent(3231/9999): loss=0.04402555457925451\n",
      "Gradient Descent(3232/9999): loss=0.04402546318591999\n",
      "Gradient Descent(3233/9999): loss=0.04402537193628623\n",
      "Gradient Descent(3234/9999): loss=0.044025280830127286\n",
      "Gradient Descent(3235/9999): loss=0.044025189867217575\n",
      "Gradient Descent(3236/9999): loss=0.044025099047331845\n",
      "Gradient Descent(3237/9999): loss=0.04402500837024524\n",
      "Gradient Descent(3238/9999): loss=0.04402491783573323\n",
      "Gradient Descent(3239/9999): loss=0.044024827443571585\n",
      "Gradient Descent(3240/9999): loss=0.044024737193536556\n",
      "Gradient Descent(3241/9999): loss=0.044024647085404656\n",
      "Gradient Descent(3242/9999): loss=0.04402455711895271\n",
      "Gradient Descent(3243/9999): loss=0.044024467293958015\n",
      "Gradient Descent(3244/9999): loss=0.04402437761019813\n",
      "Gradient Descent(3245/9999): loss=0.044024288067451\n",
      "Gradient Descent(3246/9999): loss=0.0440241986654949\n",
      "Gradient Descent(3247/9999): loss=0.04402410940410842\n",
      "Gradient Descent(3248/9999): loss=0.044024020283070585\n",
      "Gradient Descent(3249/9999): loss=0.044023931302160715\n",
      "Gradient Descent(3250/9999): loss=0.044023842461158445\n",
      "Gradient Descent(3251/9999): loss=0.04402375375984385\n",
      "Gradient Descent(3252/9999): loss=0.04402366519799727\n",
      "Gradient Descent(3253/9999): loss=0.044023576775399385\n",
      "Gradient Descent(3254/9999): loss=0.0440234884918313\n",
      "Gradient Descent(3255/9999): loss=0.04402340034707437\n",
      "Gradient Descent(3256/9999): loss=0.04402331234091037\n",
      "Gradient Descent(3257/9999): loss=0.04402322447312138\n",
      "Gradient Descent(3258/9999): loss=0.044023136743489816\n",
      "Gradient Descent(3259/9999): loss=0.04402304915179846\n",
      "Gradient Descent(3260/9999): loss=0.04402296169783039\n",
      "Gradient Descent(3261/9999): loss=0.04402287438136913\n",
      "Gradient Descent(3262/9999): loss=0.044022787202198384\n",
      "Gradient Descent(3263/9999): loss=0.044022700160102374\n",
      "Gradient Descent(3264/9999): loss=0.04402261325486549\n",
      "Gradient Descent(3265/9999): loss=0.04402252648627262\n",
      "Gradient Descent(3266/9999): loss=0.044022439854108864\n",
      "Gradient Descent(3267/9999): loss=0.044022353358159724\n",
      "Gradient Descent(3268/9999): loss=0.04402226699821101\n",
      "Gradient Descent(3269/9999): loss=0.044022180774048944\n",
      "Gradient Descent(3270/9999): loss=0.044022094685459943\n",
      "Gradient Descent(3271/9999): loss=0.04402200873223088\n",
      "Gradient Descent(3272/9999): loss=0.044021922914148935\n",
      "Gradient Descent(3273/9999): loss=0.04402183723100161\n",
      "Gradient Descent(3274/9999): loss=0.04402175168257672\n",
      "Gradient Descent(3275/9999): loss=0.044021666268662434\n",
      "Gradient Descent(3276/9999): loss=0.04402158098904731\n",
      "Gradient Descent(3277/9999): loss=0.04402149584352011\n",
      "Gradient Descent(3278/9999): loss=0.04402141083187004\n",
      "Gradient Descent(3279/9999): loss=0.044021325953886616\n",
      "Gradient Descent(3280/9999): loss=0.04402124120935964\n",
      "Gradient Descent(3281/9999): loss=0.0440211565980793\n",
      "Gradient Descent(3282/9999): loss=0.04402107211983606\n",
      "Gradient Descent(3283/9999): loss=0.044020987774420775\n",
      "Gradient Descent(3284/9999): loss=0.04402090356162457\n",
      "Gradient Descent(3285/9999): loss=0.04402081948123891\n",
      "Gradient Descent(3286/9999): loss=0.044020735533055654\n",
      "Gradient Descent(3287/9999): loss=0.04402065171686689\n",
      "Gradient Descent(3288/9999): loss=0.04402056803246508\n",
      "Gradient Descent(3289/9999): loss=0.044020484479643046\n",
      "Gradient Descent(3290/9999): loss=0.04402040105819387\n",
      "Gradient Descent(3291/9999): loss=0.044020317767911\n",
      "Gradient Descent(3292/9999): loss=0.04402023460858819\n",
      "Gradient Descent(3293/9999): loss=0.044020151580019526\n",
      "Gradient Descent(3294/9999): loss=0.04402006868199946\n",
      "Gradient Descent(3295/9999): loss=0.04401998591432266\n",
      "Gradient Descent(3296/9999): loss=0.04401990327678423\n",
      "Gradient Descent(3297/9999): loss=0.04401982076917951\n",
      "Gradient Descent(3298/9999): loss=0.04401973839130426\n",
      "Gradient Descent(3299/9999): loss=0.044019656142954465\n",
      "Gradient Descent(3300/9999): loss=0.044019574023926464\n",
      "Gradient Descent(3301/9999): loss=0.044019492034016915\n",
      "Gradient Descent(3302/9999): loss=0.04401941017302284\n",
      "Gradient Descent(3303/9999): loss=0.04401932844074148\n",
      "Gradient Descent(3304/9999): loss=0.04401924683697053\n",
      "Gradient Descent(3305/9999): loss=0.04401916536150786\n",
      "Gradient Descent(3306/9999): loss=0.04401908401415176\n",
      "Gradient Descent(3307/9999): loss=0.04401900279470079\n",
      "Gradient Descent(3308/9999): loss=0.04401892170295387\n",
      "Gradient Descent(3309/9999): loss=0.044018840738710166\n",
      "Gradient Descent(3310/9999): loss=0.04401875990176925\n",
      "Gradient Descent(3311/9999): loss=0.0440186791919309\n",
      "Gradient Descent(3312/9999): loss=0.04401859860899533\n",
      "Gradient Descent(3313/9999): loss=0.04401851815276296\n",
      "Gradient Descent(3314/9999): loss=0.04401843782303458\n",
      "Gradient Descent(3315/9999): loss=0.0440183576196113\n",
      "Gradient Descent(3316/9999): loss=0.04401827754229449\n",
      "Gradient Descent(3317/9999): loss=0.044018197590885925\n",
      "Gradient Descent(3318/9999): loss=0.044018117765187596\n",
      "Gradient Descent(3319/9999): loss=0.04401803806500183\n",
      "Gradient Descent(3320/9999): loss=0.04401795849013132\n",
      "Gradient Descent(3321/9999): loss=0.04401787904037902\n",
      "Gradient Descent(3322/9999): loss=0.04401779971554819\n",
      "Gradient Descent(3323/9999): loss=0.04401772051544242\n",
      "Gradient Descent(3324/9999): loss=0.04401764143986558\n",
      "Gradient Descent(3325/9999): loss=0.044017562488621896\n",
      "Gradient Descent(3326/9999): loss=0.044017483661515866\n",
      "Gradient Descent(3327/9999): loss=0.04401740495835229\n",
      "Gradient Descent(3328/9999): loss=0.044017326378936315\n",
      "Gradient Descent(3329/9999): loss=0.04401724792307338\n",
      "Gradient Descent(3330/9999): loss=0.044017169590569144\n",
      "Gradient Descent(3331/9999): loss=0.04401709138122973\n",
      "Gradient Descent(3332/9999): loss=0.04401701329486143\n",
      "Gradient Descent(3333/9999): loss=0.04401693533127091\n",
      "Gradient Descent(3334/9999): loss=0.04401685749026511\n",
      "Gradient Descent(3335/9999): loss=0.04401677977165133\n",
      "Gradient Descent(3336/9999): loss=0.04401670217523709\n",
      "Gradient Descent(3337/9999): loss=0.04401662470083026\n",
      "Gradient Descent(3338/9999): loss=0.04401654734823898\n",
      "Gradient Descent(3339/9999): loss=0.04401647011727177\n",
      "Gradient Descent(3340/9999): loss=0.04401639300773733\n",
      "Gradient Descent(3341/9999): loss=0.044016316019444765\n",
      "Gradient Descent(3342/9999): loss=0.044016239152203464\n",
      "Gradient Descent(3343/9999): loss=0.044016162405823046\n",
      "Gradient Descent(3344/9999): loss=0.044016085780113554\n",
      "Gradient Descent(3345/9999): loss=0.04401600927488514\n",
      "Gradient Descent(3346/9999): loss=0.04401593288994842\n",
      "Gradient Descent(3347/9999): loss=0.04401585662511432\n",
      "Gradient Descent(3348/9999): loss=0.04401578048019391\n",
      "Gradient Descent(3349/9999): loss=0.04401570445499869\n",
      "Gradient Descent(3350/9999): loss=0.04401562854934037\n",
      "Gradient Descent(3351/9999): loss=0.04401555276303104\n",
      "Gradient Descent(3352/9999): loss=0.044015477095883035\n",
      "Gradient Descent(3353/9999): loss=0.04401540154770902\n",
      "Gradient Descent(3354/9999): loss=0.044015326118321874\n",
      "Gradient Descent(3355/9999): loss=0.04401525080753482\n",
      "Gradient Descent(3356/9999): loss=0.04401517561516144\n",
      "Gradient Descent(3357/9999): loss=0.04401510054101551\n",
      "Gradient Descent(3358/9999): loss=0.04401502558491115\n",
      "Gradient Descent(3359/9999): loss=0.04401495074666275\n",
      "Gradient Descent(3360/9999): loss=0.044014876026085\n",
      "Gradient Descent(3361/9999): loss=0.04401480142299288\n",
      "Gradient Descent(3362/9999): loss=0.04401472693720168\n",
      "Gradient Descent(3363/9999): loss=0.044014652568526935\n",
      "Gradient Descent(3364/9999): loss=0.04401457831678454\n",
      "Gradient Descent(3365/9999): loss=0.04401450418179059\n",
      "Gradient Descent(3366/9999): loss=0.044014430163361574\n",
      "Gradient Descent(3367/9999): loss=0.04401435626131418\n",
      "Gradient Descent(3368/9999): loss=0.0440142824754654\n",
      "Gradient Descent(3369/9999): loss=0.04401420880563256\n",
      "Gradient Descent(3370/9999): loss=0.044014135251633224\n",
      "Gradient Descent(3371/9999): loss=0.04401406181328531\n",
      "Gradient Descent(3372/9999): loss=0.0440139884904069\n",
      "Gradient Descent(3373/9999): loss=0.04401391528281651\n",
      "Gradient Descent(3374/9999): loss=0.0440138421903328\n",
      "Gradient Descent(3375/9999): loss=0.04401376921277483\n",
      "Gradient Descent(3376/9999): loss=0.044013696349961905\n",
      "Gradient Descent(3377/9999): loss=0.04401362360171358\n",
      "Gradient Descent(3378/9999): loss=0.04401355096784974\n",
      "Gradient Descent(3379/9999): loss=0.04401347844819052\n",
      "Gradient Descent(3380/9999): loss=0.044013406042556366\n",
      "Gradient Descent(3381/9999): loss=0.04401333375076799\n",
      "Gradient Descent(3382/9999): loss=0.04401326157264638\n",
      "Gradient Descent(3383/9999): loss=0.044013189508012814\n",
      "Gradient Descent(3384/9999): loss=0.044013117556688845\n",
      "Gradient Descent(3385/9999): loss=0.044013045718496346\n",
      "Gradient Descent(3386/9999): loss=0.044012973993257404\n",
      "Gradient Descent(3387/9999): loss=0.044012902380794416\n",
      "Gradient Descent(3388/9999): loss=0.0440128308809301\n",
      "Gradient Descent(3389/9999): loss=0.04401275949348735\n",
      "Gradient Descent(3390/9999): loss=0.04401268821828947\n",
      "Gradient Descent(3391/9999): loss=0.04401261705515993\n",
      "Gradient Descent(3392/9999): loss=0.04401254600392252\n",
      "Gradient Descent(3393/9999): loss=0.04401247506440135\n",
      "Gradient Descent(3394/9999): loss=0.044012404236420714\n",
      "Gradient Descent(3395/9999): loss=0.04401233351980524\n",
      "Gradient Descent(3396/9999): loss=0.04401226291437985\n",
      "Gradient Descent(3397/9999): loss=0.04401219241996972\n",
      "Gradient Descent(3398/9999): loss=0.04401212203640028\n",
      "Gradient Descent(3399/9999): loss=0.04401205176349727\n",
      "Gradient Descent(3400/9999): loss=0.04401198160108664\n",
      "Gradient Descent(3401/9999): loss=0.04401191154899469\n",
      "Gradient Descent(3402/9999): loss=0.04401184160704799\n",
      "Gradient Descent(3403/9999): loss=0.0440117717750733\n",
      "Gradient Descent(3404/9999): loss=0.04401170205289774\n",
      "Gradient Descent(3405/9999): loss=0.044011632440348676\n",
      "Gradient Descent(3406/9999): loss=0.04401156293725372\n",
      "Gradient Descent(3407/9999): loss=0.04401149354344078\n",
      "Gradient Descent(3408/9999): loss=0.044011424258738024\n",
      "Gradient Descent(3409/9999): loss=0.044011355082973894\n",
      "Gradient Descent(3410/9999): loss=0.04401128601597712\n",
      "Gradient Descent(3411/9999): loss=0.04401121705757664\n",
      "Gradient Descent(3412/9999): loss=0.04401114820760179\n",
      "Gradient Descent(3413/9999): loss=0.04401107946588198\n",
      "Gradient Descent(3414/9999): loss=0.04401101083224706\n",
      "Gradient Descent(3415/9999): loss=0.04401094230652708\n",
      "Gradient Descent(3416/9999): loss=0.04401087388855236\n",
      "Gradient Descent(3417/9999): loss=0.04401080557815347\n",
      "Gradient Descent(3418/9999): loss=0.04401073737516132\n",
      "Gradient Descent(3419/9999): loss=0.04401066927940695\n",
      "Gradient Descent(3420/9999): loss=0.04401060129072182\n",
      "Gradient Descent(3421/9999): loss=0.04401053340893753\n",
      "Gradient Descent(3422/9999): loss=0.044010465633886\n",
      "Gradient Descent(3423/9999): loss=0.04401039796539944\n",
      "Gradient Descent(3424/9999): loss=0.04401033040331028\n",
      "Gradient Descent(3425/9999): loss=0.04401026294745123\n",
      "Gradient Descent(3426/9999): loss=0.04401019559765526\n",
      "Gradient Descent(3427/9999): loss=0.04401012835375561\n",
      "Gradient Descent(3428/9999): loss=0.04401006121558576\n",
      "Gradient Descent(3429/9999): loss=0.04400999418297947\n",
      "Gradient Descent(3430/9999): loss=0.04400992725577077\n",
      "Gradient Descent(3431/9999): loss=0.04400986043379392\n",
      "Gradient Descent(3432/9999): loss=0.04400979371688349\n",
      "Gradient Descent(3433/9999): loss=0.044009727104874254\n",
      "Gradient Descent(3434/9999): loss=0.044009660597601315\n",
      "Gradient Descent(3435/9999): loss=0.044009594194899915\n",
      "Gradient Descent(3436/9999): loss=0.044009527896605706\n",
      "Gradient Descent(3437/9999): loss=0.04400946170255449\n",
      "Gradient Descent(3438/9999): loss=0.0440093956125824\n",
      "Gradient Descent(3439/9999): loss=0.04400932962652573\n",
      "Gradient Descent(3440/9999): loss=0.04400926374422113\n",
      "Gradient Descent(3441/9999): loss=0.04400919796550545\n",
      "Gradient Descent(3442/9999): loss=0.044009132290215816\n",
      "Gradient Descent(3443/9999): loss=0.044009066718189604\n",
      "Gradient Descent(3444/9999): loss=0.044009001249264476\n",
      "Gradient Descent(3445/9999): loss=0.0440089358832783\n",
      "Gradient Descent(3446/9999): loss=0.04400887062006923\n",
      "Gradient Descent(3447/9999): loss=0.044008805459475664\n",
      "Gradient Descent(3448/9999): loss=0.04400874040133627\n",
      "Gradient Descent(3449/9999): loss=0.0440086754454899\n",
      "Gradient Descent(3450/9999): loss=0.04400861059177579\n",
      "Gradient Descent(3451/9999): loss=0.0440085458400333\n",
      "Gradient Descent(3452/9999): loss=0.04400848119010213\n",
      "Gradient Descent(3453/9999): loss=0.04400841664182217\n",
      "Gradient Descent(3454/9999): loss=0.04400835219503363\n",
      "Gradient Descent(3455/9999): loss=0.044008287849576895\n",
      "Gradient Descent(3456/9999): loss=0.04400822360529264\n",
      "Gradient Descent(3457/9999): loss=0.044008159462021815\n",
      "Gradient Descent(3458/9999): loss=0.044008095419605577\n",
      "Gradient Descent(3459/9999): loss=0.04400803147788534\n",
      "Gradient Descent(3460/9999): loss=0.04400796763670277\n",
      "Gradient Descent(3461/9999): loss=0.04400790389589985\n",
      "Gradient Descent(3462/9999): loss=0.044007840255318634\n",
      "Gradient Descent(3463/9999): loss=0.044007776714801675\n",
      "Gradient Descent(3464/9999): loss=0.044007713274191555\n",
      "Gradient Descent(3465/9999): loss=0.044007649933331186\n",
      "Gradient Descent(3466/9999): loss=0.04400758669206375\n",
      "Gradient Descent(3467/9999): loss=0.04400752355023265\n",
      "Gradient Descent(3468/9999): loss=0.044007460507681555\n",
      "Gradient Descent(3469/9999): loss=0.044007397564254355\n",
      "Gradient Descent(3470/9999): loss=0.044007334719795156\n",
      "Gradient Descent(3471/9999): loss=0.044007271974148415\n",
      "Gradient Descent(3472/9999): loss=0.044007209327158694\n",
      "Gradient Descent(3473/9999): loss=0.04400714677867094\n",
      "Gradient Descent(3474/9999): loss=0.0440070843285302\n",
      "Gradient Descent(3475/9999): loss=0.044007021976581934\n",
      "Gradient Descent(3476/9999): loss=0.044006959722671685\n",
      "Gradient Descent(3477/9999): loss=0.04400689756664531\n",
      "Gradient Descent(3478/9999): loss=0.044006835508348885\n",
      "Gradient Descent(3479/9999): loss=0.04400677354762881\n",
      "Gradient Descent(3480/9999): loss=0.04400671168433162\n",
      "Gradient Descent(3481/9999): loss=0.04400664991830412\n",
      "Gradient Descent(3482/9999): loss=0.0440065882493934\n",
      "Gradient Descent(3483/9999): loss=0.04400652667744675\n",
      "Gradient Descent(3484/9999): loss=0.04400646520231171\n",
      "Gradient Descent(3485/9999): loss=0.04400640382383607\n",
      "Gradient Descent(3486/9999): loss=0.04400634254186779\n",
      "Gradient Descent(3487/9999): loss=0.04400628135625523\n",
      "Gradient Descent(3488/9999): loss=0.04400622026684681\n",
      "Gradient Descent(3489/9999): loss=0.04400615927349128\n",
      "Gradient Descent(3490/9999): loss=0.044006098376037646\n",
      "Gradient Descent(3491/9999): loss=0.04400603757433507\n",
      "Gradient Descent(3492/9999): loss=0.044005976868233036\n",
      "Gradient Descent(3493/9999): loss=0.044005916257581205\n",
      "Gradient Descent(3494/9999): loss=0.04400585574222951\n",
      "Gradient Descent(3495/9999): loss=0.04400579532202811\n",
      "Gradient Descent(3496/9999): loss=0.04400573499682737\n",
      "Gradient Descent(3497/9999): loss=0.044005674766477966\n",
      "Gradient Descent(3498/9999): loss=0.04400561463083071\n",
      "Gradient Descent(3499/9999): loss=0.04400555458973675\n",
      "Gradient Descent(3500/9999): loss=0.04400549464304736\n",
      "Gradient Descent(3501/9999): loss=0.044005434790614154\n",
      "Gradient Descent(3502/9999): loss=0.04400537503228891\n",
      "Gradient Descent(3503/9999): loss=0.04400531536792363\n",
      "Gradient Descent(3504/9999): loss=0.04400525579737063\n",
      "Gradient Descent(3505/9999): loss=0.04400519632048235\n",
      "Gradient Descent(3506/9999): loss=0.04400513693711158\n",
      "Gradient Descent(3507/9999): loss=0.044005077647111236\n",
      "Gradient Descent(3508/9999): loss=0.04400501845033454\n",
      "Gradient Descent(3509/9999): loss=0.044004959346634884\n",
      "Gradient Descent(3510/9999): loss=0.044004900335865925\n",
      "Gradient Descent(3511/9999): loss=0.044004841417881545\n",
      "Gradient Descent(3512/9999): loss=0.04400478259253586\n",
      "Gradient Descent(3513/9999): loss=0.04400472385968323\n",
      "Gradient Descent(3514/9999): loss=0.04400466521917819\n",
      "Gradient Descent(3515/9999): loss=0.04400460667087556\n",
      "Gradient Descent(3516/9999): loss=0.04400454821463036\n",
      "Gradient Descent(3517/9999): loss=0.044004489850297854\n",
      "Gradient Descent(3518/9999): loss=0.044004431577733524\n",
      "Gradient Descent(3519/9999): loss=0.04400437339679307\n",
      "Gradient Descent(3520/9999): loss=0.04400431530733242\n",
      "Gradient Descent(3521/9999): loss=0.044004257309207775\n",
      "Gradient Descent(3522/9999): loss=0.044004199402275485\n",
      "Gradient Descent(3523/9999): loss=0.04400414158639218\n",
      "Gradient Descent(3524/9999): loss=0.04400408386141472\n",
      "Gradient Descent(3525/9999): loss=0.044004026227200115\n",
      "Gradient Descent(3526/9999): loss=0.044003968683605725\n",
      "Gradient Descent(3527/9999): loss=0.04400391123048901\n",
      "Gradient Descent(3528/9999): loss=0.04400385386770775\n",
      "Gradient Descent(3529/9999): loss=0.04400379659511986\n",
      "Gradient Descent(3530/9999): loss=0.04400373941258359\n",
      "Gradient Descent(3531/9999): loss=0.044003682319957306\n",
      "Gradient Descent(3532/9999): loss=0.044003625317099636\n",
      "Gradient Descent(3533/9999): loss=0.044003568403869445\n",
      "Gradient Descent(3534/9999): loss=0.044003511580125804\n",
      "Gradient Descent(3535/9999): loss=0.04400345484572801\n",
      "Gradient Descent(3536/9999): loss=0.04400339820053561\n",
      "Gradient Descent(3537/9999): loss=0.0440033416444083\n",
      "Gradient Descent(3538/9999): loss=0.04400328517720608\n",
      "Gradient Descent(3539/9999): loss=0.044003228798789096\n",
      "Gradient Descent(3540/9999): loss=0.04400317250901775\n",
      "Gradient Descent(3541/9999): loss=0.044003116307752714\n",
      "Gradient Descent(3542/9999): loss=0.04400306019485476\n",
      "Gradient Descent(3543/9999): loss=0.04400300417018497\n",
      "Gradient Descent(3544/9999): loss=0.04400294823360463\n",
      "Gradient Descent(3545/9999): loss=0.04400289238497521\n",
      "Gradient Descent(3546/9999): loss=0.04400283662415844\n",
      "Gradient Descent(3547/9999): loss=0.04400278095101626\n",
      "Gradient Descent(3548/9999): loss=0.04400272536541079\n",
      "Gradient Descent(3549/9999): loss=0.044002669867204414\n",
      "Gradient Descent(3550/9999): loss=0.044002614456259706\n",
      "Gradient Descent(3551/9999): loss=0.044002559132439466\n",
      "Gradient Descent(3552/9999): loss=0.04400250389560667\n",
      "Gradient Descent(3553/9999): loss=0.0440024487456246\n",
      "Gradient Descent(3554/9999): loss=0.04400239368235668\n",
      "Gradient Descent(3555/9999): loss=0.044002338705666534\n",
      "Gradient Descent(3556/9999): loss=0.04400228381541806\n",
      "Gradient Descent(3557/9999): loss=0.04400222901147535\n",
      "Gradient Descent(3558/9999): loss=0.044002174293702685\n",
      "Gradient Descent(3559/9999): loss=0.04400211966196459\n",
      "Gradient Descent(3560/9999): loss=0.04400206511612576\n",
      "Gradient Descent(3561/9999): loss=0.044002010656051194\n",
      "Gradient Descent(3562/9999): loss=0.044001956281605976\n",
      "Gradient Descent(3563/9999): loss=0.04400190199265551\n",
      "Gradient Descent(3564/9999): loss=0.04400184778906535\n",
      "Gradient Descent(3565/9999): loss=0.0440017936707013\n",
      "Gradient Descent(3566/9999): loss=0.04400173963742936\n",
      "Gradient Descent(3567/9999): loss=0.044001685689115666\n",
      "Gradient Descent(3568/9999): loss=0.04400163182562673\n",
      "Gradient Descent(3569/9999): loss=0.04400157804682913\n",
      "Gradient Descent(3570/9999): loss=0.04400152435258973\n",
      "Gradient Descent(3571/9999): loss=0.044001470742775516\n",
      "Gradient Descent(3572/9999): loss=0.04400141721725381\n",
      "Gradient Descent(3573/9999): loss=0.04400136377589207\n",
      "Gradient Descent(3574/9999): loss=0.04400131041855793\n",
      "Gradient Descent(3575/9999): loss=0.04400125714511931\n",
      "Gradient Descent(3576/9999): loss=0.04400120395544427\n",
      "Gradient Descent(3577/9999): loss=0.044001150849401094\n",
      "Gradient Descent(3578/9999): loss=0.04400109782685834\n",
      "Gradient Descent(3579/9999): loss=0.04400104488768468\n",
      "Gradient Descent(3580/9999): loss=0.04400099203174901\n",
      "Gradient Descent(3581/9999): loss=0.04400093925892049\n",
      "Gradient Descent(3582/9999): loss=0.04400088656906845\n",
      "Gradient Descent(3583/9999): loss=0.04400083396206237\n",
      "Gradient Descent(3584/9999): loss=0.04400078143777206\n",
      "Gradient Descent(3585/9999): loss=0.04400072899606743\n",
      "Gradient Descent(3586/9999): loss=0.04400067663681859\n",
      "Gradient Descent(3587/9999): loss=0.04400062435989596\n",
      "Gradient Descent(3588/9999): loss=0.04400057216517005\n",
      "Gradient Descent(3589/9999): loss=0.04400052005251166\n",
      "Gradient Descent(3590/9999): loss=0.04400046802179172\n",
      "Gradient Descent(3591/9999): loss=0.04400041607288138\n",
      "Gradient Descent(3592/9999): loss=0.04400036420565203\n",
      "Gradient Descent(3593/9999): loss=0.04400031241997527\n",
      "Gradient Descent(3594/9999): loss=0.04400026071572283\n",
      "Gradient Descent(3595/9999): loss=0.04400020909276667\n",
      "Gradient Descent(3596/9999): loss=0.04400015755097901\n",
      "Gradient Descent(3597/9999): loss=0.04400010609023222\n",
      "Gradient Descent(3598/9999): loss=0.04400005471039888\n",
      "Gradient Descent(3599/9999): loss=0.044000003411351706\n",
      "Gradient Descent(3600/9999): loss=0.04399995219296374\n",
      "Gradient Descent(3601/9999): loss=0.04399990105510816\n",
      "Gradient Descent(3602/9999): loss=0.04399984999765829\n",
      "Gradient Descent(3603/9999): loss=0.043999799020487756\n",
      "Gradient Descent(3604/9999): loss=0.04399974812347031\n",
      "Gradient Descent(3605/9999): loss=0.04399969730647993\n",
      "Gradient Descent(3606/9999): loss=0.04399964656939079\n",
      "Gradient Descent(3607/9999): loss=0.04399959591207724\n",
      "Gradient Descent(3608/9999): loss=0.043999545334413866\n",
      "Gradient Descent(3609/9999): loss=0.04399949483627542\n",
      "Gradient Descent(3610/9999): loss=0.043999444417536865\n",
      "Gradient Descent(3611/9999): loss=0.04399939407807338\n",
      "Gradient Descent(3612/9999): loss=0.04399934381776029\n",
      "Gradient Descent(3613/9999): loss=0.043999293636473154\n",
      "Gradient Descent(3614/9999): loss=0.043999243534087705\n",
      "Gradient Descent(3615/9999): loss=0.04399919351047989\n",
      "Gradient Descent(3616/9999): loss=0.04399914356552588\n",
      "Gradient Descent(3617/9999): loss=0.04399909369910195\n",
      "Gradient Descent(3618/9999): loss=0.04399904391108465\n",
      "Gradient Descent(3619/9999): loss=0.0439989942013507\n",
      "Gradient Descent(3620/9999): loss=0.043998944569777\n",
      "Gradient Descent(3621/9999): loss=0.04399889501624068\n",
      "Gradient Descent(3622/9999): loss=0.043998845540618996\n",
      "Gradient Descent(3623/9999): loss=0.043998796142789495\n",
      "Gradient Descent(3624/9999): loss=0.04399874682262984\n",
      "Gradient Descent(3625/9999): loss=0.0439986975800179\n",
      "Gradient Descent(3626/9999): loss=0.04399864841483174\n",
      "Gradient Descent(3627/9999): loss=0.043998599326949626\n",
      "Gradient Descent(3628/9999): loss=0.04399855031625001\n",
      "Gradient Descent(3629/9999): loss=0.043998501382611535\n",
      "Gradient Descent(3630/9999): loss=0.04399845252591307\n",
      "Gradient Descent(3631/9999): loss=0.043998403746033594\n",
      "Gradient Descent(3632/9999): loss=0.04399835504285231\n",
      "Gradient Descent(3633/9999): loss=0.04399830641624867\n",
      "Gradient Descent(3634/9999): loss=0.04399825786610225\n",
      "Gradient Descent(3635/9999): loss=0.04399820939229282\n",
      "Gradient Descent(3636/9999): loss=0.04399816099470037\n",
      "Gradient Descent(3637/9999): loss=0.04399811267320505\n",
      "Gradient Descent(3638/9999): loss=0.04399806442768723\n",
      "Gradient Descent(3639/9999): loss=0.043998016258027434\n",
      "Gradient Descent(3640/9999): loss=0.04399796816410637\n",
      "Gradient Descent(3641/9999): loss=0.04399792014580498\n",
      "Gradient Descent(3642/9999): loss=0.04399787220300436\n",
      "Gradient Descent(3643/9999): loss=0.043997824335585786\n",
      "Gradient Descent(3644/9999): loss=0.04399777654343074\n",
      "Gradient Descent(3645/9999): loss=0.04399772882642087\n",
      "Gradient Descent(3646/9999): loss=0.04399768118443804\n",
      "Gradient Descent(3647/9999): loss=0.043997633617364265\n",
      "Gradient Descent(3648/9999): loss=0.04399758612508178\n",
      "Gradient Descent(3649/9999): loss=0.043997538707472986\n",
      "Gradient Descent(3650/9999): loss=0.043997491364420466\n",
      "Gradient Descent(3651/9999): loss=0.04399744409580699\n",
      "Gradient Descent(3652/9999): loss=0.04399739690151553\n",
      "Gradient Descent(3653/9999): loss=0.04399734978142921\n",
      "Gradient Descent(3654/9999): loss=0.043997302735431364\n",
      "Gradient Descent(3655/9999): loss=0.0439972557634055\n",
      "Gradient Descent(3656/9999): loss=0.043997208865235296\n",
      "Gradient Descent(3657/9999): loss=0.043997162040804644\n",
      "Gradient Descent(3658/9999): loss=0.043997115289997585\n",
      "Gradient Descent(3659/9999): loss=0.04399706861269835\n",
      "Gradient Descent(3660/9999): loss=0.043997022008791416\n",
      "Gradient Descent(3661/9999): loss=0.04399697547816131\n",
      "Gradient Descent(3662/9999): loss=0.04399692902069288\n",
      "Gradient Descent(3663/9999): loss=0.043996882636271044\n",
      "Gradient Descent(3664/9999): loss=0.043996836324781005\n",
      "Gradient Descent(3665/9999): loss=0.04399679008610801\n",
      "Gradient Descent(3666/9999): loss=0.04399674392013762\n",
      "Gradient Descent(3667/9999): loss=0.04399669782675553\n",
      "Gradient Descent(3668/9999): loss=0.04399665180584757\n",
      "Gradient Descent(3669/9999): loss=0.04399660585729983\n",
      "Gradient Descent(3670/9999): loss=0.04399655998099849\n",
      "Gradient Descent(3671/9999): loss=0.043996514176829996\n",
      "Gradient Descent(3672/9999): loss=0.043996468444680886\n",
      "Gradient Descent(3673/9999): loss=0.04399642278443796\n",
      "Gradient Descent(3674/9999): loss=0.043996377195988146\n",
      "Gradient Descent(3675/9999): loss=0.04399633167921854\n",
      "Gradient Descent(3676/9999): loss=0.04399628623401648\n",
      "Gradient Descent(3677/9999): loss=0.04399624086026943\n",
      "Gradient Descent(3678/9999): loss=0.04399619555786499\n",
      "Gradient Descent(3679/9999): loss=0.04399615032669105\n",
      "Gradient Descent(3680/9999): loss=0.043996105166635546\n",
      "Gradient Descent(3681/9999): loss=0.043996060077586704\n",
      "Gradient Descent(3682/9999): loss=0.043996015059432896\n",
      "Gradient Descent(3683/9999): loss=0.043995970112062584\n",
      "Gradient Descent(3684/9999): loss=0.043995925235364525\n",
      "Gradient Descent(3685/9999): loss=0.043995880429227564\n",
      "Gradient Descent(3686/9999): loss=0.043995835693540784\n",
      "Gradient Descent(3687/9999): loss=0.043995791028193405\n",
      "Gradient Descent(3688/9999): loss=0.04399574643307483\n",
      "Gradient Descent(3689/9999): loss=0.04399570190807465\n",
      "Gradient Descent(3690/9999): loss=0.043995657453082586\n",
      "Gradient Descent(3691/9999): loss=0.04399561306798858\n",
      "Gradient Descent(3692/9999): loss=0.04399556875268273\n",
      "Gradient Descent(3693/9999): loss=0.04399552450705528\n",
      "Gradient Descent(3694/9999): loss=0.04399548033099673\n",
      "Gradient Descent(3695/9999): loss=0.04399543622439762\n",
      "Gradient Descent(3696/9999): loss=0.0439953921871488\n",
      "Gradient Descent(3697/9999): loss=0.04399534821914119\n",
      "Gradient Descent(3698/9999): loss=0.043995304320265945\n",
      "Gradient Descent(3699/9999): loss=0.043995260490414335\n",
      "Gradient Descent(3700/9999): loss=0.04399521672947787\n",
      "Gradient Descent(3701/9999): loss=0.043995173037348165\n",
      "Gradient Descent(3702/9999): loss=0.043995129413917054\n",
      "Gradient Descent(3703/9999): loss=0.0439950858590765\n",
      "Gradient Descent(3704/9999): loss=0.043995042372718646\n",
      "Gradient Descent(3705/9999): loss=0.04399499895473585\n",
      "Gradient Descent(3706/9999): loss=0.04399495560502057\n",
      "Gradient Descent(3707/9999): loss=0.0439949123234655\n",
      "Gradient Descent(3708/9999): loss=0.04399486910996344\n",
      "Gradient Descent(3709/9999): loss=0.04399482596440738\n",
      "Gradient Descent(3710/9999): loss=0.04399478288669054\n",
      "Gradient Descent(3711/9999): loss=0.04399473987670619\n",
      "Gradient Descent(3712/9999): loss=0.043994696934347884\n",
      "Gradient Descent(3713/9999): loss=0.04399465405950927\n",
      "Gradient Descent(3714/9999): loss=0.043994611252084155\n",
      "Gradient Descent(3715/9999): loss=0.0439945685119666\n",
      "Gradient Descent(3716/9999): loss=0.04399452583905071\n",
      "Gradient Descent(3717/9999): loss=0.043994483233230894\n",
      "Gradient Descent(3718/9999): loss=0.04399444069440161\n",
      "Gradient Descent(3719/9999): loss=0.043994398222457536\n",
      "Gradient Descent(3720/9999): loss=0.043994355817293496\n",
      "Gradient Descent(3721/9999): loss=0.04399431347880449\n",
      "Gradient Descent(3722/9999): loss=0.04399427120688571\n",
      "Gradient Descent(3723/9999): loss=0.04399422900143244\n",
      "Gradient Descent(3724/9999): loss=0.043994186862340225\n",
      "Gradient Descent(3725/9999): loss=0.043994144789504704\n",
      "Gradient Descent(3726/9999): loss=0.043994102782821676\n",
      "Gradient Descent(3727/9999): loss=0.043994060842187134\n",
      "Gradient Descent(3728/9999): loss=0.04399401896749725\n",
      "Gradient Descent(3729/9999): loss=0.04399397715864835\n",
      "Gradient Descent(3730/9999): loss=0.04399393541553685\n",
      "Gradient Descent(3731/9999): loss=0.04399389373805945\n",
      "Gradient Descent(3732/9999): loss=0.043993852126112926\n",
      "Gradient Descent(3733/9999): loss=0.04399381057959424\n",
      "Gradient Descent(3734/9999): loss=0.043993769098400526\n",
      "Gradient Descent(3735/9999): loss=0.04399372768242905\n",
      "Gradient Descent(3736/9999): loss=0.04399368633157727\n",
      "Gradient Descent(3737/9999): loss=0.043993645045742824\n",
      "Gradient Descent(3738/9999): loss=0.043993603824823445\n",
      "Gradient Descent(3739/9999): loss=0.04399356266871708\n",
      "Gradient Descent(3740/9999): loss=0.04399352157732183\n",
      "Gradient Descent(3741/9999): loss=0.043993480550535945\n",
      "Gradient Descent(3742/9999): loss=0.04399343958825783\n",
      "Gradient Descent(3743/9999): loss=0.04399339869038605\n",
      "Gradient Descent(3744/9999): loss=0.04399335785681934\n",
      "Gradient Descent(3745/9999): loss=0.043993317087456615\n",
      "Gradient Descent(3746/9999): loss=0.043993276382196925\n",
      "Gradient Descent(3747/9999): loss=0.04399323574093944\n",
      "Gradient Descent(3748/9999): loss=0.04399319516358355\n",
      "Gradient Descent(3749/9999): loss=0.043993154650028767\n",
      "Gradient Descent(3750/9999): loss=0.043993114200174786\n",
      "Gradient Descent(3751/9999): loss=0.04399307381392147\n",
      "Gradient Descent(3752/9999): loss=0.0439930334911688\n",
      "Gradient Descent(3753/9999): loss=0.04399299323181691\n",
      "Gradient Descent(3754/9999): loss=0.043992953035766134\n",
      "Gradient Descent(3755/9999): loss=0.04399291290291692\n",
      "Gradient Descent(3756/9999): loss=0.04399287283316993\n",
      "Gradient Descent(3757/9999): loss=0.04399283282642594\n",
      "Gradient Descent(3758/9999): loss=0.04399279288258586\n",
      "Gradient Descent(3759/9999): loss=0.043992753001550824\n",
      "Gradient Descent(3760/9999): loss=0.04399271318322204\n",
      "Gradient Descent(3761/9999): loss=0.04399267342750094\n",
      "Gradient Descent(3762/9999): loss=0.04399263373428908\n",
      "Gradient Descent(3763/9999): loss=0.04399259410348813\n",
      "Gradient Descent(3764/9999): loss=0.04399255453500005\n",
      "Gradient Descent(3765/9999): loss=0.04399251502872679\n",
      "Gradient Descent(3766/9999): loss=0.04399247558457055\n",
      "Gradient Descent(3767/9999): loss=0.04399243620243366\n",
      "Gradient Descent(3768/9999): loss=0.04399239688221862\n",
      "Gradient Descent(3769/9999): loss=0.04399235762382804\n",
      "Gradient Descent(3770/9999): loss=0.04399231842716475\n",
      "Gradient Descent(3771/9999): loss=0.04399227929213164\n",
      "Gradient Descent(3772/9999): loss=0.043992240218631865\n",
      "Gradient Descent(3773/9999): loss=0.04399220120656862\n",
      "Gradient Descent(3774/9999): loss=0.04399216225584535\n",
      "Gradient Descent(3775/9999): loss=0.04399212336636559\n",
      "Gradient Descent(3776/9999): loss=0.04399208453803302\n",
      "Gradient Descent(3777/9999): loss=0.043992045770751546\n",
      "Gradient Descent(3778/9999): loss=0.043992007064425145\n",
      "Gradient Descent(3779/9999): loss=0.04399196841895799\n",
      "Gradient Descent(3780/9999): loss=0.04399192983425438\n",
      "Gradient Descent(3781/9999): loss=0.04399189131021876\n",
      "Gradient Descent(3782/9999): loss=0.043991852846755775\n",
      "Gradient Descent(3783/9999): loss=0.043991814443770136\n",
      "Gradient Descent(3784/9999): loss=0.04399177610116682\n",
      "Gradient Descent(3785/9999): loss=0.04399173781885084\n",
      "Gradient Descent(3786/9999): loss=0.04399169959672739\n",
      "Gradient Descent(3787/9999): loss=0.04399166143470187\n",
      "Gradient Descent(3788/9999): loss=0.04399162333267974\n",
      "Gradient Descent(3789/9999): loss=0.0439915852905667\n",
      "Gradient Descent(3790/9999): loss=0.043991547308268535\n",
      "Gradient Descent(3791/9999): loss=0.0439915093856912\n",
      "Gradient Descent(3792/9999): loss=0.04399147152274077\n",
      "Gradient Descent(3793/9999): loss=0.043991433719323514\n",
      "Gradient Descent(3794/9999): loss=0.04399139597534582\n",
      "Gradient Descent(3795/9999): loss=0.043991358290714225\n",
      "Gradient Descent(3796/9999): loss=0.04399132066533545\n",
      "Gradient Descent(3797/9999): loss=0.04399128309911629\n",
      "Gradient Descent(3798/9999): loss=0.04399124559196371\n",
      "Gradient Descent(3799/9999): loss=0.04399120814378489\n",
      "Gradient Descent(3800/9999): loss=0.04399117075448709\n",
      "Gradient Descent(3801/9999): loss=0.0439911334239777\n",
      "Gradient Descent(3802/9999): loss=0.043991096152164316\n",
      "Gradient Descent(3803/9999): loss=0.04399105893895465\n",
      "Gradient Descent(3804/9999): loss=0.04399102178425654\n",
      "Gradient Descent(3805/9999): loss=0.04399098468797796\n",
      "Gradient Descent(3806/9999): loss=0.04399094765002712\n",
      "Gradient Descent(3807/9999): loss=0.043990910670312265\n",
      "Gradient Descent(3808/9999): loss=0.04399087374874183\n",
      "Gradient Descent(3809/9999): loss=0.04399083688522443\n",
      "Gradient Descent(3810/9999): loss=0.04399080007966872\n",
      "Gradient Descent(3811/9999): loss=0.043990763331983616\n",
      "Gradient Descent(3812/9999): loss=0.04399072664207812\n",
      "Gradient Descent(3813/9999): loss=0.043990690009861375\n",
      "Gradient Descent(3814/9999): loss=0.04399065343524269\n",
      "Gradient Descent(3815/9999): loss=0.04399061691813145\n",
      "Gradient Descent(3816/9999): loss=0.04399058045843731\n",
      "Gradient Descent(3817/9999): loss=0.04399054405606992\n",
      "Gradient Descent(3818/9999): loss=0.04399050771093921\n",
      "Gradient Descent(3819/9999): loss=0.043990471422955126\n",
      "Gradient Descent(3820/9999): loss=0.04399043519202784\n",
      "Gradient Descent(3821/9999): loss=0.043990399018067665\n",
      "Gradient Descent(3822/9999): loss=0.043990362900984986\n",
      "Gradient Descent(3823/9999): loss=0.043990326840690396\n",
      "Gradient Descent(3824/9999): loss=0.043990290837094596\n",
      "Gradient Descent(3825/9999): loss=0.04399025489010843\n",
      "Gradient Descent(3826/9999): loss=0.04399021899964291\n",
      "Gradient Descent(3827/9999): loss=0.043990183165609154\n",
      "Gradient Descent(3828/9999): loss=0.04399014738791845\n",
      "Gradient Descent(3829/9999): loss=0.04399011166648217\n",
      "Gradient Descent(3830/9999): loss=0.04399007600121188\n",
      "Gradient Descent(3831/9999): loss=0.04399004039201932\n",
      "Gradient Descent(3832/9999): loss=0.04399000483881624\n",
      "Gradient Descent(3833/9999): loss=0.043989969341514626\n",
      "Gradient Descent(3834/9999): loss=0.04398993390002664\n",
      "Gradient Descent(3835/9999): loss=0.043989898514264415\n",
      "Gradient Descent(3836/9999): loss=0.04398986318414045\n",
      "Gradient Descent(3837/9999): loss=0.043989827909567174\n",
      "Gradient Descent(3838/9999): loss=0.043989792690457274\n",
      "Gradient Descent(3839/9999): loss=0.043989757526723565\n",
      "Gradient Descent(3840/9999): loss=0.04398972241827896\n",
      "Gradient Descent(3841/9999): loss=0.04398968736503648\n",
      "Gradient Descent(3842/9999): loss=0.04398965236690943\n",
      "Gradient Descent(3843/9999): loss=0.04398961742381106\n",
      "Gradient Descent(3844/9999): loss=0.04398958253565489\n",
      "Gradient Descent(3845/9999): loss=0.04398954770235452\n",
      "Gradient Descent(3846/9999): loss=0.0439895129238237\n",
      "Gradient Descent(3847/9999): loss=0.04398947819997634\n",
      "Gradient Descent(3848/9999): loss=0.04398944353072639\n",
      "Gradient Descent(3849/9999): loss=0.04398940891598807\n",
      "Gradient Descent(3850/9999): loss=0.043989374355675626\n",
      "Gradient Descent(3851/9999): loss=0.043989339849703546\n",
      "Gradient Descent(3852/9999): loss=0.043989305397986324\n",
      "Gradient Descent(3853/9999): loss=0.04398927100043869\n",
      "Gradient Descent(3854/9999): loss=0.04398923665697545\n",
      "Gradient Descent(3855/9999): loss=0.043989202367511554\n",
      "Gradient Descent(3856/9999): loss=0.04398916813196213\n",
      "Gradient Descent(3857/9999): loss=0.043989133950242405\n",
      "Gradient Descent(3858/9999): loss=0.043989099822267716\n",
      "Gradient Descent(3859/9999): loss=0.043989065747953575\n",
      "Gradient Descent(3860/9999): loss=0.04398903172721561\n",
      "Gradient Descent(3861/9999): loss=0.043988997759969554\n",
      "Gradient Descent(3862/9999): loss=0.04398896384613136\n",
      "Gradient Descent(3863/9999): loss=0.04398892998561697\n",
      "Gradient Descent(3864/9999): loss=0.043988896178342625\n",
      "Gradient Descent(3865/9999): loss=0.04398886242422456\n",
      "Gradient Descent(3866/9999): loss=0.043988828723179225\n",
      "Gradient Descent(3867/9999): loss=0.043988795075123155\n",
      "Gradient Descent(3868/9999): loss=0.04398876147997302\n",
      "Gradient Descent(3869/9999): loss=0.04398872793764567\n",
      "Gradient Descent(3870/9999): loss=0.043988694448058\n",
      "Gradient Descent(3871/9999): loss=0.04398866101112715\n",
      "Gradient Descent(3872/9999): loss=0.043988627626770284\n",
      "Gradient Descent(3873/9999): loss=0.04398859429490475\n",
      "Gradient Descent(3874/9999): loss=0.04398856101544799\n",
      "Gradient Descent(3875/9999): loss=0.04398852778831763\n",
      "Gradient Descent(3876/9999): loss=0.0439884946134314\n",
      "Gradient Descent(3877/9999): loss=0.04398846149070713\n",
      "Gradient Descent(3878/9999): loss=0.04398842842006283\n",
      "Gradient Descent(3879/9999): loss=0.043988395401416594\n",
      "Gradient Descent(3880/9999): loss=0.04398836243468665\n",
      "Gradient Descent(3881/9999): loss=0.0439883295197914\n",
      "Gradient Descent(3882/9999): loss=0.04398829665664932\n",
      "Gradient Descent(3883/9999): loss=0.043988263845179044\n",
      "Gradient Descent(3884/9999): loss=0.04398823108529934\n",
      "Gradient Descent(3885/9999): loss=0.043988198376929064\n",
      "Gradient Descent(3886/9999): loss=0.04398816571998726\n",
      "Gradient Descent(3887/9999): loss=0.043988133114393035\n",
      "Gradient Descent(3888/9999): loss=0.043988100560065656\n",
      "Gradient Descent(3889/9999): loss=0.04398806805692454\n",
      "Gradient Descent(3890/9999): loss=0.04398803560488917\n",
      "Gradient Descent(3891/9999): loss=0.04398800320387923\n",
      "Gradient Descent(3892/9999): loss=0.04398797085381445\n",
      "Gradient Descent(3893/9999): loss=0.043987938554614754\n",
      "Gradient Descent(3894/9999): loss=0.043987906306200156\n",
      "Gradient Descent(3895/9999): loss=0.043987874108490824\n",
      "Gradient Descent(3896/9999): loss=0.043987841961406995\n",
      "Gradient Descent(3897/9999): loss=0.0439878098648691\n",
      "Gradient Descent(3898/9999): loss=0.04398777781879765\n",
      "Gradient Descent(3899/9999): loss=0.0439877458231133\n",
      "Gradient Descent(3900/9999): loss=0.043987713877736825\n",
      "Gradient Descent(3901/9999): loss=0.043987681982589094\n",
      "Gradient Descent(3902/9999): loss=0.0439876501375912\n",
      "Gradient Descent(3903/9999): loss=0.043987618342664236\n",
      "Gradient Descent(3904/9999): loss=0.04398758659772947\n",
      "Gradient Descent(3905/9999): loss=0.04398755490270833\n",
      "Gradient Descent(3906/9999): loss=0.043987523257522324\n",
      "Gradient Descent(3907/9999): loss=0.04398749166209308\n",
      "Gradient Descent(3908/9999): loss=0.043987460116342386\n",
      "Gradient Descent(3909/9999): loss=0.04398742862019211\n",
      "Gradient Descent(3910/9999): loss=0.04398739717356429\n",
      "Gradient Descent(3911/9999): loss=0.043987365776381016\n",
      "Gradient Descent(3912/9999): loss=0.043987334428564596\n",
      "Gradient Descent(3913/9999): loss=0.043987303130037375\n",
      "Gradient Descent(3914/9999): loss=0.04398727188072185\n",
      "Gradient Descent(3915/9999): loss=0.043987240680540676\n",
      "Gradient Descent(3916/9999): loss=0.04398720952941658\n",
      "Gradient Descent(3917/9999): loss=0.043987178427272414\n",
      "Gradient Descent(3918/9999): loss=0.04398714737403122\n",
      "Gradient Descent(3919/9999): loss=0.043987116369616025\n",
      "Gradient Descent(3920/9999): loss=0.04398708541395008\n",
      "Gradient Descent(3921/9999): loss=0.043987054506956766\n",
      "Gradient Descent(3922/9999): loss=0.043987023648559574\n",
      "Gradient Descent(3923/9999): loss=0.043986992838682036\n",
      "Gradient Descent(3924/9999): loss=0.04398696207724788\n",
      "Gradient Descent(3925/9999): loss=0.04398693136418096\n",
      "Gradient Descent(3926/9999): loss=0.04398690069940518\n",
      "Gradient Descent(3927/9999): loss=0.04398687008284469\n",
      "Gradient Descent(3928/9999): loss=0.04398683951442359\n",
      "Gradient Descent(3929/9999): loss=0.043986808994066255\n",
      "Gradient Descent(3930/9999): loss=0.043986778521697086\n",
      "Gradient Descent(3931/9999): loss=0.04398674809724061\n",
      "Gradient Descent(3932/9999): loss=0.04398671772062152\n",
      "Gradient Descent(3933/9999): loss=0.04398668739176461\n",
      "Gradient Descent(3934/9999): loss=0.04398665711059475\n",
      "Gradient Descent(3935/9999): loss=0.043986626877036986\n",
      "Gradient Descent(3936/9999): loss=0.043986596691016434\n",
      "Gradient Descent(3937/9999): loss=0.04398656655245836\n",
      "Gradient Descent(3938/9999): loss=0.04398653646128814\n",
      "Gradient Descent(3939/9999): loss=0.04398650641743128\n",
      "Gradient Descent(3940/9999): loss=0.043986476420813346\n",
      "Gradient Descent(3941/9999): loss=0.043986446471360095\n",
      "Gradient Descent(3942/9999): loss=0.04398641656899736\n",
      "Gradient Descent(3943/9999): loss=0.04398638671365111\n",
      "Gradient Descent(3944/9999): loss=0.0439863569052474\n",
      "Gradient Descent(3945/9999): loss=0.04398632714371243\n",
      "Gradient Descent(3946/9999): loss=0.043986297428972504\n",
      "Gradient Descent(3947/9999): loss=0.043986267760954025\n",
      "Gradient Descent(3948/9999): loss=0.043986238139583585\n",
      "Gradient Descent(3949/9999): loss=0.0439862085647878\n",
      "Gradient Descent(3950/9999): loss=0.043986179036493454\n",
      "Gradient Descent(3951/9999): loss=0.0439861495546274\n",
      "Gradient Descent(3952/9999): loss=0.04398612011911665\n",
      "Gradient Descent(3953/9999): loss=0.04398609072988837\n",
      "Gradient Descent(3954/9999): loss=0.0439860613868697\n",
      "Gradient Descent(3955/9999): loss=0.043986032089988074\n",
      "Gradient Descent(3956/9999): loss=0.043986002839170875\n",
      "Gradient Descent(3957/9999): loss=0.04398597363434569\n",
      "Gradient Descent(3958/9999): loss=0.04398594447544026\n",
      "Gradient Descent(3959/9999): loss=0.043985915362382305\n",
      "Gradient Descent(3960/9999): loss=0.043985886295099794\n",
      "Gradient Descent(3961/9999): loss=0.04398585727352072\n",
      "Gradient Descent(3962/9999): loss=0.04398582829757324\n",
      "Gradient Descent(3963/9999): loss=0.04398579936718559\n",
      "Gradient Descent(3964/9999): loss=0.04398577048228616\n",
      "Gradient Descent(3965/9999): loss=0.04398574164280342\n",
      "Gradient Descent(3966/9999): loss=0.043985712848665946\n",
      "Gradient Descent(3967/9999): loss=0.043985684099802455\n",
      "Gradient Descent(3968/9999): loss=0.043985655396141736\n",
      "Gradient Descent(3969/9999): loss=0.04398562673761273\n",
      "Gradient Descent(3970/9999): loss=0.04398559812414447\n",
      "Gradient Descent(3971/9999): loss=0.04398556955566615\n",
      "Gradient Descent(3972/9999): loss=0.043985541032106966\n",
      "Gradient Descent(3973/9999): loss=0.04398551255339633\n",
      "Gradient Descent(3974/9999): loss=0.043985484119463694\n",
      "Gradient Descent(3975/9999): loss=0.0439854557302387\n",
      "Gradient Descent(3976/9999): loss=0.043985427385651\n",
      "Gradient Descent(3977/9999): loss=0.04398539908563044\n",
      "Gradient Descent(3978/9999): loss=0.043985370830106924\n",
      "Gradient Descent(3979/9999): loss=0.04398534261901053\n",
      "Gradient Descent(3980/9999): loss=0.04398531445227136\n",
      "Gradient Descent(3981/9999): loss=0.04398528632981969\n",
      "Gradient Descent(3982/9999): loss=0.043985258251585876\n",
      "Gradient Descent(3983/9999): loss=0.04398523021750041\n",
      "Gradient Descent(3984/9999): loss=0.043985202227493844\n",
      "Gradient Descent(3985/9999): loss=0.04398517428149691\n",
      "Gradient Descent(3986/9999): loss=0.043985146379440385\n",
      "Gradient Descent(3987/9999): loss=0.04398511852125519\n",
      "Gradient Descent(3988/9999): loss=0.043985090706872336\n",
      "Gradient Descent(3989/9999): loss=0.043985062936222945\n",
      "Gradient Descent(3990/9999): loss=0.0439850352092383\n",
      "Gradient Descent(3991/9999): loss=0.04398500752584968\n",
      "Gradient Descent(3992/9999): loss=0.04398497988598857\n",
      "Gradient Descent(3993/9999): loss=0.043984952289586536\n",
      "Gradient Descent(3994/9999): loss=0.04398492473657521\n",
      "Gradient Descent(3995/9999): loss=0.043984897226886456\n",
      "Gradient Descent(3996/9999): loss=0.04398486976045204\n",
      "Gradient Descent(3997/9999): loss=0.043984842337204046\n",
      "Gradient Descent(3998/9999): loss=0.04398481495707449\n",
      "Gradient Descent(3999/9999): loss=0.043984787619995636\n",
      "Gradient Descent(4000/9999): loss=0.04398476032589979\n",
      "Gradient Descent(4001/9999): loss=0.04398473307471934\n",
      "Gradient Descent(4002/9999): loss=0.04398470586638683\n",
      "Gradient Descent(4003/9999): loss=0.04398467870083489\n",
      "Gradient Descent(4004/9999): loss=0.04398465157799622\n",
      "Gradient Descent(4005/9999): loss=0.04398462449780371\n",
      "Gradient Descent(4006/9999): loss=0.04398459746019026\n",
      "Gradient Descent(4007/9999): loss=0.04398457046508896\n",
      "Gradient Descent(4008/9999): loss=0.043984543512432965\n",
      "Gradient Descent(4009/9999): loss=0.04398451660215549\n",
      "Gradient Descent(4010/9999): loss=0.04398448973418993\n",
      "Gradient Descent(4011/9999): loss=0.04398446290846978\n",
      "Gradient Descent(4012/9999): loss=0.04398443612492863\n",
      "Gradient Descent(4013/9999): loss=0.043984409383500074\n",
      "Gradient Descent(4014/9999): loss=0.04398438268411798\n",
      "Gradient Descent(4015/9999): loss=0.0439843560267162\n",
      "Gradient Descent(4016/9999): loss=0.043984329411228734\n",
      "Gradient Descent(4017/9999): loss=0.04398430283758969\n",
      "Gradient Descent(4018/9999): loss=0.04398427630573326\n",
      "Gradient Descent(4019/9999): loss=0.04398424981559373\n",
      "Gradient Descent(4020/9999): loss=0.04398422336710552\n",
      "Gradient Descent(4021/9999): loss=0.04398419696020312\n",
      "Gradient Descent(4022/9999): loss=0.043984170594821216\n",
      "Gradient Descent(4023/9999): loss=0.043984144270894455\n",
      "Gradient Descent(4024/9999): loss=0.043984117988357656\n",
      "Gradient Descent(4025/9999): loss=0.0439840917471458\n",
      "Gradient Descent(4026/9999): loss=0.043984065547193835\n",
      "Gradient Descent(4027/9999): loss=0.04398403938843694\n",
      "Gradient Descent(4028/9999): loss=0.04398401327081032\n",
      "Gradient Descent(4029/9999): loss=0.0439839871942493\n",
      "Gradient Descent(4030/9999): loss=0.04398396115868932\n",
      "Gradient Descent(4031/9999): loss=0.04398393516406593\n",
      "Gradient Descent(4032/9999): loss=0.043983909210314726\n",
      "Gradient Descent(4033/9999): loss=0.04398388329737147\n",
      "Gradient Descent(4034/9999): loss=0.04398385742517198\n",
      "Gradient Descent(4035/9999): loss=0.04398383159365223\n",
      "Gradient Descent(4036/9999): loss=0.04398380580274822\n",
      "Gradient Descent(4037/9999): loss=0.043983780052396115\n",
      "Gradient Descent(4038/9999): loss=0.0439837543425321\n",
      "Gradient Descent(4039/9999): loss=0.043983728673092595\n",
      "Gradient Descent(4040/9999): loss=0.04398370304401399\n",
      "Gradient Descent(4041/9999): loss=0.043983677455232816\n",
      "Gradient Descent(4042/9999): loss=0.043983651906685746\n",
      "Gradient Descent(4043/9999): loss=0.043983626398309494\n",
      "Gradient Descent(4044/9999): loss=0.04398360093004091\n",
      "Gradient Descent(4045/9999): loss=0.04398357550181693\n",
      "Gradient Descent(4046/9999): loss=0.043983550113574604\n",
      "Gradient Descent(4047/9999): loss=0.04398352476525102\n",
      "Gradient Descent(4048/9999): loss=0.043983499456783454\n",
      "Gradient Descent(4049/9999): loss=0.04398347418810923\n",
      "Gradient Descent(4050/9999): loss=0.04398344895916578\n",
      "Gradient Descent(4051/9999): loss=0.043983423769890624\n",
      "Gradient Descent(4052/9999): loss=0.04398339862022141\n",
      "Gradient Descent(4053/9999): loss=0.043983373510095854\n",
      "Gradient Descent(4054/9999): loss=0.04398334843945176\n",
      "Gradient Descent(4055/9999): loss=0.043983323408227096\n",
      "Gradient Descent(4056/9999): loss=0.04398329841635983\n",
      "Gradient Descent(4057/9999): loss=0.04398327346378813\n",
      "Gradient Descent(4058/9999): loss=0.04398324855045016\n",
      "Gradient Descent(4059/9999): loss=0.04398322367628427\n",
      "Gradient Descent(4060/9999): loss=0.04398319884122885\n",
      "Gradient Descent(4061/9999): loss=0.04398317404522242\n",
      "Gradient Descent(4062/9999): loss=0.043983149288203544\n",
      "Gradient Descent(4063/9999): loss=0.04398312457011096\n",
      "Gradient Descent(4064/9999): loss=0.043983099890883476\n",
      "Gradient Descent(4065/9999): loss=0.043983075250459906\n",
      "Gradient Descent(4066/9999): loss=0.04398305064877931\n",
      "Gradient Descent(4067/9999): loss=0.04398302608578078\n",
      "Gradient Descent(4068/9999): loss=0.04398300156140345\n",
      "Gradient Descent(4069/9999): loss=0.04398297707558659\n",
      "Gradient Descent(4070/9999): loss=0.04398295262826961\n",
      "Gradient Descent(4071/9999): loss=0.04398292821939194\n",
      "Gradient Descent(4072/9999): loss=0.04398290384889317\n",
      "Gradient Descent(4073/9999): loss=0.04398287951671294\n",
      "Gradient Descent(4074/9999): loss=0.043982855222790974\n",
      "Gradient Descent(4075/9999): loss=0.043982830967067184\n",
      "Gradient Descent(4076/9999): loss=0.04398280674948146\n",
      "Gradient Descent(4077/9999): loss=0.043982782569973844\n",
      "Gradient Descent(4078/9999): loss=0.04398275842848448\n",
      "Gradient Descent(4079/9999): loss=0.043982734324953546\n",
      "Gradient Descent(4080/9999): loss=0.04398271025932142\n",
      "Gradient Descent(4081/9999): loss=0.043982686231528484\n",
      "Gradient Descent(4082/9999): loss=0.043982662241515216\n",
      "Gradient Descent(4083/9999): loss=0.04398263828922225\n",
      "Gradient Descent(4084/9999): loss=0.04398261437459028\n",
      "Gradient Descent(4085/9999): loss=0.04398259049756006\n",
      "Gradient Descent(4086/9999): loss=0.043982566658072514\n",
      "Gradient Descent(4087/9999): loss=0.04398254285606855\n",
      "Gradient Descent(4088/9999): loss=0.043982519091489286\n",
      "Gradient Descent(4089/9999): loss=0.04398249536427585\n",
      "Gradient Descent(4090/9999): loss=0.04398247167436947\n",
      "Gradient Descent(4091/9999): loss=0.04398244802171154\n",
      "Gradient Descent(4092/9999): loss=0.04398242440624347\n",
      "Gradient Descent(4093/9999): loss=0.04398240082790677\n",
      "Gradient Descent(4094/9999): loss=0.04398237728664309\n",
      "Gradient Descent(4095/9999): loss=0.04398235378239408\n",
      "Gradient Descent(4096/9999): loss=0.0439823303151016\n",
      "Gradient Descent(4097/9999): loss=0.043982306884707514\n",
      "Gradient Descent(4098/9999): loss=0.04398228349115383\n",
      "Gradient Descent(4099/9999): loss=0.0439822601343826\n",
      "Gradient Descent(4100/9999): loss=0.04398223681433599\n",
      "Gradient Descent(4101/9999): loss=0.04398221353095625\n",
      "Gradient Descent(4102/9999): loss=0.04398219028418574\n",
      "Gradient Descent(4103/9999): loss=0.043982167073966905\n",
      "Gradient Descent(4104/9999): loss=0.04398214390024226\n",
      "Gradient Descent(4105/9999): loss=0.043982120762954435\n",
      "Gradient Descent(4106/9999): loss=0.04398209766204613\n",
      "Gradient Descent(4107/9999): loss=0.04398207459746015\n",
      "Gradient Descent(4108/9999): loss=0.04398205156913937\n",
      "Gradient Descent(4109/9999): loss=0.043982028577026794\n",
      "Gradient Descent(4110/9999): loss=0.043982005621065474\n",
      "Gradient Descent(4111/9999): loss=0.04398198270119855\n",
      "Gradient Descent(4112/9999): loss=0.04398195981736932\n",
      "Gradient Descent(4113/9999): loss=0.043981936969521074\n",
      "Gradient Descent(4114/9999): loss=0.04398191415759725\n",
      "Gradient Descent(4115/9999): loss=0.04398189138154141\n",
      "Gradient Descent(4116/9999): loss=0.04398186864129707\n",
      "Gradient Descent(4117/9999): loss=0.043981845936807984\n",
      "Gradient Descent(4118/9999): loss=0.04398182326801791\n",
      "Gradient Descent(4119/9999): loss=0.04398180063487074\n",
      "Gradient Descent(4120/9999): loss=0.04398177803731041\n",
      "Gradient Descent(4121/9999): loss=0.043981755475280986\n",
      "Gradient Descent(4122/9999): loss=0.04398173294872655\n",
      "Gradient Descent(4123/9999): loss=0.0439817104575914\n",
      "Gradient Descent(4124/9999): loss=0.04398168800181978\n",
      "Gradient Descent(4125/9999): loss=0.04398166558135613\n",
      "Gradient Descent(4126/9999): loss=0.04398164319614489\n",
      "Gradient Descent(4127/9999): loss=0.04398162084613069\n",
      "Gradient Descent(4128/9999): loss=0.04398159853125813\n",
      "Gradient Descent(4129/9999): loss=0.04398157625147202\n",
      "Gradient Descent(4130/9999): loss=0.0439815540067171\n",
      "Gradient Descent(4131/9999): loss=0.04398153179693838\n",
      "Gradient Descent(4132/9999): loss=0.04398150962208084\n",
      "Gradient Descent(4133/9999): loss=0.043981487482089535\n",
      "Gradient Descent(4134/9999): loss=0.04398146537690968\n",
      "Gradient Descent(4135/9999): loss=0.043981443306486546\n",
      "Gradient Descent(4136/9999): loss=0.04398142127076545\n",
      "Gradient Descent(4137/9999): loss=0.04398139926969186\n",
      "Gradient Descent(4138/9999): loss=0.04398137730321125\n",
      "Gradient Descent(4139/9999): loss=0.04398135537126932\n",
      "Gradient Descent(4140/9999): loss=0.043981333473811654\n",
      "Gradient Descent(4141/9999): loss=0.04398131161078412\n",
      "Gradient Descent(4142/9999): loss=0.04398128978213253\n",
      "Gradient Descent(4143/9999): loss=0.04398126798780286\n",
      "Gradient Descent(4144/9999): loss=0.04398124622774114\n",
      "Gradient Descent(4145/9999): loss=0.043981224501893484\n",
      "Gradient Descent(4146/9999): loss=0.04398120281020608\n",
      "Gradient Descent(4147/9999): loss=0.043981181152625254\n",
      "Gradient Descent(4148/9999): loss=0.04398115952909735\n",
      "Gradient Descent(4149/9999): loss=0.04398113793956884\n",
      "Gradient Descent(4150/9999): loss=0.04398111638398627\n",
      "Gradient Descent(4151/9999): loss=0.043981094862296226\n",
      "Gradient Descent(4152/9999): loss=0.04398107337444546\n",
      "Gradient Descent(4153/9999): loss=0.043981051920380765\n",
      "Gradient Descent(4154/9999): loss=0.04398103050004901\n",
      "Gradient Descent(4155/9999): loss=0.043981009113397126\n",
      "Gradient Descent(4156/9999): loss=0.04398098776037219\n",
      "Gradient Descent(4157/9999): loss=0.043980966440921324\n",
      "Gradient Descent(4158/9999): loss=0.04398094515499174\n",
      "Gradient Descent(4159/9999): loss=0.043980923902530715\n",
      "Gradient Descent(4160/9999): loss=0.04398090268348565\n",
      "Gradient Descent(4161/9999): loss=0.043980881497804\n",
      "Gradient Descent(4162/9999): loss=0.043980860345433255\n",
      "Gradient Descent(4163/9999): loss=0.04398083922632114\n",
      "Gradient Descent(4164/9999): loss=0.04398081814041527\n",
      "Gradient Descent(4165/9999): loss=0.043980797087663455\n",
      "Gradient Descent(4166/9999): loss=0.0439807760680136\n",
      "Gradient Descent(4167/9999): loss=0.04398075508141364\n",
      "Gradient Descent(4168/9999): loss=0.0439807341278116\n",
      "Gradient Descent(4169/9999): loss=0.0439807132071556\n",
      "Gradient Descent(4170/9999): loss=0.04398069231939384\n",
      "Gradient Descent(4171/9999): loss=0.043980671464474604\n",
      "Gradient Descent(4172/9999): loss=0.04398065064234625\n",
      "Gradient Descent(4173/9999): loss=0.043980629852957184\n",
      "Gradient Descent(4174/9999): loss=0.043980609096255986\n",
      "Gradient Descent(4175/9999): loss=0.04398058837219125\n",
      "Gradient Descent(4176/9999): loss=0.043980567680711624\n",
      "Gradient Descent(4177/9999): loss=0.04398054702176592\n",
      "Gradient Descent(4178/9999): loss=0.043980526395302935\n",
      "Gradient Descent(4179/9999): loss=0.04398050580127164\n",
      "Gradient Descent(4180/9999): loss=0.043980485239620996\n",
      "Gradient Descent(4181/9999): loss=0.04398046471030015\n",
      "Gradient Descent(4182/9999): loss=0.04398044421325821\n",
      "Gradient Descent(4183/9999): loss=0.04398042374844446\n",
      "Gradient Descent(4184/9999): loss=0.04398040331580819\n",
      "Gradient Descent(4185/9999): loss=0.04398038291529885\n",
      "Gradient Descent(4186/9999): loss=0.0439803625468659\n",
      "Gradient Descent(4187/9999): loss=0.04398034221045893\n",
      "Gradient Descent(4188/9999): loss=0.04398032190602753\n",
      "Gradient Descent(4189/9999): loss=0.043980301633521485\n",
      "Gradient Descent(4190/9999): loss=0.04398028139289057\n",
      "Gradient Descent(4191/9999): loss=0.04398026118408467\n",
      "Gradient Descent(4192/9999): loss=0.04398024100705372\n",
      "Gradient Descent(4193/9999): loss=0.04398022086174781\n",
      "Gradient Descent(4194/9999): loss=0.04398020074811702\n",
      "Gradient Descent(4195/9999): loss=0.04398018066611156\n",
      "Gradient Descent(4196/9999): loss=0.043980160615681704\n",
      "Gradient Descent(4197/9999): loss=0.04398014059677779\n",
      "Gradient Descent(4198/9999): loss=0.043980120609350276\n",
      "Gradient Descent(4199/9999): loss=0.04398010065334963\n",
      "Gradient Descent(4200/9999): loss=0.043980080728726494\n",
      "Gradient Descent(4201/9999): loss=0.043980060835431496\n",
      "Gradient Descent(4202/9999): loss=0.04398004097341536\n",
      "Gradient Descent(4203/9999): loss=0.04398002114262897\n",
      "Gradient Descent(4204/9999): loss=0.043980001343023156\n",
      "Gradient Descent(4205/9999): loss=0.0439799815745489\n",
      "Gradient Descent(4206/9999): loss=0.0439799618371573\n",
      "Gradient Descent(4207/9999): loss=0.043979942130799456\n",
      "Gradient Descent(4208/9999): loss=0.04397992245542655\n",
      "Gradient Descent(4209/9999): loss=0.04397990281098989\n",
      "Gradient Descent(4210/9999): loss=0.04397988319744081\n",
      "Gradient Descent(4211/9999): loss=0.04397986361473081\n",
      "Gradient Descent(4212/9999): loss=0.043979844062811316\n",
      "Gradient Descent(4213/9999): loss=0.043979824541633974\n",
      "Gradient Descent(4214/9999): loss=0.0439798050511504\n",
      "Gradient Descent(4215/9999): loss=0.043979785591312354\n",
      "Gradient Descent(4216/9999): loss=0.043979766162071686\n",
      "Gradient Descent(4217/9999): loss=0.04397974676338026\n",
      "Gradient Descent(4218/9999): loss=0.04397972739519\n",
      "Gradient Descent(4219/9999): loss=0.04397970805745301\n",
      "Gradient Descent(4220/9999): loss=0.04397968875012137\n",
      "Gradient Descent(4221/9999): loss=0.04397966947314731\n",
      "Gradient Descent(4222/9999): loss=0.043979650226483036\n",
      "Gradient Descent(4223/9999): loss=0.04397963101008097\n",
      "Gradient Descent(4224/9999): loss=0.04397961182389345\n",
      "Gradient Descent(4225/9999): loss=0.04397959266787303\n",
      "Gradient Descent(4226/9999): loss=0.04397957354197226\n",
      "Gradient Descent(4227/9999): loss=0.04397955444614376\n",
      "Gradient Descent(4228/9999): loss=0.043979535380340294\n",
      "Gradient Descent(4229/9999): loss=0.043979516344514566\n",
      "Gradient Descent(4230/9999): loss=0.04397949733861957\n",
      "Gradient Descent(4231/9999): loss=0.04397947836260812\n",
      "Gradient Descent(4232/9999): loss=0.043979459416433275\n",
      "Gradient Descent(4233/9999): loss=0.04397944050004815\n",
      "Gradient Descent(4234/9999): loss=0.04397942161340587\n",
      "Gradient Descent(4235/9999): loss=0.04397940275645969\n",
      "Gradient Descent(4236/9999): loss=0.043979383929162924\n",
      "Gradient Descent(4237/9999): loss=0.043979365131468925\n",
      "Gradient Descent(4238/9999): loss=0.04397934636333116\n",
      "Gradient Descent(4239/9999): loss=0.04397932762470317\n",
      "Gradient Descent(4240/9999): loss=0.043979308915538515\n",
      "Gradient Descent(4241/9999): loss=0.043979290235790926\n",
      "Gradient Descent(4242/9999): loss=0.04397927158541411\n",
      "Gradient Descent(4243/9999): loss=0.04397925296436191\n",
      "Gradient Descent(4244/9999): loss=0.043979234372588195\n",
      "Gradient Descent(4245/9999): loss=0.043979215810046955\n",
      "Gradient Descent(4246/9999): loss=0.0439791972766922\n",
      "Gradient Descent(4247/9999): loss=0.043979178772478054\n",
      "Gradient Descent(4248/9999): loss=0.04397916029735871\n",
      "Gradient Descent(4249/9999): loss=0.04397914185128839\n",
      "Gradient Descent(4250/9999): loss=0.043979123434221445\n",
      "Gradient Descent(4251/9999): loss=0.043979105046112264\n",
      "Gradient Descent(4252/9999): loss=0.04397908668691531\n",
      "Gradient Descent(4253/9999): loss=0.04397906835658513\n",
      "Gradient Descent(4254/9999): loss=0.043979050055076324\n",
      "Gradient Descent(4255/9999): loss=0.043979031782343615\n",
      "Gradient Descent(4256/9999): loss=0.04397901353834171\n",
      "Gradient Descent(4257/9999): loss=0.04397899532302546\n",
      "Gradient Descent(4258/9999): loss=0.04397897713634974\n",
      "Gradient Descent(4259/9999): loss=0.04397895897826953\n",
      "Gradient Descent(4260/9999): loss=0.04397894084873988\n",
      "Gradient Descent(4261/9999): loss=0.04397892274771589\n",
      "Gradient Descent(4262/9999): loss=0.04397890467515273\n",
      "Gradient Descent(4263/9999): loss=0.043978886631005666\n",
      "Gradient Descent(4264/9999): loss=0.04397886861523001\n",
      "Gradient Descent(4265/9999): loss=0.04397885062778115\n",
      "Gradient Descent(4266/9999): loss=0.043978832668614576\n",
      "Gradient Descent(4267/9999): loss=0.043978814737685765\n",
      "Gradient Descent(4268/9999): loss=0.04397879683495035\n",
      "Gradient Descent(4269/9999): loss=0.043978778960364\n",
      "Gradient Descent(4270/9999): loss=0.04397876111388246\n",
      "Gradient Descent(4271/9999): loss=0.04397874329546154\n",
      "Gradient Descent(4272/9999): loss=0.0439787255050571\n",
      "Gradient Descent(4273/9999): loss=0.04397870774262512\n",
      "Gradient Descent(4274/9999): loss=0.043978690008121574\n",
      "Gradient Descent(4275/9999): loss=0.043978672301502576\n",
      "Gradient Descent(4276/9999): loss=0.04397865462272429\n",
      "Gradient Descent(4277/9999): loss=0.043978636971742924\n",
      "Gradient Descent(4278/9999): loss=0.0439786193485148\n",
      "Gradient Descent(4279/9999): loss=0.043978601752996234\n",
      "Gradient Descent(4280/9999): loss=0.0439785841851437\n",
      "Gradient Descent(4281/9999): loss=0.043978566644913696\n",
      "Gradient Descent(4282/9999): loss=0.043978549132262755\n",
      "Gradient Descent(4283/9999): loss=0.043978531647147545\n",
      "Gradient Descent(4284/9999): loss=0.04397851418952474\n",
      "Gradient Descent(4285/9999): loss=0.04397849675935115\n",
      "Gradient Descent(4286/9999): loss=0.04397847935658359\n",
      "Gradient Descent(4287/9999): loss=0.04397846198117898\n",
      "Gradient Descent(4288/9999): loss=0.04397844463309431\n",
      "Gradient Descent(4289/9999): loss=0.04397842731228658\n",
      "Gradient Descent(4290/9999): loss=0.04397841001871295\n",
      "Gradient Descent(4291/9999): loss=0.04397839275233057\n",
      "Gradient Descent(4292/9999): loss=0.04397837551309669\n",
      "Gradient Descent(4293/9999): loss=0.04397835830096865\n",
      "Gradient Descent(4294/9999): loss=0.04397834111590378\n",
      "Gradient Descent(4295/9999): loss=0.04397832395785957\n",
      "Gradient Descent(4296/9999): loss=0.04397830682679353\n",
      "Gradient Descent(4297/9999): loss=0.04397828972266321\n",
      "Gradient Descent(4298/9999): loss=0.043978272645426274\n",
      "Gradient Descent(4299/9999): loss=0.04397825559504047\n",
      "Gradient Descent(4300/9999): loss=0.04397823857146354\n",
      "Gradient Descent(4301/9999): loss=0.04397822157465334\n",
      "Gradient Descent(4302/9999): loss=0.04397820460456778\n",
      "Gradient Descent(4303/9999): loss=0.04397818766116487\n",
      "Gradient Descent(4304/9999): loss=0.0439781707444026\n",
      "Gradient Descent(4305/9999): loss=0.043978153854239165\n",
      "Gradient Descent(4306/9999): loss=0.043978136990632626\n",
      "Gradient Descent(4307/9999): loss=0.04397812015354134\n",
      "Gradient Descent(4308/9999): loss=0.04397810334292356\n",
      "Gradient Descent(4309/9999): loss=0.04397808655873768\n",
      "Gradient Descent(4310/9999): loss=0.04397806980094211\n",
      "Gradient Descent(4311/9999): loss=0.04397805306949538\n",
      "Gradient Descent(4312/9999): loss=0.043978036364356064\n",
      "Gradient Descent(4313/9999): loss=0.04397801968548279\n",
      "Gradient Descent(4314/9999): loss=0.04397800303283427\n",
      "Gradient Descent(4315/9999): loss=0.04397798640636925\n",
      "Gradient Descent(4316/9999): loss=0.043977969806046575\n",
      "Gradient Descent(4317/9999): loss=0.043977953231825115\n",
      "Gradient Descent(4318/9999): loss=0.04397793668366388\n",
      "Gradient Descent(4319/9999): loss=0.04397792016152188\n",
      "Gradient Descent(4320/9999): loss=0.043977903665358135\n",
      "Gradient Descent(4321/9999): loss=0.0439778871951319\n",
      "Gradient Descent(4322/9999): loss=0.04397787075080232\n",
      "Gradient Descent(4323/9999): loss=0.04397785433232873\n",
      "Gradient Descent(4324/9999): loss=0.043977837939670444\n",
      "Gradient Descent(4325/9999): loss=0.04397782157278687\n",
      "Gradient Descent(4326/9999): loss=0.04397780523163747\n",
      "Gradient Descent(4327/9999): loss=0.04397778891618181\n",
      "Gradient Descent(4328/9999): loss=0.04397777262637948\n",
      "Gradient Descent(4329/9999): loss=0.043977756362190173\n",
      "Gradient Descent(4330/9999): loss=0.04397774012357356\n",
      "Gradient Descent(4331/9999): loss=0.04397772391048948\n",
      "Gradient Descent(4332/9999): loss=0.04397770772289775\n",
      "Gradient Descent(4333/9999): loss=0.043977691560758314\n",
      "Gradient Descent(4334/9999): loss=0.04397767542403114\n",
      "Gradient Descent(4335/9999): loss=0.04397765931267628\n",
      "Gradient Descent(4336/9999): loss=0.04397764322665384\n",
      "Gradient Descent(4337/9999): loss=0.043977627165923974\n",
      "Gradient Descent(4338/9999): loss=0.04397761113044693\n",
      "Gradient Descent(4339/9999): loss=0.04397759512018301\n",
      "Gradient Descent(4340/9999): loss=0.043977579135092544\n",
      "Gradient Descent(4341/9999): loss=0.04397756317513597\n",
      "Gradient Descent(4342/9999): loss=0.04397754724027375\n",
      "Gradient Descent(4343/9999): loss=0.04397753133046647\n",
      "Gradient Descent(4344/9999): loss=0.043977515445674686\n",
      "Gradient Descent(4345/9999): loss=0.04397749958585908\n",
      "Gradient Descent(4346/9999): loss=0.0439774837509804\n",
      "Gradient Descent(4347/9999): loss=0.043977467940999417\n",
      "Gradient Descent(4348/9999): loss=0.043977452155876985\n",
      "Gradient Descent(4349/9999): loss=0.04397743639557402\n",
      "Gradient Descent(4350/9999): loss=0.04397742066005152\n",
      "Gradient Descent(4351/9999): loss=0.0439774049492705\n",
      "Gradient Descent(4352/9999): loss=0.043977389263192036\n",
      "Gradient Descent(4353/9999): loss=0.04397737360177735\n",
      "Gradient Descent(4354/9999): loss=0.04397735796498761\n",
      "Gradient Descent(4355/9999): loss=0.04397734235278409\n",
      "Gradient Descent(4356/9999): loss=0.04397732676512818\n",
      "Gradient Descent(4357/9999): loss=0.043977311201981256\n",
      "Gradient Descent(4358/9999): loss=0.04397729566330476\n",
      "Gradient Descent(4359/9999): loss=0.04397728014906026\n",
      "Gradient Descent(4360/9999): loss=0.0439772646592093\n",
      "Gradient Descent(4361/9999): loss=0.043977249193713555\n",
      "Gradient Descent(4362/9999): loss=0.04397723375253471\n",
      "Gradient Descent(4363/9999): loss=0.04397721833563458\n",
      "Gradient Descent(4364/9999): loss=0.043977202942974906\n",
      "Gradient Descent(4365/9999): loss=0.04397718757451765\n",
      "Gradient Descent(4366/9999): loss=0.043977172230224716\n",
      "Gradient Descent(4367/9999): loss=0.04397715691005812\n",
      "Gradient Descent(4368/9999): loss=0.04397714161397996\n",
      "Gradient Descent(4369/9999): loss=0.04397712634195229\n",
      "Gradient Descent(4370/9999): loss=0.04397711109393735\n",
      "Gradient Descent(4371/9999): loss=0.043977095869897356\n",
      "Gradient Descent(4372/9999): loss=0.04397708066979464\n",
      "Gradient Descent(4373/9999): loss=0.04397706549359154\n",
      "Gradient Descent(4374/9999): loss=0.04397705034125047\n",
      "Gradient Descent(4375/9999): loss=0.04397703521273395\n",
      "Gradient Descent(4376/9999): loss=0.04397702010800449\n",
      "Gradient Descent(4377/9999): loss=0.04397700502702471\n",
      "Gradient Descent(4378/9999): loss=0.043976989969757234\n",
      "Gradient Descent(4379/9999): loss=0.0439769749361648\n",
      "Gradient Descent(4380/9999): loss=0.04397695992621019\n",
      "Gradient Descent(4381/9999): loss=0.04397694493985624\n",
      "Gradient Descent(4382/9999): loss=0.04397692997706581\n",
      "Gradient Descent(4383/9999): loss=0.043976915037801864\n",
      "Gradient Descent(4384/9999): loss=0.04397690012202743\n",
      "Gradient Descent(4385/9999): loss=0.04397688522970556\n",
      "Gradient Descent(4386/9999): loss=0.04397687036079936\n",
      "Gradient Descent(4387/9999): loss=0.043976855515272056\n",
      "Gradient Descent(4388/9999): loss=0.043976840693086876\n",
      "Gradient Descent(4389/9999): loss=0.043976825894207094\n",
      "Gradient Descent(4390/9999): loss=0.04397681111859608\n",
      "Gradient Descent(4391/9999): loss=0.04397679636621724\n",
      "Gradient Descent(4392/9999): loss=0.04397678163703407\n",
      "Gradient Descent(4393/9999): loss=0.043976766931010054\n",
      "Gradient Descent(4394/9999): loss=0.04397675224810883\n",
      "Gradient Descent(4395/9999): loss=0.04397673758829402\n",
      "Gradient Descent(4396/9999): loss=0.0439767229515293\n",
      "Gradient Descent(4397/9999): loss=0.04397670833777848\n",
      "Gradient Descent(4398/9999): loss=0.043976693747005316\n",
      "Gradient Descent(4399/9999): loss=0.043976679179173715\n",
      "Gradient Descent(4400/9999): loss=0.0439766646342476\n",
      "Gradient Descent(4401/9999): loss=0.043976650112190964\n",
      "Gradient Descent(4402/9999): loss=0.04397663561296781\n",
      "Gradient Descent(4403/9999): loss=0.043976621136542306\n",
      "Gradient Descent(4404/9999): loss=0.043976606682878526\n",
      "Gradient Descent(4405/9999): loss=0.04397659225194075\n",
      "Gradient Descent(4406/9999): loss=0.0439765778436932\n",
      "Gradient Descent(4407/9999): loss=0.04397656345810022\n",
      "Gradient Descent(4408/9999): loss=0.04397654909512618\n",
      "Gradient Descent(4409/9999): loss=0.0439765347547355\n",
      "Gradient Descent(4410/9999): loss=0.043976520436892715\n",
      "Gradient Descent(4411/9999): loss=0.04397650614156234\n",
      "Gradient Descent(4412/9999): loss=0.04397649186870901\n",
      "Gradient Descent(4413/9999): loss=0.04397647761829734\n",
      "Gradient Descent(4414/9999): loss=0.04397646339029203\n",
      "Gradient Descent(4415/9999): loss=0.04397644918465791\n",
      "Gradient Descent(4416/9999): loss=0.043976435001359795\n",
      "Gradient Descent(4417/9999): loss=0.04397642084036253\n",
      "Gradient Descent(4418/9999): loss=0.04397640670163106\n",
      "Gradient Descent(4419/9999): loss=0.0439763925851304\n",
      "Gradient Descent(4420/9999): loss=0.043976378490825606\n",
      "Gradient Descent(4421/9999): loss=0.04397636441868172\n",
      "Gradient Descent(4422/9999): loss=0.04397635036866396\n",
      "Gradient Descent(4423/9999): loss=0.043976336340737464\n",
      "Gradient Descent(4424/9999): loss=0.04397632233486756\n",
      "Gradient Descent(4425/9999): loss=0.04397630835101958\n",
      "Gradient Descent(4426/9999): loss=0.043976294389158836\n",
      "Gradient Descent(4427/9999): loss=0.04397628044925079\n",
      "Gradient Descent(4428/9999): loss=0.04397626653126091\n",
      "Gradient Descent(4429/9999): loss=0.04397625263515477\n",
      "Gradient Descent(4430/9999): loss=0.043976238760897936\n",
      "Gradient Descent(4431/9999): loss=0.04397622490845604\n",
      "Gradient Descent(4432/9999): loss=0.04397621107779479\n",
      "Gradient Descent(4433/9999): loss=0.04397619726887997\n",
      "Gradient Descent(4434/9999): loss=0.04397618348167735\n",
      "Gradient Descent(4435/9999): loss=0.04397616971615283\n",
      "Gradient Descent(4436/9999): loss=0.04397615597227226\n",
      "Gradient Descent(4437/9999): loss=0.04397614225000166\n",
      "Gradient Descent(4438/9999): loss=0.04397612854930706\n",
      "Gradient Descent(4439/9999): loss=0.04397611487015448\n",
      "Gradient Descent(4440/9999): loss=0.043976101212510124\n",
      "Gradient Descent(4441/9999): loss=0.04397608757634011\n",
      "Gradient Descent(4442/9999): loss=0.04397607396161069\n",
      "Gradient Descent(4443/9999): loss=0.04397606036828819\n",
      "Gradient Descent(4444/9999): loss=0.04397604679633888\n",
      "Gradient Descent(4445/9999): loss=0.04397603324572924\n",
      "Gradient Descent(4446/9999): loss=0.043976019716425646\n",
      "Gradient Descent(4447/9999): loss=0.04397600620839462\n",
      "Gradient Descent(4448/9999): loss=0.043975992721602714\n",
      "Gradient Descent(4449/9999): loss=0.04397597925601656\n",
      "Gradient Descent(4450/9999): loss=0.04397596581160279\n",
      "Gradient Descent(4451/9999): loss=0.043975952388328104\n",
      "Gradient Descent(4452/9999): loss=0.04397593898615928\n",
      "Gradient Descent(4453/9999): loss=0.04397592560506312\n",
      "Gradient Descent(4454/9999): loss=0.043975912245006514\n",
      "Gradient Descent(4455/9999): loss=0.04397589890595637\n",
      "Gradient Descent(4456/9999): loss=0.043975885587879654\n",
      "Gradient Descent(4457/9999): loss=0.04397587229074337\n",
      "Gradient Descent(4458/9999): loss=0.04397585901451463\n",
      "Gradient Descent(4459/9999): loss=0.04397584575916056\n",
      "Gradient Descent(4460/9999): loss=0.04397583252464829\n",
      "Gradient Descent(4461/9999): loss=0.04397581931094509\n",
      "Gradient Descent(4462/9999): loss=0.04397580611801824\n",
      "Gradient Descent(4463/9999): loss=0.04397579294583505\n",
      "Gradient Descent(4464/9999): loss=0.04397577979436296\n",
      "Gradient Descent(4465/9999): loss=0.0439757666635693\n",
      "Gradient Descent(4466/9999): loss=0.04397575355342169\n",
      "Gradient Descent(4467/9999): loss=0.043975740463887567\n",
      "Gradient Descent(4468/9999): loss=0.04397572739493455\n",
      "Gradient Descent(4469/9999): loss=0.04397571434653028\n",
      "Gradient Descent(4470/9999): loss=0.04397570131864245\n",
      "Gradient Descent(4471/9999): loss=0.04397568831123879\n",
      "Gradient Descent(4472/9999): loss=0.04397567532428712\n",
      "Gradient Descent(4473/9999): loss=0.04397566235775525\n",
      "Gradient Descent(4474/9999): loss=0.04397564941161109\n",
      "Gradient Descent(4475/9999): loss=0.04397563648582258\n",
      "Gradient Descent(4476/9999): loss=0.04397562358035773\n",
      "Gradient Descent(4477/9999): loss=0.04397561069518454\n",
      "Gradient Descent(4478/9999): loss=0.04397559783027116\n",
      "Gradient Descent(4479/9999): loss=0.04397558498558567\n",
      "Gradient Descent(4480/9999): loss=0.04397557216109635\n",
      "Gradient Descent(4481/9999): loss=0.04397555935677138\n",
      "Gradient Descent(4482/9999): loss=0.04397554657257909\n",
      "Gradient Descent(4483/9999): loss=0.04397553380848776\n",
      "Gradient Descent(4484/9999): loss=0.04397552106446587\n",
      "Gradient Descent(4485/9999): loss=0.043975508340481834\n",
      "Gradient Descent(4486/9999): loss=0.04397549563650413\n",
      "Gradient Descent(4487/9999): loss=0.0439754829525013\n",
      "Gradient Descent(4488/9999): loss=0.04397547028844195\n",
      "Gradient Descent(4489/9999): loss=0.04397545764429474\n",
      "Gradient Descent(4490/9999): loss=0.04397544502002834\n",
      "Gradient Descent(4491/9999): loss=0.04397543241561147\n",
      "Gradient Descent(4492/9999): loss=0.04397541983101295\n",
      "Gradient Descent(4493/9999): loss=0.0439754072662016\n",
      "Gradient Descent(4494/9999): loss=0.04397539472114634\n",
      "Gradient Descent(4495/9999): loss=0.04397538219581607\n",
      "Gradient Descent(4496/9999): loss=0.043975369690179796\n",
      "Gradient Descent(4497/9999): loss=0.04397535720420654\n",
      "Gradient Descent(4498/9999): loss=0.04397534473786541\n",
      "Gradient Descent(4499/9999): loss=0.04397533229112551\n",
      "Gradient Descent(4500/9999): loss=0.04397531986395603\n",
      "Gradient Descent(4501/9999): loss=0.04397530745632619\n",
      "Gradient Descent(4502/9999): loss=0.0439752950682053\n",
      "Gradient Descent(4503/9999): loss=0.04397528269956264\n",
      "Gradient Descent(4504/9999): loss=0.04397527035036762\n",
      "Gradient Descent(4505/9999): loss=0.04397525802058965\n",
      "Gradient Descent(4506/9999): loss=0.04397524571019818\n",
      "Gradient Descent(4507/9999): loss=0.04397523341916277\n",
      "Gradient Descent(4508/9999): loss=0.043975221147452934\n",
      "Gradient Descent(4509/9999): loss=0.043975208895038344\n",
      "Gradient Descent(4510/9999): loss=0.04397519666188862\n",
      "Gradient Descent(4511/9999): loss=0.04397518444797347\n",
      "Gradient Descent(4512/9999): loss=0.04397517225326267\n",
      "Gradient Descent(4513/9999): loss=0.043975160077726\n",
      "Gradient Descent(4514/9999): loss=0.04397514792133337\n",
      "Gradient Descent(4515/9999): loss=0.043975135784054604\n",
      "Gradient Descent(4516/9999): loss=0.04397512366585969\n",
      "Gradient Descent(4517/9999): loss=0.04397511156671862\n",
      "Gradient Descent(4518/9999): loss=0.04397509948660143\n",
      "Gradient Descent(4519/9999): loss=0.043975087425478204\n",
      "Gradient Descent(4520/9999): loss=0.04397507538331908\n",
      "Gradient Descent(4521/9999): loss=0.043975063360094255\n",
      "Gradient Descent(4522/9999): loss=0.043975051355773925\n",
      "Gradient Descent(4523/9999): loss=0.0439750393703284\n",
      "Gradient Descent(4524/9999): loss=0.04397502740372797\n",
      "Gradient Descent(4525/9999): loss=0.04397501545594304\n",
      "Gradient Descent(4526/9999): loss=0.04397500352694401\n",
      "Gradient Descent(4527/9999): loss=0.04397499161670132\n",
      "Gradient Descent(4528/9999): loss=0.043974979725185485\n",
      "Gradient Descent(4529/9999): loss=0.04397496785236713\n",
      "Gradient Descent(4530/9999): loss=0.04397495599821675\n",
      "Gradient Descent(4531/9999): loss=0.04397494416270507\n",
      "Gradient Descent(4532/9999): loss=0.04397493234580274\n",
      "Gradient Descent(4533/9999): loss=0.04397492054748053\n",
      "Gradient Descent(4534/9999): loss=0.04397490876770921\n",
      "Gradient Descent(4535/9999): loss=0.0439748970064596\n",
      "Gradient Descent(4536/9999): loss=0.04397488526370259\n",
      "Gradient Descent(4537/9999): loss=0.043974873539409126\n",
      "Gradient Descent(4538/9999): loss=0.043974861833550156\n",
      "Gradient Descent(4539/9999): loss=0.04397485014609668\n",
      "Gradient Descent(4540/9999): loss=0.0439748384770198\n",
      "Gradient Descent(4541/9999): loss=0.043974826826290565\n",
      "Gradient Descent(4542/9999): loss=0.04397481519388016\n",
      "Gradient Descent(4543/9999): loss=0.0439748035797598\n",
      "Gradient Descent(4544/9999): loss=0.043974791983900675\n",
      "Gradient Descent(4545/9999): loss=0.043974780406274123\n",
      "Gradient Descent(4546/9999): loss=0.043974768846851454\n",
      "Gradient Descent(4547/9999): loss=0.04397475730560405\n",
      "Gradient Descent(4548/9999): loss=0.04397474578250331\n",
      "Gradient Descent(4549/9999): loss=0.04397473427752077\n",
      "Gradient Descent(4550/9999): loss=0.043974722790627856\n",
      "Gradient Descent(4551/9999): loss=0.04397471132179616\n",
      "Gradient Descent(4552/9999): loss=0.043974699870997294\n",
      "Gradient Descent(4553/9999): loss=0.04397468843820293\n",
      "Gradient Descent(4554/9999): loss=0.04397467702338469\n",
      "Gradient Descent(4555/9999): loss=0.043974665626514364\n",
      "Gradient Descent(4556/9999): loss=0.043974654247563694\n",
      "Gradient Descent(4557/9999): loss=0.04397464288650455\n",
      "Gradient Descent(4558/9999): loss=0.04397463154330875\n",
      "Gradient Descent(4559/9999): loss=0.04397462021794825\n",
      "Gradient Descent(4560/9999): loss=0.04397460891039497\n",
      "Gradient Descent(4561/9999): loss=0.04397459762062097\n",
      "Gradient Descent(4562/9999): loss=0.04397458634859821\n",
      "Gradient Descent(4563/9999): loss=0.04397457509429886\n",
      "Gradient Descent(4564/9999): loss=0.043974563857695\n",
      "Gradient Descent(4565/9999): loss=0.04397455263875881\n",
      "Gradient Descent(4566/9999): loss=0.043974541437462544\n",
      "Gradient Descent(4567/9999): loss=0.043974530253778454\n",
      "Gradient Descent(4568/9999): loss=0.04397451908767882\n",
      "Gradient Descent(4569/9999): loss=0.04397450793913602\n",
      "Gradient Descent(4570/9999): loss=0.04397449680812246\n",
      "Gradient Descent(4571/9999): loss=0.04397448569461053\n",
      "Gradient Descent(4572/9999): loss=0.04397447459857276\n",
      "Gradient Descent(4573/9999): loss=0.04397446351998167\n",
      "Gradient Descent(4574/9999): loss=0.04397445245880978\n",
      "Gradient Descent(4575/9999): loss=0.043974441415029764\n",
      "Gradient Descent(4576/9999): loss=0.043974430388614226\n",
      "Gradient Descent(4577/9999): loss=0.04397441937953592\n",
      "Gradient Descent(4578/9999): loss=0.04397440838776754\n",
      "Gradient Descent(4579/9999): loss=0.043974397413281865\n",
      "Gradient Descent(4580/9999): loss=0.043974386456051756\n",
      "Gradient Descent(4581/9999): loss=0.04397437551605003\n",
      "Gradient Descent(4582/9999): loss=0.04397436459324967\n",
      "Gradient Descent(4583/9999): loss=0.04397435368762356\n",
      "Gradient Descent(4584/9999): loss=0.043974342799144764\n",
      "Gradient Descent(4585/9999): loss=0.043974331927786256\n",
      "Gradient Descent(4586/9999): loss=0.04397432107352115\n",
      "Gradient Descent(4587/9999): loss=0.043974310236322554\n",
      "Gradient Descent(4588/9999): loss=0.04397429941616366\n",
      "Gradient Descent(4589/9999): loss=0.043974288613017644\n",
      "Gradient Descent(4590/9999): loss=0.04397427782685779\n",
      "Gradient Descent(4591/9999): loss=0.04397426705765737\n",
      "Gradient Descent(4592/9999): loss=0.043974256305389696\n",
      "Gradient Descent(4593/9999): loss=0.04397424557002821\n",
      "Gradient Descent(4594/9999): loss=0.04397423485154625\n",
      "Gradient Descent(4595/9999): loss=0.04397422414991732\n",
      "Gradient Descent(4596/9999): loss=0.04397421346511493\n",
      "Gradient Descent(4597/9999): loss=0.04397420279711259\n",
      "Gradient Descent(4598/9999): loss=0.04397419214588392\n",
      "Gradient Descent(4599/9999): loss=0.04397418151140253\n",
      "Gradient Descent(4600/9999): loss=0.04397417089364206\n",
      "Gradient Descent(4601/9999): loss=0.043974160292576274\n",
      "Gradient Descent(4602/9999): loss=0.0439741497081789\n",
      "Gradient Descent(4603/9999): loss=0.043974139140423675\n",
      "Gradient Descent(4604/9999): loss=0.04397412858928454\n",
      "Gradient Descent(4605/9999): loss=0.04397411805473527\n",
      "Gradient Descent(4606/9999): loss=0.04397410753674985\n",
      "Gradient Descent(4607/9999): loss=0.0439740970353022\n",
      "Gradient Descent(4608/9999): loss=0.043974086550366304\n",
      "Gradient Descent(4609/9999): loss=0.043974076081916225\n",
      "Gradient Descent(4610/9999): loss=0.043974065629926055\n",
      "Gradient Descent(4611/9999): loss=0.043974055194369886\n",
      "Gradient Descent(4612/9999): loss=0.04397404477522189\n",
      "Gradient Descent(4613/9999): loss=0.043974034372456235\n",
      "Gradient Descent(4614/9999): loss=0.04397402398604724\n",
      "Gradient Descent(4615/9999): loss=0.04397401361596911\n",
      "Gradient Descent(4616/9999): loss=0.043974003262196214\n",
      "Gradient Descent(4617/9999): loss=0.04397399292470286\n",
      "Gradient Descent(4618/9999): loss=0.0439739826034635\n",
      "Gradient Descent(4619/9999): loss=0.043973972298452596\n",
      "Gradient Descent(4620/9999): loss=0.04397396200964461\n",
      "Gradient Descent(4621/9999): loss=0.043973951737014044\n",
      "Gradient Descent(4622/9999): loss=0.04397394148053544\n",
      "Gradient Descent(4623/9999): loss=0.04397393124018347\n",
      "Gradient Descent(4624/9999): loss=0.04397392101593271\n",
      "Gradient Descent(4625/9999): loss=0.04397391080775792\n",
      "Gradient Descent(4626/9999): loss=0.04397390061563376\n",
      "Gradient Descent(4627/9999): loss=0.043973890439535014\n",
      "Gradient Descent(4628/9999): loss=0.043973880279436495\n",
      "Gradient Descent(4629/9999): loss=0.043973870135313044\n",
      "Gradient Descent(4630/9999): loss=0.04397386000713952\n",
      "Gradient Descent(4631/9999): loss=0.04397384989489086\n",
      "Gradient Descent(4632/9999): loss=0.043973839798542\n",
      "Gradient Descent(4633/9999): loss=0.043973829718068\n",
      "Gradient Descent(4634/9999): loss=0.04397381965344383\n",
      "Gradient Descent(4635/9999): loss=0.043973809604644636\n",
      "Gradient Descent(4636/9999): loss=0.043973799571645475\n",
      "Gradient Descent(4637/9999): loss=0.04397378955442154\n",
      "Gradient Descent(4638/9999): loss=0.04397377955294802\n",
      "Gradient Descent(4639/9999): loss=0.04397376956720014\n",
      "Gradient Descent(4640/9999): loss=0.04397375959715318\n",
      "Gradient Descent(4641/9999): loss=0.043973749642782466\n",
      "Gradient Descent(4642/9999): loss=0.04397373970406333\n",
      "Gradient Descent(4643/9999): loss=0.04397372978097118\n",
      "Gradient Descent(4644/9999): loss=0.04397371987348144\n",
      "Gradient Descent(4645/9999): loss=0.043973709981569555\n",
      "Gradient Descent(4646/9999): loss=0.04397370010521106\n",
      "Gradient Descent(4647/9999): loss=0.04397369024438149\n",
      "Gradient Descent(4648/9999): loss=0.04397368039905643\n",
      "Gradient Descent(4649/9999): loss=0.04397367056921149\n",
      "Gradient Descent(4650/9999): loss=0.043973660754822355\n",
      "Gradient Descent(4651/9999): loss=0.04397365095586471\n",
      "Gradient Descent(4652/9999): loss=0.043973641172314275\n",
      "Gradient Descent(4653/9999): loss=0.043973631404146854\n",
      "Gradient Descent(4654/9999): loss=0.04397362165133823\n",
      "Gradient Descent(4655/9999): loss=0.043973611913864286\n",
      "Gradient Descent(4656/9999): loss=0.043973602191700885\n",
      "Gradient Descent(4657/9999): loss=0.04397359248482397\n",
      "Gradient Descent(4658/9999): loss=0.04397358279320948\n",
      "Gradient Descent(4659/9999): loss=0.04397357311683346\n",
      "Gradient Descent(4660/9999): loss=0.04397356345567193\n",
      "Gradient Descent(4661/9999): loss=0.04397355380970094\n",
      "Gradient Descent(4662/9999): loss=0.04397354417889662\n",
      "Gradient Descent(4663/9999): loss=0.04397353456323517\n",
      "Gradient Descent(4664/9999): loss=0.043973524962692745\n",
      "Gradient Descent(4665/9999): loss=0.04397351537724553\n",
      "Gradient Descent(4666/9999): loss=0.04397350580686985\n",
      "Gradient Descent(4667/9999): loss=0.04397349625154198\n",
      "Gradient Descent(4668/9999): loss=0.043973486711238276\n",
      "Gradient Descent(4669/9999): loss=0.043973477185935125\n",
      "Gradient Descent(4670/9999): loss=0.04397346767560891\n",
      "Gradient Descent(4671/9999): loss=0.043973458180236076\n",
      "Gradient Descent(4672/9999): loss=0.04397344869979314\n",
      "Gradient Descent(4673/9999): loss=0.043973439234256645\n",
      "Gradient Descent(4674/9999): loss=0.04397342978360309\n",
      "Gradient Descent(4675/9999): loss=0.04397342034780913\n",
      "Gradient Descent(4676/9999): loss=0.04397341092685138\n",
      "Gradient Descent(4677/9999): loss=0.04397340152070652\n",
      "Gradient Descent(4678/9999): loss=0.043973392129351245\n",
      "Gradient Descent(4679/9999): loss=0.04397338275276229\n",
      "Gradient Descent(4680/9999): loss=0.043973373390916476\n",
      "Gradient Descent(4681/9999): loss=0.04397336404379059\n",
      "Gradient Descent(4682/9999): loss=0.04397335471136152\n",
      "Gradient Descent(4683/9999): loss=0.04397334539360612\n",
      "Gradient Descent(4684/9999): loss=0.04397333609050134\n",
      "Gradient Descent(4685/9999): loss=0.043973326802024156\n",
      "Gradient Descent(4686/9999): loss=0.043973317528151507\n",
      "Gradient Descent(4687/9999): loss=0.04397330826886052\n",
      "Gradient Descent(4688/9999): loss=0.04397329902412822\n",
      "Gradient Descent(4689/9999): loss=0.0439732897939317\n",
      "Gradient Descent(4690/9999): loss=0.04397328057824813\n",
      "Gradient Descent(4691/9999): loss=0.043973271377054673\n",
      "Gradient Descent(4692/9999): loss=0.043973262190328564\n",
      "Gradient Descent(4693/9999): loss=0.04397325301804704\n",
      "Gradient Descent(4694/9999): loss=0.04397324386018741\n",
      "Gradient Descent(4695/9999): loss=0.04397323471672699\n",
      "Gradient Descent(4696/9999): loss=0.04397322558764313\n",
      "Gradient Descent(4697/9999): loss=0.04397321647291322\n",
      "Gradient Descent(4698/9999): loss=0.04397320737251472\n",
      "Gradient Descent(4699/9999): loss=0.0439731982864251\n",
      "Gradient Descent(4700/9999): loss=0.04397318921462177\n",
      "Gradient Descent(4701/9999): loss=0.043973180157082375\n",
      "Gradient Descent(4702/9999): loss=0.04397317111378445\n",
      "Gradient Descent(4703/9999): loss=0.04397316208470561\n",
      "Gradient Descent(4704/9999): loss=0.04397315306982348\n",
      "Gradient Descent(4705/9999): loss=0.043973144069115754\n",
      "Gradient Descent(4706/9999): loss=0.04397313508256012\n",
      "Gradient Descent(4707/9999): loss=0.04397312611013434\n",
      "Gradient Descent(4708/9999): loss=0.043973117151816225\n",
      "Gradient Descent(4709/9999): loss=0.04397310820758357\n",
      "Gradient Descent(4710/9999): loss=0.043973099277414204\n",
      "Gradient Descent(4711/9999): loss=0.04397309036128605\n",
      "Gradient Descent(4712/9999): loss=0.043973081459177034\n",
      "Gradient Descent(4713/9999): loss=0.04397307257106509\n",
      "Gradient Descent(4714/9999): loss=0.043973063696928216\n",
      "Gradient Descent(4715/9999): loss=0.043973054836744435\n",
      "Gradient Descent(4716/9999): loss=0.04397304599049183\n",
      "Gradient Descent(4717/9999): loss=0.04397303715814848\n",
      "Gradient Descent(4718/9999): loss=0.0439730283396925\n",
      "Gradient Descent(4719/9999): loss=0.043973019535102076\n",
      "Gradient Descent(4720/9999): loss=0.04397301074435539\n",
      "Gradient Descent(4721/9999): loss=0.043973001967430696\n",
      "Gradient Descent(4722/9999): loss=0.043972993204306275\n",
      "Gradient Descent(4723/9999): loss=0.043972984454960395\n",
      "Gradient Descent(4724/9999): loss=0.04397297571937136\n",
      "Gradient Descent(4725/9999): loss=0.043972966997517605\n",
      "Gradient Descent(4726/9999): loss=0.04397295828937751\n",
      "Gradient Descent(4727/9999): loss=0.043972949594929533\n",
      "Gradient Descent(4728/9999): loss=0.04397294091415211\n",
      "Gradient Descent(4729/9999): loss=0.04397293224702376\n",
      "Gradient Descent(4730/9999): loss=0.043972923593523\n",
      "Gradient Descent(4731/9999): loss=0.043972914953628456\n",
      "Gradient Descent(4732/9999): loss=0.043972906327318685\n",
      "Gradient Descent(4733/9999): loss=0.04397289771457235\n",
      "Gradient Descent(4734/9999): loss=0.043972889115368126\n",
      "Gradient Descent(4735/9999): loss=0.04397288052968472\n",
      "Gradient Descent(4736/9999): loss=0.04397287195750086\n",
      "Gradient Descent(4737/9999): loss=0.04397286339879534\n",
      "Gradient Descent(4738/9999): loss=0.04397285485354692\n",
      "Gradient Descent(4739/9999): loss=0.043972846321734524\n",
      "Gradient Descent(4740/9999): loss=0.04397283780333695\n",
      "Gradient Descent(4741/9999): loss=0.043972829298333134\n",
      "Gradient Descent(4742/9999): loss=0.043972820806702034\n",
      "Gradient Descent(4743/9999): loss=0.04397281232842261\n",
      "Gradient Descent(4744/9999): loss=0.043972803863473846\n",
      "Gradient Descent(4745/9999): loss=0.043972795411834804\n",
      "Gradient Descent(4746/9999): loss=0.043972786973484564\n",
      "Gradient Descent(4747/9999): loss=0.043972778548402196\n",
      "Gradient Descent(4748/9999): loss=0.04397277013656689\n",
      "Gradient Descent(4749/9999): loss=0.043972761737957784\n",
      "Gradient Descent(4750/9999): loss=0.043972753352554114\n",
      "Gradient Descent(4751/9999): loss=0.04397274498033506\n",
      "Gradient Descent(4752/9999): loss=0.04397273662127991\n",
      "Gradient Descent(4753/9999): loss=0.04397272827536798\n",
      "Gradient Descent(4754/9999): loss=0.043972719942578654\n",
      "Gradient Descent(4755/9999): loss=0.04397271162289121\n",
      "Gradient Descent(4756/9999): loss=0.04397270331628508\n",
      "Gradient Descent(4757/9999): loss=0.04397269502273971\n",
      "Gradient Descent(4758/9999): loss=0.043972686742234555\n",
      "Gradient Descent(4759/9999): loss=0.04397267847474914\n",
      "Gradient Descent(4760/9999): loss=0.043972670220262924\n",
      "Gradient Descent(4761/9999): loss=0.043972661978755524\n",
      "Gradient Descent(4762/9999): loss=0.04397265375020653\n",
      "Gradient Descent(4763/9999): loss=0.04397264553459554\n",
      "Gradient Descent(4764/9999): loss=0.04397263733190222\n",
      "Gradient Descent(4765/9999): loss=0.043972629142106305\n",
      "Gradient Descent(4766/9999): loss=0.043972620965187455\n",
      "Gradient Descent(4767/9999): loss=0.04397261280112543\n",
      "Gradient Descent(4768/9999): loss=0.04397260464990005\n",
      "Gradient Descent(4769/9999): loss=0.043972596511491095\n",
      "Gradient Descent(4770/9999): loss=0.043972588385878454\n",
      "Gradient Descent(4771/9999): loss=0.04397258027304196\n",
      "Gradient Descent(4772/9999): loss=0.04397257217296157\n",
      "Gradient Descent(4773/9999): loss=0.043972564085617165\n",
      "Gradient Descent(4774/9999): loss=0.04397255601098879\n",
      "Gradient Descent(4775/9999): loss=0.043972547949056404\n",
      "Gradient Descent(4776/9999): loss=0.04397253989980008\n",
      "Gradient Descent(4777/9999): loss=0.043972531863199854\n",
      "Gradient Descent(4778/9999): loss=0.043972523839235854\n",
      "Gradient Descent(4779/9999): loss=0.043972515827888185\n",
      "Gradient Descent(4780/9999): loss=0.043972507829137024\n",
      "Gradient Descent(4781/9999): loss=0.04397249984296257\n",
      "Gradient Descent(4782/9999): loss=0.04397249186934505\n",
      "Gradient Descent(4783/9999): loss=0.043972483908264685\n",
      "Gradient Descent(4784/9999): loss=0.043972475959701815\n",
      "Gradient Descent(4785/9999): loss=0.04397246802363673\n",
      "Gradient Descent(4786/9999): loss=0.04397246010004976\n",
      "Gradient Descent(4787/9999): loss=0.043972452188921346\n",
      "Gradient Descent(4788/9999): loss=0.04397244429023183\n",
      "Gradient Descent(4789/9999): loss=0.043972436403961694\n",
      "Gradient Descent(4790/9999): loss=0.04397242853009141\n",
      "Gradient Descent(4791/9999): loss=0.043972420668601464\n",
      "Gradient Descent(4792/9999): loss=0.04397241281947241\n",
      "Gradient Descent(4793/9999): loss=0.04397240498268478\n",
      "Gradient Descent(4794/9999): loss=0.043972397158219205\n",
      "Gradient Descent(4795/9999): loss=0.04397238934605631\n",
      "Gradient Descent(4796/9999): loss=0.04397238154617669\n",
      "Gradient Descent(4797/9999): loss=0.04397237375856111\n",
      "Gradient Descent(4798/9999): loss=0.043972365983190265\n",
      "Gradient Descent(4799/9999): loss=0.043972358220044895\n",
      "Gradient Descent(4800/9999): loss=0.04397235046910574\n",
      "Gradient Descent(4801/9999): loss=0.04397234273035368\n",
      "Gradient Descent(4802/9999): loss=0.043972335003769486\n",
      "Gradient Descent(4803/9999): loss=0.043972327289334076\n",
      "Gradient Descent(4804/9999): loss=0.043972319587028325\n",
      "Gradient Descent(4805/9999): loss=0.04397231189683318\n",
      "Gradient Descent(4806/9999): loss=0.04397230421872956\n",
      "Gradient Descent(4807/9999): loss=0.04397229655269845\n",
      "Gradient Descent(4808/9999): loss=0.04397228889872093\n",
      "Gradient Descent(4809/9999): loss=0.04397228125677803\n",
      "Gradient Descent(4810/9999): loss=0.04397227362685077\n",
      "Gradient Descent(4811/9999): loss=0.043972266008920326\n",
      "Gradient Descent(4812/9999): loss=0.04397225840296779\n",
      "Gradient Descent(4813/9999): loss=0.04397225080897435\n",
      "Gradient Descent(4814/9999): loss=0.0439722432269212\n",
      "Gradient Descent(4815/9999): loss=0.04397223565678957\n",
      "Gradient Descent(4816/9999): loss=0.043972228098560696\n",
      "Gradient Descent(4817/9999): loss=0.04397222055221586\n",
      "Gradient Descent(4818/9999): loss=0.04397221301773642\n",
      "Gradient Descent(4819/9999): loss=0.04397220549510368\n",
      "Gradient Descent(4820/9999): loss=0.04397219798429904\n",
      "Gradient Descent(4821/9999): loss=0.04397219048530387\n",
      "Gradient Descent(4822/9999): loss=0.043972182998099615\n",
      "Gradient Descent(4823/9999): loss=0.04397217552266775\n",
      "Gradient Descent(4824/9999): loss=0.043972168058989757\n",
      "Gradient Descent(4825/9999): loss=0.04397216060704712\n",
      "Gradient Descent(4826/9999): loss=0.043972153166821445\n",
      "Gradient Descent(4827/9999): loss=0.04397214573829426\n",
      "Gradient Descent(4828/9999): loss=0.04397213832144724\n",
      "Gradient Descent(4829/9999): loss=0.043972130916261946\n",
      "Gradient Descent(4830/9999): loss=0.04397212352272007\n",
      "Gradient Descent(4831/9999): loss=0.0439721161408033\n",
      "Gradient Descent(4832/9999): loss=0.043972108770493366\n",
      "Gradient Descent(4833/9999): loss=0.043972101411772026\n",
      "Gradient Descent(4834/9999): loss=0.04397209406462102\n",
      "Gradient Descent(4835/9999): loss=0.04397208672902221\n",
      "Gradient Descent(4836/9999): loss=0.04397207940495739\n",
      "Gradient Descent(4837/9999): loss=0.04397207209240843\n",
      "Gradient Descent(4838/9999): loss=0.043972064791357256\n",
      "Gradient Descent(4839/9999): loss=0.04397205750178576\n",
      "Gradient Descent(4840/9999): loss=0.04397205022367588\n",
      "Gradient Descent(4841/9999): loss=0.043972042957009634\n",
      "Gradient Descent(4842/9999): loss=0.043972035701769004\n",
      "Gradient Descent(4843/9999): loss=0.04397202845793603\n",
      "Gradient Descent(4844/9999): loss=0.04397202122549278\n",
      "Gradient Descent(4845/9999): loss=0.04397201400442133\n",
      "Gradient Descent(4846/9999): loss=0.043972006794703804\n",
      "Gradient Descent(4847/9999): loss=0.043971999596322364\n",
      "Gradient Descent(4848/9999): loss=0.04397199240925917\n",
      "Gradient Descent(4849/9999): loss=0.04397198523349644\n",
      "Gradient Descent(4850/9999): loss=0.0439719780690164\n",
      "Gradient Descent(4851/9999): loss=0.0439719709158013\n",
      "Gradient Descent(4852/9999): loss=0.04397196377383343\n",
      "Gradient Descent(4853/9999): loss=0.043971956643095125\n",
      "Gradient Descent(4854/9999): loss=0.04397194952356874\n",
      "Gradient Descent(4855/9999): loss=0.043971942415236595\n",
      "Gradient Descent(4856/9999): loss=0.0439719353180811\n",
      "Gradient Descent(4857/9999): loss=0.04397192823208473\n",
      "Gradient Descent(4858/9999): loss=0.04397192115722988\n",
      "Gradient Descent(4859/9999): loss=0.04397191409349907\n",
      "Gradient Descent(4860/9999): loss=0.043971907040874794\n",
      "Gradient Descent(4861/9999): loss=0.04397189999933961\n",
      "Gradient Descent(4862/9999): loss=0.04397189296887606\n",
      "Gradient Descent(4863/9999): loss=0.043971885949466716\n",
      "Gradient Descent(4864/9999): loss=0.043971878941094236\n",
      "Gradient Descent(4865/9999): loss=0.043971871943741254\n",
      "Gradient Descent(4866/9999): loss=0.04397186495739043\n",
      "Gradient Descent(4867/9999): loss=0.04397185798202449\n",
      "Gradient Descent(4868/9999): loss=0.04397185101762615\n",
      "Gradient Descent(4869/9999): loss=0.04397184406417816\n",
      "Gradient Descent(4870/9999): loss=0.043971837121663304\n",
      "Gradient Descent(4871/9999): loss=0.043971830190064384\n",
      "Gradient Descent(4872/9999): loss=0.04397182326936428\n",
      "Gradient Descent(4873/9999): loss=0.043971816359545786\n",
      "Gradient Descent(4874/9999): loss=0.043971809460591864\n",
      "Gradient Descent(4875/9999): loss=0.04397180257248536\n",
      "Gradient Descent(4876/9999): loss=0.043971795695209266\n",
      "Gradient Descent(4877/9999): loss=0.04397178882874653\n",
      "Gradient Descent(4878/9999): loss=0.04397178197308019\n",
      "Gradient Descent(4879/9999): loss=0.043971775128193204\n",
      "Gradient Descent(4880/9999): loss=0.043971768294068694\n",
      "Gradient Descent(4881/9999): loss=0.04397176147068968\n",
      "Gradient Descent(4882/9999): loss=0.04397175465803931\n",
      "Gradient Descent(4883/9999): loss=0.043971747856100675\n",
      "Gradient Descent(4884/9999): loss=0.043971741064856955\n",
      "Gradient Descent(4885/9999): loss=0.04397173428429135\n",
      "Gradient Descent(4886/9999): loss=0.04397172751438704\n",
      "Gradient Descent(4887/9999): loss=0.0439717207551273\n",
      "Gradient Descent(4888/9999): loss=0.04397171400649534\n",
      "Gradient Descent(4889/9999): loss=0.043971707268474464\n",
      "Gradient Descent(4890/9999): loss=0.043971700541048035\n",
      "Gradient Descent(4891/9999): loss=0.043971693824199354\n",
      "Gradient Descent(4892/9999): loss=0.04397168711791178\n",
      "Gradient Descent(4893/9999): loss=0.043971680422168744\n",
      "Gradient Descent(4894/9999): loss=0.043971673736953605\n",
      "Gradient Descent(4895/9999): loss=0.04397166706224989\n",
      "Gradient Descent(4896/9999): loss=0.04397166039804102\n",
      "Gradient Descent(4897/9999): loss=0.043971653744310515\n",
      "Gradient Descent(4898/9999): loss=0.04397164710104188\n",
      "Gradient Descent(4899/9999): loss=0.043971640468218685\n",
      "Gradient Descent(4900/9999): loss=0.04397163384582451\n",
      "Gradient Descent(4901/9999): loss=0.04397162723384293\n",
      "Gradient Descent(4902/9999): loss=0.04397162063225757\n",
      "Gradient Descent(4903/9999): loss=0.043971614041052105\n",
      "Gradient Descent(4904/9999): loss=0.04397160746021022\n",
      "Gradient Descent(4905/9999): loss=0.04397160088971563\n",
      "Gradient Descent(4906/9999): loss=0.04397159432955202\n",
      "Gradient Descent(4907/9999): loss=0.0439715877797032\n",
      "Gradient Descent(4908/9999): loss=0.04397158124015288\n",
      "Gradient Descent(4909/9999): loss=0.04397157471088493\n",
      "Gradient Descent(4910/9999): loss=0.04397156819188317\n",
      "Gradient Descent(4911/9999): loss=0.04397156168313145\n",
      "Gradient Descent(4912/9999): loss=0.04397155518461367\n",
      "Gradient Descent(4913/9999): loss=0.04397154869631371\n",
      "Gradient Descent(4914/9999): loss=0.04397154221821552\n",
      "Gradient Descent(4915/9999): loss=0.043971535750303055\n",
      "Gradient Descent(4916/9999): loss=0.04397152929256031\n",
      "Gradient Descent(4917/9999): loss=0.04397152284497127\n",
      "Gradient Descent(4918/9999): loss=0.043971516407520006\n",
      "Gradient Descent(4919/9999): loss=0.043971509980190567\n",
      "Gradient Descent(4920/9999): loss=0.043971503562967\n",
      "Gradient Descent(4921/9999): loss=0.04397149715583347\n",
      "Gradient Descent(4922/9999): loss=0.04397149075877409\n",
      "Gradient Descent(4923/9999): loss=0.043971484371773\n",
      "Gradient Descent(4924/9999): loss=0.04397147799481442\n",
      "Gradient Descent(4925/9999): loss=0.04397147162788254\n",
      "Gradient Descent(4926/9999): loss=0.04397146527096159\n",
      "Gradient Descent(4927/9999): loss=0.043971458924035846\n",
      "Gradient Descent(4928/9999): loss=0.043971452587089566\n",
      "Gradient Descent(4929/9999): loss=0.04397144626010711\n",
      "Gradient Descent(4930/9999): loss=0.04397143994307274\n",
      "Gradient Descent(4931/9999): loss=0.043971433635970884\n",
      "Gradient Descent(4932/9999): loss=0.04397142733878587\n",
      "Gradient Descent(4933/9999): loss=0.043971421051502126\n",
      "Gradient Descent(4934/9999): loss=0.043971414774104114\n",
      "Gradient Descent(4935/9999): loss=0.04397140850657623\n",
      "Gradient Descent(4936/9999): loss=0.043971402248903016\n",
      "Gradient Descent(4937/9999): loss=0.04397139600106891\n",
      "Gradient Descent(4938/9999): loss=0.04397138976305852\n",
      "Gradient Descent(4939/9999): loss=0.043971383534856356\n",
      "Gradient Descent(4940/9999): loss=0.04397137731644698\n",
      "Gradient Descent(4941/9999): loss=0.04397137110781503\n",
      "Gradient Descent(4942/9999): loss=0.043971364908945106\n",
      "Gradient Descent(4943/9999): loss=0.043971358719821864\n",
      "Gradient Descent(4944/9999): loss=0.043971352540430005\n",
      "Gradient Descent(4945/9999): loss=0.04397134637075419\n",
      "Gradient Descent(4946/9999): loss=0.04397134021077918\n",
      "Gradient Descent(4947/9999): loss=0.0439713340604897\n",
      "Gradient Descent(4948/9999): loss=0.04397132791987051\n",
      "Gradient Descent(4949/9999): loss=0.04397132178890643\n",
      "Gradient Descent(4950/9999): loss=0.04397131566758226\n",
      "Gradient Descent(4951/9999): loss=0.04397130955588284\n",
      "Gradient Descent(4952/9999): loss=0.04397130345379306\n",
      "Gradient Descent(4953/9999): loss=0.043971297361297824\n",
      "Gradient Descent(4954/9999): loss=0.043971291278381994\n",
      "Gradient Descent(4955/9999): loss=0.043971285205030526\n",
      "Gradient Descent(4956/9999): loss=0.04397127914122839\n",
      "Gradient Descent(4957/9999): loss=0.043971273086960576\n",
      "Gradient Descent(4958/9999): loss=0.043971267042212096\n",
      "Gradient Descent(4959/9999): loss=0.04397126100696796\n",
      "Gradient Descent(4960/9999): loss=0.043971254981213244\n",
      "Gradient Descent(4961/9999): loss=0.043971248964933034\n",
      "Gradient Descent(4962/9999): loss=0.04397124295811241\n",
      "Gradient Descent(4963/9999): loss=0.04397123696073649\n",
      "Gradient Descent(4964/9999): loss=0.04397123097279047\n",
      "Gradient Descent(4965/9999): loss=0.0439712249942595\n",
      "Gradient Descent(4966/9999): loss=0.04397121902512875\n",
      "Gradient Descent(4967/9999): loss=0.04397121306538347\n",
      "Gradient Descent(4968/9999): loss=0.0439712071150089\n",
      "Gradient Descent(4969/9999): loss=0.043971201173990296\n",
      "Gradient Descent(4970/9999): loss=0.04397119524231296\n",
      "Gradient Descent(4971/9999): loss=0.04397118931996219\n",
      "Gradient Descent(4972/9999): loss=0.04397118340692331\n",
      "Gradient Descent(4973/9999): loss=0.04397117750318172\n",
      "Gradient Descent(4974/9999): loss=0.04397117160872277\n",
      "Gradient Descent(4975/9999): loss=0.04397116572353189\n",
      "Gradient Descent(4976/9999): loss=0.04397115984759444\n",
      "Gradient Descent(4977/9999): loss=0.043971153980895974\n",
      "Gradient Descent(4978/9999): loss=0.04397114812342189\n",
      "Gradient Descent(4979/9999): loss=0.04397114227515772\n",
      "Gradient Descent(4980/9999): loss=0.04397113643608896\n",
      "Gradient Descent(4981/9999): loss=0.04397113060620115\n",
      "Gradient Descent(4982/9999): loss=0.043971124785479876\n",
      "Gradient Descent(4983/9999): loss=0.04397111897391071\n",
      "Gradient Descent(4984/9999): loss=0.043971113171479274\n",
      "Gradient Descent(4985/9999): loss=0.04397110737817117\n",
      "Gradient Descent(4986/9999): loss=0.04397110159397211\n",
      "Gradient Descent(4987/9999): loss=0.04397109581886774\n",
      "Gradient Descent(4988/9999): loss=0.04397109005284373\n",
      "Gradient Descent(4989/9999): loss=0.04397108429588583\n",
      "Gradient Descent(4990/9999): loss=0.04397107854797979\n",
      "Gradient Descent(4991/9999): loss=0.04397107280911138\n",
      "Gradient Descent(4992/9999): loss=0.0439710670792664\n",
      "Gradient Descent(4993/9999): loss=0.043971061358430624\n",
      "Gradient Descent(4994/9999): loss=0.0439710556465899\n",
      "Gradient Descent(4995/9999): loss=0.04397104994373011\n",
      "Gradient Descent(4996/9999): loss=0.0439710442498371\n",
      "Gradient Descent(4997/9999): loss=0.043971038564896786\n",
      "Gradient Descent(4998/9999): loss=0.043971032888895104\n",
      "Gradient Descent(4999/9999): loss=0.043971027221818\n",
      "Gradient Descent(5000/9999): loss=0.04397102156365139\n",
      "Gradient Descent(5001/9999): loss=0.04397101591438132\n",
      "Gradient Descent(5002/9999): loss=0.0439710102739938\n",
      "Gradient Descent(5003/9999): loss=0.04397100464247483\n",
      "Gradient Descent(5004/9999): loss=0.04397099901981049\n",
      "Gradient Descent(5005/9999): loss=0.04397099340598686\n",
      "Gradient Descent(5006/9999): loss=0.043970987800990013\n",
      "Gradient Descent(5007/9999): loss=0.043970982204806094\n",
      "Gradient Descent(5008/9999): loss=0.043970976617421255\n",
      "Gradient Descent(5009/9999): loss=0.04397097103882163\n",
      "Gradient Descent(5010/9999): loss=0.043970965468993414\n",
      "Gradient Descent(5011/9999): loss=0.04397095990792283\n",
      "Gradient Descent(5012/9999): loss=0.04397095435559611\n",
      "Gradient Descent(5013/9999): loss=0.04397094881199948\n",
      "Gradient Descent(5014/9999): loss=0.04397094327711927\n",
      "Gradient Descent(5015/9999): loss=0.0439709377509417\n",
      "Gradient Descent(5016/9999): loss=0.04397093223345313\n",
      "Gradient Descent(5017/9999): loss=0.04397092672463989\n",
      "Gradient Descent(5018/9999): loss=0.04397092122448834\n",
      "Gradient Descent(5019/9999): loss=0.043970915732984846\n",
      "Gradient Descent(5020/9999): loss=0.04397091025011583\n",
      "Gradient Descent(5021/9999): loss=0.04397090477586774\n",
      "Gradient Descent(5022/9999): loss=0.04397089931022695\n",
      "Gradient Descent(5023/9999): loss=0.04397089385318\n",
      "Gradient Descent(5024/9999): loss=0.04397088840471333\n",
      "Gradient Descent(5025/9999): loss=0.04397088296481345\n",
      "Gradient Descent(5026/9999): loss=0.043970877533466936\n",
      "Gradient Descent(5027/9999): loss=0.043970872110660295\n",
      "Gradient Descent(5028/9999): loss=0.043970866696380125\n",
      "Gradient Descent(5029/9999): loss=0.043970861290612964\n",
      "Gradient Descent(5030/9999): loss=0.043970855893345526\n",
      "Gradient Descent(5031/9999): loss=0.04397085050456434\n",
      "Gradient Descent(5032/9999): loss=0.043970845124256155\n",
      "Gradient Descent(5033/9999): loss=0.04397083975240757\n",
      "Gradient Descent(5034/9999): loss=0.04397083438900535\n",
      "Gradient Descent(5035/9999): loss=0.04397082903403617\n",
      "Gradient Descent(5036/9999): loss=0.04397082368748677\n",
      "Gradient Descent(5037/9999): loss=0.043970818349343965\n",
      "Gradient Descent(5038/9999): loss=0.04397081301959447\n",
      "Gradient Descent(5039/9999): loss=0.0439708076982251\n",
      "Gradient Descent(5040/9999): loss=0.04397080238522273\n",
      "Gradient Descent(5041/9999): loss=0.04397079708057418\n",
      "Gradient Descent(5042/9999): loss=0.04397079178426628\n",
      "Gradient Descent(5043/9999): loss=0.04397078649628591\n",
      "Gradient Descent(5044/9999): loss=0.043970781216620056\n",
      "Gradient Descent(5045/9999): loss=0.04397077594525557\n",
      "Gradient Descent(5046/9999): loss=0.043970770682179444\n",
      "Gradient Descent(5047/9999): loss=0.043970765427378614\n",
      "Gradient Descent(5048/9999): loss=0.04397076018084006\n",
      "Gradient Descent(5049/9999): loss=0.04397075494255085\n",
      "Gradient Descent(5050/9999): loss=0.04397074971249795\n",
      "Gradient Descent(5051/9999): loss=0.043970744490668456\n",
      "Gradient Descent(5052/9999): loss=0.04397073927704941\n",
      "Gradient Descent(5053/9999): loss=0.04397073407162789\n",
      "Gradient Descent(5054/9999): loss=0.043970728874391046\n",
      "Gradient Descent(5055/9999): loss=0.04397072368532601\n",
      "Gradient Descent(5056/9999): loss=0.04397071850441987\n",
      "Gradient Descent(5057/9999): loss=0.043970713331659875\n",
      "Gradient Descent(5058/9999): loss=0.04397070816703316\n",
      "Gradient Descent(5059/9999): loss=0.04397070301052696\n",
      "Gradient Descent(5060/9999): loss=0.043970697862128504\n",
      "Gradient Descent(5061/9999): loss=0.04397069272182504\n",
      "Gradient Descent(5062/9999): loss=0.04397068758960386\n",
      "Gradient Descent(5063/9999): loss=0.04397068246545224\n",
      "Gradient Descent(5064/9999): loss=0.04397067734935749\n",
      "Gradient Descent(5065/9999): loss=0.04397067224130693\n",
      "Gradient Descent(5066/9999): loss=0.04397066714128796\n",
      "Gradient Descent(5067/9999): loss=0.04397066204928788\n",
      "Gradient Descent(5068/9999): loss=0.043970656965294175\n",
      "Gradient Descent(5069/9999): loss=0.043970651889294145\n",
      "Gradient Descent(5070/9999): loss=0.04397064682127531\n",
      "Gradient Descent(5071/9999): loss=0.043970641761225074\n",
      "Gradient Descent(5072/9999): loss=0.04397063670913092\n",
      "Gradient Descent(5073/9999): loss=0.04397063166498035\n",
      "Gradient Descent(5074/9999): loss=0.043970626628760875\n",
      "Gradient Descent(5075/9999): loss=0.043970621600459996\n",
      "Gradient Descent(5076/9999): loss=0.04397061658006527\n",
      "Gradient Descent(5077/9999): loss=0.043970611567564315\n",
      "Gradient Descent(5078/9999): loss=0.04397060656294466\n",
      "Gradient Descent(5079/9999): loss=0.043970601566193916\n",
      "Gradient Descent(5080/9999): loss=0.04397059657729976\n",
      "Gradient Descent(5081/9999): loss=0.0439705915962498\n",
      "Gradient Descent(5082/9999): loss=0.04397058662303168\n",
      "Gradient Descent(5083/9999): loss=0.043970581657633144\n",
      "Gradient Descent(5084/9999): loss=0.043970576700041876\n",
      "Gradient Descent(5085/9999): loss=0.043970571750245574\n",
      "Gradient Descent(5086/9999): loss=0.043970566808232005\n",
      "Gradient Descent(5087/9999): loss=0.04397056187398892\n",
      "Gradient Descent(5088/9999): loss=0.04397055694750412\n",
      "Gradient Descent(5089/9999): loss=0.04397055202876539\n",
      "Gradient Descent(5090/9999): loss=0.043970547117760564\n",
      "Gradient Descent(5091/9999): loss=0.04397054221447745\n",
      "Gradient Descent(5092/9999): loss=0.04397053731890397\n",
      "Gradient Descent(5093/9999): loss=0.04397053243102794\n",
      "Gradient Descent(5094/9999): loss=0.04397052755083727\n",
      "Gradient Descent(5095/9999): loss=0.043970522678319884\n",
      "Gradient Descent(5096/9999): loss=0.043970517813463735\n",
      "Gradient Descent(5097/9999): loss=0.04397051295625675\n",
      "Gradient Descent(5098/9999): loss=0.04397050810668693\n",
      "Gradient Descent(5099/9999): loss=0.04397050326474222\n",
      "Gradient Descent(5100/9999): loss=0.04397049843041067\n",
      "Gradient Descent(5101/9999): loss=0.043970493603680307\n",
      "Gradient Descent(5102/9999): loss=0.043970488784539186\n",
      "Gradient Descent(5103/9999): loss=0.04397048397297534\n",
      "Gradient Descent(5104/9999): loss=0.04397047916897685\n",
      "Gradient Descent(5105/9999): loss=0.04397047437253189\n",
      "Gradient Descent(5106/9999): loss=0.043970469583628526\n",
      "Gradient Descent(5107/9999): loss=0.04397046480225491\n",
      "Gradient Descent(5108/9999): loss=0.04397046002839919\n",
      "Gradient Descent(5109/9999): loss=0.043970455262049575\n",
      "Gradient Descent(5110/9999): loss=0.043970450503194265\n",
      "Gradient Descent(5111/9999): loss=0.043970445751821456\n",
      "Gradient Descent(5112/9999): loss=0.043970441007919366\n",
      "Gradient Descent(5113/9999): loss=0.04397043627147628\n",
      "Gradient Descent(5114/9999): loss=0.043970431542480475\n",
      "Gradient Descent(5115/9999): loss=0.04397042682092021\n",
      "Gradient Descent(5116/9999): loss=0.04397042210678382\n",
      "Gradient Descent(5117/9999): loss=0.04397041740005961\n",
      "Gradient Descent(5118/9999): loss=0.04397041270073594\n",
      "Gradient Descent(5119/9999): loss=0.043970408008801184\n",
      "Gradient Descent(5120/9999): loss=0.04397040332424371\n",
      "Gradient Descent(5121/9999): loss=0.043970398647051906\n",
      "Gradient Descent(5122/9999): loss=0.04397039397721421\n",
      "Gradient Descent(5123/9999): loss=0.04397038931471906\n",
      "Gradient Descent(5124/9999): loss=0.04397038465955492\n",
      "Gradient Descent(5125/9999): loss=0.043970380011710226\n",
      "Gradient Descent(5126/9999): loss=0.0439703753711735\n",
      "Gradient Descent(5127/9999): loss=0.043970370737933244\n",
      "Gradient Descent(5128/9999): loss=0.04397036611197798\n",
      "Gradient Descent(5129/9999): loss=0.04397036149329626\n",
      "Gradient Descent(5130/9999): loss=0.04397035688187666\n",
      "Gradient Descent(5131/9999): loss=0.043970352277707736\n",
      "Gradient Descent(5132/9999): loss=0.043970347680778094\n",
      "Gradient Descent(5133/9999): loss=0.04397034309107635\n",
      "Gradient Descent(5134/9999): loss=0.04397033850859118\n",
      "Gradient Descent(5135/9999): loss=0.04397033393331117\n",
      "Gradient Descent(5136/9999): loss=0.043970329365225054\n",
      "Gradient Descent(5137/9999): loss=0.04397032480432146\n",
      "Gradient Descent(5138/9999): loss=0.04397032025058913\n",
      "Gradient Descent(5139/9999): loss=0.04397031570401679\n",
      "Gradient Descent(5140/9999): loss=0.04397031116459318\n",
      "Gradient Descent(5141/9999): loss=0.04397030663230707\n",
      "Gradient Descent(5142/9999): loss=0.0439703021071472\n",
      "Gradient Descent(5143/9999): loss=0.043970297589102376\n",
      "Gradient Descent(5144/9999): loss=0.04397029307816144\n",
      "Gradient Descent(5145/9999): loss=0.04397028857431321\n",
      "Gradient Descent(5146/9999): loss=0.04397028407754656\n",
      "Gradient Descent(5147/9999): loss=0.04397027958785027\n",
      "Gradient Descent(5148/9999): loss=0.04397027510521332\n",
      "Gradient Descent(5149/9999): loss=0.04397027062962453\n",
      "Gradient Descent(5150/9999): loss=0.043970266161072885\n",
      "Gradient Descent(5151/9999): loss=0.04397026169954729\n",
      "Gradient Descent(5152/9999): loss=0.04397025724503668\n",
      "Gradient Descent(5153/9999): loss=0.04397025279753006\n",
      "Gradient Descent(5154/9999): loss=0.043970248357016406\n",
      "Gradient Descent(5155/9999): loss=0.04397024392348471\n",
      "Gradient Descent(5156/9999): loss=0.043970239496924\n",
      "Gradient Descent(5157/9999): loss=0.04397023507732331\n",
      "Gradient Descent(5158/9999): loss=0.04397023066467172\n",
      "Gradient Descent(5159/9999): loss=0.04397022625895829\n",
      "Gradient Descent(5160/9999): loss=0.043970221860172104\n",
      "Gradient Descent(5161/9999): loss=0.04397021746830226\n",
      "Gradient Descent(5162/9999): loss=0.04397021308333793\n",
      "Gradient Descent(5163/9999): loss=0.0439702087052682\n",
      "Gradient Descent(5164/9999): loss=0.04397020433408226\n",
      "Gradient Descent(5165/9999): loss=0.04397019996976926\n",
      "Gradient Descent(5166/9999): loss=0.043970195612318436\n",
      "Gradient Descent(5167/9999): loss=0.04397019126171899\n",
      "Gradient Descent(5168/9999): loss=0.043970186917960126\n",
      "Gradient Descent(5169/9999): loss=0.04397018258103108\n",
      "Gradient Descent(5170/9999): loss=0.043970178250921144\n",
      "Gradient Descent(5171/9999): loss=0.04397017392761958\n",
      "Gradient Descent(5172/9999): loss=0.04397016961111568\n",
      "Gradient Descent(5173/9999): loss=0.04397016530139877\n",
      "Gradient Descent(5174/9999): loss=0.043970160998458185\n",
      "Gradient Descent(5175/9999): loss=0.043970156702283235\n",
      "Gradient Descent(5176/9999): loss=0.04397015241286329\n",
      "Gradient Descent(5177/9999): loss=0.04397014813018778\n",
      "Gradient Descent(5178/9999): loss=0.04397014385424604\n",
      "Gradient Descent(5179/9999): loss=0.0439701395850275\n",
      "Gradient Descent(5180/9999): loss=0.0439701353225216\n",
      "Gradient Descent(5181/9999): loss=0.043970131066717794\n",
      "Gradient Descent(5182/9999): loss=0.0439701268176055\n",
      "Gradient Descent(5183/9999): loss=0.04397012257517424\n",
      "Gradient Descent(5184/9999): loss=0.043970118339413486\n",
      "Gradient Descent(5185/9999): loss=0.043970114110312784\n",
      "Gradient Descent(5186/9999): loss=0.043970109887861604\n",
      "Gradient Descent(5187/9999): loss=0.04397010567204955\n",
      "Gradient Descent(5188/9999): loss=0.043970101462866173\n",
      "Gradient Descent(5189/9999): loss=0.043970097260300996\n",
      "Gradient Descent(5190/9999): loss=0.04397009306434366\n",
      "Gradient Descent(5191/9999): loss=0.043970088874983757\n",
      "Gradient Descent(5192/9999): loss=0.04397008469221095\n",
      "Gradient Descent(5193/9999): loss=0.04397008051601485\n",
      "Gradient Descent(5194/9999): loss=0.04397007634638511\n",
      "Gradient Descent(5195/9999): loss=0.04397007218331143\n",
      "Gradient Descent(5196/9999): loss=0.04397006802678345\n",
      "Gradient Descent(5197/9999): loss=0.04397006387679096\n",
      "Gradient Descent(5198/9999): loss=0.04397005973332363\n",
      "Gradient Descent(5199/9999): loss=0.043970055596371196\n",
      "Gradient Descent(5200/9999): loss=0.04397005146592345\n",
      "Gradient Descent(5201/9999): loss=0.043970047341970124\n",
      "Gradient Descent(5202/9999): loss=0.04397004322450103\n",
      "Gradient Descent(5203/9999): loss=0.04397003911350597\n",
      "Gradient Descent(5204/9999): loss=0.04397003500897478\n",
      "Gradient Descent(5205/9999): loss=0.043970030910897275\n",
      "Gradient Descent(5206/9999): loss=0.04397002681926329\n",
      "Gradient Descent(5207/9999): loss=0.04397002273406275\n",
      "Gradient Descent(5208/9999): loss=0.04397001865528548\n",
      "Gradient Descent(5209/9999): loss=0.043970014582921414\n",
      "Gradient Descent(5210/9999): loss=0.04397001051696045\n",
      "Gradient Descent(5211/9999): loss=0.04397000645739255\n",
      "Gradient Descent(5212/9999): loss=0.04397000240420765\n",
      "Gradient Descent(5213/9999): loss=0.04396999835739569\n",
      "Gradient Descent(5214/9999): loss=0.04396999431694664\n",
      "Gradient Descent(5215/9999): loss=0.04396999028285056\n",
      "Gradient Descent(5216/9999): loss=0.04396998625509743\n",
      "Gradient Descent(5217/9999): loss=0.04396998223367725\n",
      "Gradient Descent(5218/9999): loss=0.04396997821858007\n",
      "Gradient Descent(5219/9999): loss=0.04396997420979599\n",
      "Gradient Descent(5220/9999): loss=0.04396997020731504\n",
      "Gradient Descent(5221/9999): loss=0.043969966211127295\n",
      "Gradient Descent(5222/9999): loss=0.04396996222122294\n",
      "Gradient Descent(5223/9999): loss=0.04396995823759202\n",
      "Gradient Descent(5224/9999): loss=0.043969954260224674\n",
      "Gradient Descent(5225/9999): loss=0.043969950289111104\n",
      "Gradient Descent(5226/9999): loss=0.04396994632424142\n",
      "Gradient Descent(5227/9999): loss=0.04396994236560585\n",
      "Gradient Descent(5228/9999): loss=0.04396993841319459\n",
      "Gradient Descent(5229/9999): loss=0.043969934466997825\n",
      "Gradient Descent(5230/9999): loss=0.04396993052700578\n",
      "Gradient Descent(5231/9999): loss=0.04396992659320873\n",
      "Gradient Descent(5232/9999): loss=0.04396992266559692\n",
      "Gradient Descent(5233/9999): loss=0.043969918744160584\n",
      "Gradient Descent(5234/9999): loss=0.043969914828890115\n",
      "Gradient Descent(5235/9999): loss=0.04396991091977572\n",
      "Gradient Descent(5236/9999): loss=0.04396990701680775\n",
      "Gradient Descent(5237/9999): loss=0.04396990311997655\n",
      "Gradient Descent(5238/9999): loss=0.0439698992292725\n",
      "Gradient Descent(5239/9999): loss=0.04396989534468589\n",
      "Gradient Descent(5240/9999): loss=0.04396989146620715\n",
      "Gradient Descent(5241/9999): loss=0.04396988759382669\n",
      "Gradient Descent(5242/9999): loss=0.04396988372753489\n",
      "Gradient Descent(5243/9999): loss=0.0439698798673222\n",
      "Gradient Descent(5244/9999): loss=0.04396987601317903\n",
      "Gradient Descent(5245/9999): loss=0.04396987216509587\n",
      "Gradient Descent(5246/9999): loss=0.04396986832306316\n",
      "Gradient Descent(5247/9999): loss=0.04396986448707144\n",
      "Gradient Descent(5248/9999): loss=0.04396986065711115\n",
      "Gradient Descent(5249/9999): loss=0.043969856833172845\n",
      "Gradient Descent(5250/9999): loss=0.04396985301524703\n",
      "Gradient Descent(5251/9999): loss=0.043969849203324286\n",
      "Gradient Descent(5252/9999): loss=0.043969845397395145\n",
      "Gradient Descent(5253/9999): loss=0.043969841597450196\n",
      "Gradient Descent(5254/9999): loss=0.043969837803480016\n",
      "Gradient Descent(5255/9999): loss=0.04396983401547524\n",
      "Gradient Descent(5256/9999): loss=0.04396983023342646\n",
      "Gradient Descent(5257/9999): loss=0.043969826457324314\n",
      "Gradient Descent(5258/9999): loss=0.04396982268715946\n",
      "Gradient Descent(5259/9999): loss=0.04396981892292255\n",
      "Gradient Descent(5260/9999): loss=0.043969815164604284\n",
      "Gradient Descent(5261/9999): loss=0.04396981141219535\n",
      "Gradient Descent(5262/9999): loss=0.04396980766568646\n",
      "Gradient Descent(5263/9999): loss=0.04396980392506831\n",
      "Gradient Descent(5264/9999): loss=0.04396980019033169\n",
      "Gradient Descent(5265/9999): loss=0.0439697964614673\n",
      "Gradient Descent(5266/9999): loss=0.04396979273846591\n",
      "Gradient Descent(5267/9999): loss=0.04396978902131833\n",
      "Gradient Descent(5268/9999): loss=0.04396978531001533\n",
      "Gradient Descent(5269/9999): loss=0.043969781604547765\n",
      "Gradient Descent(5270/9999): loss=0.04396977790490644\n",
      "Gradient Descent(5271/9999): loss=0.04396977421108214\n",
      "Gradient Descent(5272/9999): loss=0.04396977052306579\n",
      "Gradient Descent(5273/9999): loss=0.04396976684084821\n",
      "Gradient Descent(5274/9999): loss=0.043969763164420334\n",
      "Gradient Descent(5275/9999): loss=0.043969759493773\n",
      "Gradient Descent(5276/9999): loss=0.04396975582889715\n",
      "Gradient Descent(5277/9999): loss=0.043969752169783716\n",
      "Gradient Descent(5278/9999): loss=0.043969748516423615\n",
      "Gradient Descent(5279/9999): loss=0.04396974486880783\n",
      "Gradient Descent(5280/9999): loss=0.04396974122692731\n",
      "Gradient Descent(5281/9999): loss=0.043969737590773016\n",
      "Gradient Descent(5282/9999): loss=0.04396973396033597\n",
      "Gradient Descent(5283/9999): loss=0.04396973033560721\n",
      "Gradient Descent(5284/9999): loss=0.04396972671657771\n",
      "Gradient Descent(5285/9999): loss=0.04396972310323852\n",
      "Gradient Descent(5286/9999): loss=0.04396971949558072\n",
      "Gradient Descent(5287/9999): loss=0.04396971589359535\n",
      "Gradient Descent(5288/9999): loss=0.04396971229727351\n",
      "Gradient Descent(5289/9999): loss=0.04396970870660628\n",
      "Gradient Descent(5290/9999): loss=0.04396970512158479\n",
      "Gradient Descent(5291/9999): loss=0.043969701542200125\n",
      "Gradient Descent(5292/9999): loss=0.04396969796844345\n",
      "Gradient Descent(5293/9999): loss=0.04396969440030589\n",
      "Gradient Descent(5294/9999): loss=0.043969690837778685\n",
      "Gradient Descent(5295/9999): loss=0.04396968728085291\n",
      "Gradient Descent(5296/9999): loss=0.04396968372951985\n",
      "Gradient Descent(5297/9999): loss=0.04396968018377063\n",
      "Gradient Descent(5298/9999): loss=0.04396967664359654\n",
      "Gradient Descent(5299/9999): loss=0.04396967310898876\n",
      "Gradient Descent(5300/9999): loss=0.043969669579938565\n",
      "Gradient Descent(5301/9999): loss=0.04396966605643723\n",
      "Gradient Descent(5302/9999): loss=0.043969662538475986\n",
      "Gradient Descent(5303/9999): loss=0.043969659026046166\n",
      "Gradient Descent(5304/9999): loss=0.043969655519139034\n",
      "Gradient Descent(5305/9999): loss=0.04396965201774596\n",
      "Gradient Descent(5306/9999): loss=0.04396964852185823\n",
      "Gradient Descent(5307/9999): loss=0.04396964503146719\n",
      "Gradient Descent(5308/9999): loss=0.0439696415465642\n",
      "Gradient Descent(5309/9999): loss=0.04396963806714067\n",
      "Gradient Descent(5310/9999): loss=0.04396963459318795\n",
      "Gradient Descent(5311/9999): loss=0.043969631124697414\n",
      "Gradient Descent(5312/9999): loss=0.043969627661660515\n",
      "Gradient Descent(5313/9999): loss=0.043969624204068664\n",
      "Gradient Descent(5314/9999): loss=0.043969620751913296\n",
      "Gradient Descent(5315/9999): loss=0.043969617305185865\n",
      "Gradient Descent(5316/9999): loss=0.04396961386387782\n",
      "Gradient Descent(5317/9999): loss=0.043969610427980685\n",
      "Gradient Descent(5318/9999): loss=0.04396960699748591\n",
      "Gradient Descent(5319/9999): loss=0.043969603572385024\n",
      "Gradient Descent(5320/9999): loss=0.043969600152669555\n",
      "Gradient Descent(5321/9999): loss=0.04396959673833098\n",
      "Gradient Descent(5322/9999): loss=0.0439695933293609\n",
      "Gradient Descent(5323/9999): loss=0.04396958992575086\n",
      "Gradient Descent(5324/9999): loss=0.04396958652749243\n",
      "Gradient Descent(5325/9999): loss=0.04396958313457719\n",
      "Gradient Descent(5326/9999): loss=0.043969579746996716\n",
      "Gradient Descent(5327/9999): loss=0.043969576364742684\n",
      "Gradient Descent(5328/9999): loss=0.04396957298780668\n",
      "Gradient Descent(5329/9999): loss=0.04396956961618034\n",
      "Gradient Descent(5330/9999): loss=0.04396956624985531\n",
      "Gradient Descent(5331/9999): loss=0.043969562888823255\n",
      "Gradient Descent(5332/9999): loss=0.04396955953307589\n",
      "Gradient Descent(5333/9999): loss=0.04396955618260487\n",
      "Gradient Descent(5334/9999): loss=0.04396955283740188\n",
      "Gradient Descent(5335/9999): loss=0.043969549497458696\n",
      "Gradient Descent(5336/9999): loss=0.043969546162767\n",
      "Gradient Descent(5337/9999): loss=0.04396954283331856\n",
      "Gradient Descent(5338/9999): loss=0.043969539509105114\n",
      "Gradient Descent(5339/9999): loss=0.043969536190118425\n",
      "Gradient Descent(5340/9999): loss=0.0439695328763503\n",
      "Gradient Descent(5341/9999): loss=0.04396952956779252\n",
      "Gradient Descent(5342/9999): loss=0.043969526264436905\n",
      "Gradient Descent(5343/9999): loss=0.043969522966275265\n",
      "Gradient Descent(5344/9999): loss=0.04396951967329943\n",
      "Gradient Descent(5345/9999): loss=0.043969516385501235\n",
      "Gradient Descent(5346/9999): loss=0.04396951310287258\n",
      "Gradient Descent(5347/9999): loss=0.04396950982540529\n",
      "Gradient Descent(5348/9999): loss=0.04396950655309128\n",
      "Gradient Descent(5349/9999): loss=0.04396950328592244\n",
      "Gradient Descent(5350/9999): loss=0.04396950002389069\n",
      "Gradient Descent(5351/9999): loss=0.04396949676698791\n",
      "Gradient Descent(5352/9999): loss=0.04396949351520608\n",
      "Gradient Descent(5353/9999): loss=0.043969490268537144\n",
      "Gradient Descent(5354/9999): loss=0.04396948702697303\n",
      "Gradient Descent(5355/9999): loss=0.04396948379050575\n",
      "Gradient Descent(5356/9999): loss=0.04396948055912727\n",
      "Gradient Descent(5357/9999): loss=0.043969477332829604\n",
      "Gradient Descent(5358/9999): loss=0.04396947411160473\n",
      "Gradient Descent(5359/9999): loss=0.0439694708954447\n",
      "Gradient Descent(5360/9999): loss=0.043969467684341554\n",
      "Gradient Descent(5361/9999): loss=0.04396946447828735\n",
      "Gradient Descent(5362/9999): loss=0.04396946127727409\n",
      "Gradient Descent(5363/9999): loss=0.043969458081293926\n",
      "Gradient Descent(5364/9999): loss=0.0439694548903389\n",
      "Gradient Descent(5365/9999): loss=0.04396945170440111\n",
      "Gradient Descent(5366/9999): loss=0.04396944852347266\n",
      "Gradient Descent(5367/9999): loss=0.04396944534754571\n",
      "Gradient Descent(5368/9999): loss=0.04396944217661237\n",
      "Gradient Descent(5369/9999): loss=0.0439694390106648\n",
      "Gradient Descent(5370/9999): loss=0.04396943584969512\n",
      "Gradient Descent(5371/9999): loss=0.04396943269369559\n",
      "Gradient Descent(5372/9999): loss=0.04396942954265832\n",
      "Gradient Descent(5373/9999): loss=0.04396942639657551\n",
      "Gradient Descent(5374/9999): loss=0.04396942325543941\n",
      "Gradient Descent(5375/9999): loss=0.043969420119242215\n",
      "Gradient Descent(5376/9999): loss=0.04396941698797618\n",
      "Gradient Descent(5377/9999): loss=0.04396941386163355\n",
      "Gradient Descent(5378/9999): loss=0.04396941074020653\n",
      "Gradient Descent(5379/9999): loss=0.04396940762368746\n",
      "Gradient Descent(5380/9999): loss=0.043969404512068586\n",
      "Gradient Descent(5381/9999): loss=0.04396940140534223\n",
      "Gradient Descent(5382/9999): loss=0.04396939830350065\n",
      "Gradient Descent(5383/9999): loss=0.04396939520653622\n",
      "Gradient Descent(5384/9999): loss=0.043969392114441244\n",
      "Gradient Descent(5385/9999): loss=0.04396938902720808\n",
      "Gradient Descent(5386/9999): loss=0.04396938594482907\n",
      "Gradient Descent(5387/9999): loss=0.0439693828672966\n",
      "Gradient Descent(5388/9999): loss=0.043969379794603004\n",
      "Gradient Descent(5389/9999): loss=0.043969376726740714\n",
      "Gradient Descent(5390/9999): loss=0.04396937366370215\n",
      "Gradient Descent(5391/9999): loss=0.04396937060547969\n",
      "Gradient Descent(5392/9999): loss=0.043969367552065756\n",
      "Gradient Descent(5393/9999): loss=0.04396936450345284\n",
      "Gradient Descent(5394/9999): loss=0.043969361459633344\n",
      "Gradient Descent(5395/9999): loss=0.04396935842059973\n",
      "Gradient Descent(5396/9999): loss=0.0439693553863445\n",
      "Gradient Descent(5397/9999): loss=0.04396935235686013\n",
      "Gradient Descent(5398/9999): loss=0.043969349332139133\n",
      "Gradient Descent(5399/9999): loss=0.043969346312173994\n",
      "Gradient Descent(5400/9999): loss=0.04396934329695726\n",
      "Gradient Descent(5401/9999): loss=0.043969340286481404\n",
      "Gradient Descent(5402/9999): loss=0.04396933728073906\n",
      "Gradient Descent(5403/9999): loss=0.043969334279722744\n",
      "Gradient Descent(5404/9999): loss=0.04396933128342502\n",
      "Gradient Descent(5405/9999): loss=0.04396932829183846\n",
      "Gradient Descent(5406/9999): loss=0.04396932530495569\n",
      "Gradient Descent(5407/9999): loss=0.04396932232276926\n",
      "Gradient Descent(5408/9999): loss=0.04396931934527186\n",
      "Gradient Descent(5409/9999): loss=0.043969316372456074\n",
      "Gradient Descent(5410/9999): loss=0.04396931340431453\n",
      "Gradient Descent(5411/9999): loss=0.043969310440839904\n",
      "Gradient Descent(5412/9999): loss=0.04396930748202484\n",
      "Gradient Descent(5413/9999): loss=0.04396930452786202\n",
      "Gradient Descent(5414/9999): loss=0.04396930157834412\n",
      "Gradient Descent(5415/9999): loss=0.04396929863346385\n",
      "Gradient Descent(5416/9999): loss=0.04396929569321391\n",
      "Gradient Descent(5417/9999): loss=0.04396929275758702\n",
      "Gradient Descent(5418/9999): loss=0.04396928982657593\n",
      "Gradient Descent(5419/9999): loss=0.04396928690017336\n",
      "Gradient Descent(5420/9999): loss=0.043969283978372055\n",
      "Gradient Descent(5421/9999): loss=0.04396928106116483\n",
      "Gradient Descent(5422/9999): loss=0.04396927814854438\n",
      "Gradient Descent(5423/9999): loss=0.04396927524050356\n",
      "Gradient Descent(5424/9999): loss=0.04396927233703514\n",
      "Gradient Descent(5425/9999): loss=0.04396926943813195\n",
      "Gradient Descent(5426/9999): loss=0.043969266543786814\n",
      "Gradient Descent(5427/9999): loss=0.04396926365399252\n",
      "Gradient Descent(5428/9999): loss=0.043969260768741955\n",
      "Gradient Descent(5429/9999): loss=0.04396925788802799\n",
      "Gradient Descent(5430/9999): loss=0.04396925501184344\n",
      "Gradient Descent(5431/9999): loss=0.043969252140181235\n",
      "Gradient Descent(5432/9999): loss=0.043969249273034236\n",
      "Gradient Descent(5433/9999): loss=0.04396924641039531\n",
      "Gradient Descent(5434/9999): loss=0.04396924355225747\n",
      "Gradient Descent(5435/9999): loss=0.04396924069861353\n",
      "Gradient Descent(5436/9999): loss=0.043969237849456465\n",
      "Gradient Descent(5437/9999): loss=0.043969235004779235\n",
      "Gradient Descent(5438/9999): loss=0.04396923216457479\n",
      "Gradient Descent(5439/9999): loss=0.04396922932883609\n",
      "Gradient Descent(5440/9999): loss=0.04396922649755611\n",
      "Gradient Descent(5441/9999): loss=0.04396922367072783\n",
      "Gradient Descent(5442/9999): loss=0.043969220848344294\n",
      "Gradient Descent(5443/9999): loss=0.04396921803039848\n",
      "Gradient Descent(5444/9999): loss=0.04396921521688341\n",
      "Gradient Descent(5445/9999): loss=0.043969212407792106\n",
      "Gradient Descent(5446/9999): loss=0.04396920960311767\n",
      "Gradient Descent(5447/9999): loss=0.04396920680285308\n",
      "Gradient Descent(5448/9999): loss=0.043969204006991454\n",
      "Gradient Descent(5449/9999): loss=0.043969201215525854\n",
      "Gradient Descent(5450/9999): loss=0.04396919842844935\n",
      "Gradient Descent(5451/9999): loss=0.0439691956457551\n",
      "Gradient Descent(5452/9999): loss=0.043969192867436134\n",
      "Gradient Descent(5453/9999): loss=0.04396919009348563\n",
      "Gradient Descent(5454/9999): loss=0.04396918732389667\n",
      "Gradient Descent(5455/9999): loss=0.043969184558662444\n",
      "Gradient Descent(5456/9999): loss=0.0439691817977761\n",
      "Gradient Descent(5457/9999): loss=0.04396917904123078\n",
      "Gradient Descent(5458/9999): loss=0.04396917628901968\n",
      "Gradient Descent(5459/9999): loss=0.04396917354113596\n",
      "Gradient Descent(5460/9999): loss=0.043969170797572815\n",
      "Gradient Descent(5461/9999): loss=0.0439691680583235\n",
      "Gradient Descent(5462/9999): loss=0.04396916532338117\n",
      "Gradient Descent(5463/9999): loss=0.04396916259273909\n",
      "Gradient Descent(5464/9999): loss=0.043969159866390474\n",
      "Gradient Descent(5465/9999): loss=0.043969157144328606\n",
      "Gradient Descent(5466/9999): loss=0.043969154426546704\n",
      "Gradient Descent(5467/9999): loss=0.0439691517130381\n",
      "Gradient Descent(5468/9999): loss=0.04396914900379599\n",
      "Gradient Descent(5469/9999): loss=0.04396914629881374\n",
      "Gradient Descent(5470/9999): loss=0.043969143598084626\n",
      "Gradient Descent(5471/9999): loss=0.043969140901601936\n",
      "Gradient Descent(5472/9999): loss=0.04396913820935902\n",
      "Gradient Descent(5473/9999): loss=0.043969135521349206\n",
      "Gradient Descent(5474/9999): loss=0.04396913283756585\n",
      "Gradient Descent(5475/9999): loss=0.043969130158002294\n",
      "Gradient Descent(5476/9999): loss=0.04396912748265189\n",
      "Gradient Descent(5477/9999): loss=0.04396912481150804\n",
      "Gradient Descent(5478/9999): loss=0.04396912214456411\n",
      "Gradient Descent(5479/9999): loss=0.04396911948181352\n",
      "Gradient Descent(5480/9999): loss=0.043969116823249645\n",
      "Gradient Descent(5481/9999): loss=0.04396911416886593\n",
      "Gradient Descent(5482/9999): loss=0.04396911151865579\n",
      "Gradient Descent(5483/9999): loss=0.04396910887261265\n",
      "Gradient Descent(5484/9999): loss=0.04396910623072998\n",
      "Gradient Descent(5485/9999): loss=0.043969103593001234\n",
      "Gradient Descent(5486/9999): loss=0.04396910095941986\n",
      "Gradient Descent(5487/9999): loss=0.04396909832997938\n",
      "Gradient Descent(5488/9999): loss=0.04396909570467322\n",
      "Gradient Descent(5489/9999): loss=0.043969093083494934\n",
      "Gradient Descent(5490/9999): loss=0.04396909046643801\n",
      "Gradient Descent(5491/9999): loss=0.043969087853495965\n",
      "Gradient Descent(5492/9999): loss=0.04396908524466234\n",
      "Gradient Descent(5493/9999): loss=0.0439690826399307\n",
      "Gradient Descent(5494/9999): loss=0.04396908003929454\n",
      "Gradient Descent(5495/9999): loss=0.04396907744274744\n",
      "Gradient Descent(5496/9999): loss=0.04396907485028299\n",
      "Gradient Descent(5497/9999): loss=0.04396907226189475\n",
      "Gradient Descent(5498/9999): loss=0.04396906967757632\n",
      "Gradient Descent(5499/9999): loss=0.043969067097321304\n",
      "Gradient Descent(5500/9999): loss=0.04396906452112331\n",
      "Gradient Descent(5501/9999): loss=0.043969061948975964\n",
      "Gradient Descent(5502/9999): loss=0.043969059380872866\n",
      "Gradient Descent(5503/9999): loss=0.0439690568168077\n",
      "Gradient Descent(5504/9999): loss=0.04396905425677408\n",
      "Gradient Descent(5505/9999): loss=0.04396905170076569\n",
      "Gradient Descent(5506/9999): loss=0.0439690491487762\n",
      "Gradient Descent(5507/9999): loss=0.043969046600799304\n",
      "Gradient Descent(5508/9999): loss=0.04396904405682868\n",
      "Gradient Descent(5509/9999): loss=0.04396904151685798\n",
      "Gradient Descent(5510/9999): loss=0.04396903898088099\n",
      "Gradient Descent(5511/9999): loss=0.04396903644889142\n",
      "Gradient Descent(5512/9999): loss=0.04396903392088295\n",
      "Gradient Descent(5513/9999): loss=0.043969031396849365\n",
      "Gradient Descent(5514/9999): loss=0.04396902887678441\n",
      "Gradient Descent(5515/9999): loss=0.04396902636068181\n",
      "Gradient Descent(5516/9999): loss=0.04396902384853536\n",
      "Gradient Descent(5517/9999): loss=0.04396902134033888\n",
      "Gradient Descent(5518/9999): loss=0.0439690188360861\n",
      "Gradient Descent(5519/9999): loss=0.04396901633577083\n",
      "Gradient Descent(5520/9999): loss=0.043969013839386915\n",
      "Gradient Descent(5521/9999): loss=0.04396901134692812\n",
      "Gradient Descent(5522/9999): loss=0.0439690088583883\n",
      "Gradient Descent(5523/9999): loss=0.043969006373761305\n",
      "Gradient Descent(5524/9999): loss=0.04396900389304096\n",
      "Gradient Descent(5525/9999): loss=0.04396900141622115\n",
      "Gradient Descent(5526/9999): loss=0.04396899894329571\n",
      "Gradient Descent(5527/9999): loss=0.04396899647425853\n",
      "Gradient Descent(5528/9999): loss=0.04396899400910352\n",
      "Gradient Descent(5529/9999): loss=0.043968991547824555\n",
      "Gradient Descent(5530/9999): loss=0.043968989090415515\n",
      "Gradient Descent(5531/9999): loss=0.04396898663687034\n",
      "Gradient Descent(5532/9999): loss=0.04396898418718299\n",
      "Gradient Descent(5533/9999): loss=0.043968981741347356\n",
      "Gradient Descent(5534/9999): loss=0.04396897929935736\n",
      "Gradient Descent(5535/9999): loss=0.043968976861207015\n",
      "Gradient Descent(5536/9999): loss=0.04396897442689023\n",
      "Gradient Descent(5537/9999): loss=0.04396897199640102\n",
      "Gradient Descent(5538/9999): loss=0.04396896956973334\n",
      "Gradient Descent(5539/9999): loss=0.04396896714688121\n",
      "Gradient Descent(5540/9999): loss=0.04396896472783858\n",
      "Gradient Descent(5541/9999): loss=0.043968962312599516\n",
      "Gradient Descent(5542/9999): loss=0.043968959901158\n",
      "Gradient Descent(5543/9999): loss=0.04396895749350807\n",
      "Gradient Descent(5544/9999): loss=0.043968955089643755\n",
      "Gradient Descent(5545/9999): loss=0.043968952689559165\n",
      "Gradient Descent(5546/9999): loss=0.04396895029324825\n",
      "Gradient Descent(5547/9999): loss=0.043968947900705165\n",
      "Gradient Descent(5548/9999): loss=0.043968945511923936\n",
      "Gradient Descent(5549/9999): loss=0.04396894312689869\n",
      "Gradient Descent(5550/9999): loss=0.04396894074562349\n",
      "Gradient Descent(5551/9999): loss=0.04396893836809245\n",
      "Gradient Descent(5552/9999): loss=0.04396893599429965\n",
      "Gradient Descent(5553/9999): loss=0.04396893362423928\n",
      "Gradient Descent(5554/9999): loss=0.04396893125790543\n",
      "Gradient Descent(5555/9999): loss=0.04396892889529224\n",
      "Gradient Descent(5556/9999): loss=0.043968926536393865\n",
      "Gradient Descent(5557/9999): loss=0.043968924181204454\n",
      "Gradient Descent(5558/9999): loss=0.043968921829718205\n",
      "Gradient Descent(5559/9999): loss=0.04396891948192924\n",
      "Gradient Descent(5560/9999): loss=0.04396891713783181\n",
      "Gradient Descent(5561/9999): loss=0.043968914797420086\n",
      "Gradient Descent(5562/9999): loss=0.04396891246068825\n",
      "Gradient Descent(5563/9999): loss=0.04396891012763056\n",
      "Gradient Descent(5564/9999): loss=0.04396890779824118\n",
      "Gradient Descent(5565/9999): loss=0.0439689054725144\n",
      "Gradient Descent(5566/9999): loss=0.04396890315044443\n",
      "Gradient Descent(5567/9999): loss=0.04396890083202553\n",
      "Gradient Descent(5568/9999): loss=0.04396889851725198\n",
      "Gradient Descent(5569/9999): loss=0.04396889620611797\n",
      "Gradient Descent(5570/9999): loss=0.04396889389861786\n",
      "Gradient Descent(5571/9999): loss=0.04396889159474592\n",
      "Gradient Descent(5572/9999): loss=0.04396888929449643\n",
      "Gradient Descent(5573/9999): loss=0.04396888699786369\n",
      "Gradient Descent(5574/9999): loss=0.04396888470484203\n",
      "Gradient Descent(5575/9999): loss=0.04396888241542576\n",
      "Gradient Descent(5576/9999): loss=0.0439688801296092\n",
      "Gradient Descent(5577/9999): loss=0.04396887784738672\n",
      "Gradient Descent(5578/9999): loss=0.043968875568752654\n",
      "Gradient Descent(5579/9999): loss=0.04396887329370136\n",
      "Gradient Descent(5580/9999): loss=0.04396887102222722\n",
      "Gradient Descent(5581/9999): loss=0.043968868754324576\n",
      "Gradient Descent(5582/9999): loss=0.04396886648998782\n",
      "Gradient Descent(5583/9999): loss=0.043968864229211375\n",
      "Gradient Descent(5584/9999): loss=0.043968861971989626\n",
      "Gradient Descent(5585/9999): loss=0.04396885971831696\n",
      "Gradient Descent(5586/9999): loss=0.043968857468187836\n",
      "Gradient Descent(5587/9999): loss=0.04396885522159666\n",
      "Gradient Descent(5588/9999): loss=0.04396885297853787\n",
      "Gradient Descent(5589/9999): loss=0.043968850739005916\n",
      "Gradient Descent(5590/9999): loss=0.04396884850299524\n",
      "Gradient Descent(5591/9999): loss=0.04396884627050033\n",
      "Gradient Descent(5592/9999): loss=0.043968844041515666\n",
      "Gradient Descent(5593/9999): loss=0.04396884181603568\n",
      "Gradient Descent(5594/9999): loss=0.0439688395940549\n",
      "Gradient Descent(5595/9999): loss=0.04396883737556779\n",
      "Gradient Descent(5596/9999): loss=0.043968835160568924\n",
      "Gradient Descent(5597/9999): loss=0.04396883294905276\n",
      "Gradient Descent(5598/9999): loss=0.04396883074101382\n",
      "Gradient Descent(5599/9999): loss=0.04396882853644665\n",
      "Gradient Descent(5600/9999): loss=0.0439688263353458\n",
      "Gradient Descent(5601/9999): loss=0.043968824137705836\n",
      "Gradient Descent(5602/9999): loss=0.04396882194352128\n",
      "Gradient Descent(5603/9999): loss=0.043968819752786714\n",
      "Gradient Descent(5604/9999): loss=0.0439688175654967\n",
      "Gradient Descent(5605/9999): loss=0.043968815381645855\n",
      "Gradient Descent(5606/9999): loss=0.043968813201228724\n",
      "Gradient Descent(5607/9999): loss=0.04396881102423996\n",
      "Gradient Descent(5608/9999): loss=0.04396880885067414\n",
      "Gradient Descent(5609/9999): loss=0.043968806680525885\n",
      "Gradient Descent(5610/9999): loss=0.043968804513789816\n",
      "Gradient Descent(5611/9999): loss=0.043968802350460606\n",
      "Gradient Descent(5612/9999): loss=0.04396880019053285\n",
      "Gradient Descent(5613/9999): loss=0.043968798034001215\n",
      "Gradient Descent(5614/9999): loss=0.04396879588086037\n",
      "Gradient Descent(5615/9999): loss=0.043968793731105\n",
      "Gradient Descent(5616/9999): loss=0.04396879158472974\n",
      "Gradient Descent(5617/9999): loss=0.04396878944172931\n",
      "Gradient Descent(5618/9999): loss=0.04396878730209835\n",
      "Gradient Descent(5619/9999): loss=0.043968785165831636\n",
      "Gradient Descent(5620/9999): loss=0.04396878303292387\n",
      "Gradient Descent(5621/9999): loss=0.04396878090336972\n",
      "Gradient Descent(5622/9999): loss=0.04396877877716393\n",
      "Gradient Descent(5623/9999): loss=0.04396877665430124\n",
      "Gradient Descent(5624/9999): loss=0.04396877453477643\n",
      "Gradient Descent(5625/9999): loss=0.043968772418584205\n",
      "Gradient Descent(5626/9999): loss=0.043968770305719315\n",
      "Gradient Descent(5627/9999): loss=0.04396876819617656\n",
      "Gradient Descent(5628/9999): loss=0.04396876608995073\n",
      "Gradient Descent(5629/9999): loss=0.04396876398703659\n",
      "Gradient Descent(5630/9999): loss=0.04396876188742891\n",
      "Gradient Descent(5631/9999): loss=0.04396875979112255\n",
      "Gradient Descent(5632/9999): loss=0.043968757698112225\n",
      "Gradient Descent(5633/9999): loss=0.04396875560839284\n",
      "Gradient Descent(5634/9999): loss=0.0439687535219592\n",
      "Gradient Descent(5635/9999): loss=0.04396875143880611\n",
      "Gradient Descent(5636/9999): loss=0.04396874935892844\n",
      "Gradient Descent(5637/9999): loss=0.043968747282321016\n",
      "Gradient Descent(5638/9999): loss=0.04396874520897871\n",
      "Gradient Descent(5639/9999): loss=0.043968743138896406\n",
      "Gradient Descent(5640/9999): loss=0.04396874107206896\n",
      "Gradient Descent(5641/9999): loss=0.04396873900849124\n",
      "Gradient Descent(5642/9999): loss=0.043968736948158164\n",
      "Gradient Descent(5643/9999): loss=0.04396873489106462\n",
      "Gradient Descent(5644/9999): loss=0.0439687328372055\n",
      "Gradient Descent(5645/9999): loss=0.04396873078657573\n",
      "Gradient Descent(5646/9999): loss=0.04396872873917026\n",
      "Gradient Descent(5647/9999): loss=0.04396872669498398\n",
      "Gradient Descent(5648/9999): loss=0.043968724654011825\n",
      "Gradient Descent(5649/9999): loss=0.04396872261624876\n",
      "Gradient Descent(5650/9999): loss=0.043968720581689755\n",
      "Gradient Descent(5651/9999): loss=0.04396871855032975\n",
      "Gradient Descent(5652/9999): loss=0.043968716522163734\n",
      "Gradient Descent(5653/9999): loss=0.04396871449718663\n",
      "Gradient Descent(5654/9999): loss=0.04396871247539351\n",
      "Gradient Descent(5655/9999): loss=0.043968710456779275\n",
      "Gradient Descent(5656/9999): loss=0.043968708441339\n",
      "Gradient Descent(5657/9999): loss=0.04396870642906767\n",
      "Gradient Descent(5658/9999): loss=0.0439687044199603\n",
      "Gradient Descent(5659/9999): loss=0.043968702414011906\n",
      "Gradient Descent(5660/9999): loss=0.043968700411217546\n",
      "Gradient Descent(5661/9999): loss=0.043968698411572235\n",
      "Gradient Descent(5662/9999): loss=0.04396869641507104\n",
      "Gradient Descent(5663/9999): loss=0.043968694421709\n",
      "Gradient Descent(5664/9999): loss=0.04396869243148119\n",
      "Gradient Descent(5665/9999): loss=0.043968690444382695\n",
      "Gradient Descent(5666/9999): loss=0.04396868846040859\n",
      "Gradient Descent(5667/9999): loss=0.04396868647955391\n",
      "Gradient Descent(5668/9999): loss=0.043968684501813814\n",
      "Gradient Descent(5669/9999): loss=0.0439686825271834\n",
      "Gradient Descent(5670/9999): loss=0.04396868055565774\n",
      "Gradient Descent(5671/9999): loss=0.04396867858723199\n",
      "Gradient Descent(5672/9999): loss=0.04396867662190125\n",
      "Gradient Descent(5673/9999): loss=0.043968674659660686\n",
      "Gradient Descent(5674/9999): loss=0.04396867270050539\n",
      "Gradient Descent(5675/9999): loss=0.04396867074443055\n",
      "Gradient Descent(5676/9999): loss=0.04396866879143132\n",
      "Gradient Descent(5677/9999): loss=0.04396866684150285\n",
      "Gradient Descent(5678/9999): loss=0.04396866489464031\n",
      "Gradient Descent(5679/9999): loss=0.04396866295083888\n",
      "Gradient Descent(5680/9999): loss=0.04396866101009376\n",
      "Gradient Descent(5681/9999): loss=0.043968659072400156\n",
      "Gradient Descent(5682/9999): loss=0.04396865713775324\n",
      "Gradient Descent(5683/9999): loss=0.04396865520614823\n",
      "Gradient Descent(5684/9999): loss=0.04396865327758036\n",
      "Gradient Descent(5685/9999): loss=0.04396865135204482\n",
      "Gradient Descent(5686/9999): loss=0.04396864942953686\n",
      "Gradient Descent(5687/9999): loss=0.04396864751005174\n",
      "Gradient Descent(5688/9999): loss=0.04396864559358469\n",
      "Gradient Descent(5689/9999): loss=0.043968643680130946\n",
      "Gradient Descent(5690/9999): loss=0.04396864176968582\n",
      "Gradient Descent(5691/9999): loss=0.04396863986224453\n",
      "Gradient Descent(5692/9999): loss=0.04396863795780237\n",
      "Gradient Descent(5693/9999): loss=0.04396863605635465\n",
      "Gradient Descent(5694/9999): loss=0.043968634157896605\n",
      "Gradient Descent(5695/9999): loss=0.04396863226242359\n",
      "Gradient Descent(5696/9999): loss=0.0439686303699309\n",
      "Gradient Descent(5697/9999): loss=0.0439686284804138\n",
      "Gradient Descent(5698/9999): loss=0.04396862659386768\n",
      "Gradient Descent(5699/9999): loss=0.04396862471028783\n",
      "Gradient Descent(5700/9999): loss=0.043968622829669596\n",
      "Gradient Descent(5701/9999): loss=0.043968620952008326\n",
      "Gradient Descent(5702/9999): loss=0.04396861907729936\n",
      "Gradient Descent(5703/9999): loss=0.043968617205538074\n",
      "Gradient Descent(5704/9999): loss=0.04396861533671981\n",
      "Gradient Descent(5705/9999): loss=0.04396861347083993\n",
      "Gradient Descent(5706/9999): loss=0.04396861160789386\n",
      "Gradient Descent(5707/9999): loss=0.04396860974787696\n",
      "Gradient Descent(5708/9999): loss=0.04396860789078461\n",
      "Gradient Descent(5709/9999): loss=0.043968606036612234\n",
      "Gradient Descent(5710/9999): loss=0.04396860418535525\n",
      "Gradient Descent(5711/9999): loss=0.04396860233700905\n",
      "Gradient Descent(5712/9999): loss=0.043968600491569074\n",
      "Gradient Descent(5713/9999): loss=0.043968598649030737\n",
      "Gradient Descent(5714/9999): loss=0.04396859680938948\n",
      "Gradient Descent(5715/9999): loss=0.043968594972640745\n",
      "Gradient Descent(5716/9999): loss=0.04396859313878\n",
      "Gradient Descent(5717/9999): loss=0.0439685913078027\n",
      "Gradient Descent(5718/9999): loss=0.0439685894797043\n",
      "Gradient Descent(5719/9999): loss=0.043968587654480286\n",
      "Gradient Descent(5720/9999): loss=0.04396858583212612\n",
      "Gradient Descent(5721/9999): loss=0.04396858401263731\n",
      "Gradient Descent(5722/9999): loss=0.04396858219600932\n",
      "Gradient Descent(5723/9999): loss=0.0439685803822377\n",
      "Gradient Descent(5724/9999): loss=0.04396857857131792\n",
      "Gradient Descent(5725/9999): loss=0.043968576763245545\n",
      "Gradient Descent(5726/9999): loss=0.043968574958016024\n",
      "Gradient Descent(5727/9999): loss=0.043968573155624927\n",
      "Gradient Descent(5728/9999): loss=0.0439685713560678\n",
      "Gradient Descent(5729/9999): loss=0.04396856955934017\n",
      "Gradient Descent(5730/9999): loss=0.043968567765437616\n",
      "Gradient Descent(5731/9999): loss=0.04396856597435563\n",
      "Gradient Descent(5732/9999): loss=0.043968564186089854\n",
      "Gradient Descent(5733/9999): loss=0.043968562400635826\n",
      "Gradient Descent(5734/9999): loss=0.043968560617989115\n",
      "Gradient Descent(5735/9999): loss=0.04396855883814533\n",
      "Gradient Descent(5736/9999): loss=0.04396855706110005\n",
      "Gradient Descent(5737/9999): loss=0.043968555286848884\n",
      "Gradient Descent(5738/9999): loss=0.043968553515387414\n",
      "Gradient Descent(5739/9999): loss=0.04396855174671126\n",
      "Gradient Descent(5740/9999): loss=0.04396854998081609\n",
      "Gradient Descent(5741/9999): loss=0.043968548217697474\n",
      "Gradient Descent(5742/9999): loss=0.043968546457351085\n",
      "Gradient Descent(5743/9999): loss=0.04396854469977252\n",
      "Gradient Descent(5744/9999): loss=0.04396854294495747\n",
      "Gradient Descent(5745/9999): loss=0.043968541192901583\n",
      "Gradient Descent(5746/9999): loss=0.043968539443600484\n",
      "Gradient Descent(5747/9999): loss=0.0439685376970499\n",
      "Gradient Descent(5748/9999): loss=0.04396853595324547\n",
      "Gradient Descent(5749/9999): loss=0.04396853421218285\n",
      "Gradient Descent(5750/9999): loss=0.043968532473857784\n",
      "Gradient Descent(5751/9999): loss=0.04396853073826594\n",
      "Gradient Descent(5752/9999): loss=0.04396852900540304\n",
      "Gradient Descent(5753/9999): loss=0.04396852727526476\n",
      "Gradient Descent(5754/9999): loss=0.04396852554784683\n",
      "Gradient Descent(5755/9999): loss=0.043968523823144985\n",
      "Gradient Descent(5756/9999): loss=0.04396852210115495\n",
      "Gradient Descent(5757/9999): loss=0.043968520381872454\n",
      "Gradient Descent(5758/9999): loss=0.043968518665293235\n",
      "Gradient Descent(5759/9999): loss=0.04396851695141306\n",
      "Gradient Descent(5760/9999): loss=0.043968515240227685\n",
      "Gradient Descent(5761/9999): loss=0.04396851353173284\n",
      "Gradient Descent(5762/9999): loss=0.043968511825924324\n",
      "Gradient Descent(5763/9999): loss=0.04396851012279791\n",
      "Gradient Descent(5764/9999): loss=0.04396850842234938\n",
      "Gradient Descent(5765/9999): loss=0.04396850672457452\n",
      "Gradient Descent(5766/9999): loss=0.043968505029469133\n",
      "Gradient Descent(5767/9999): loss=0.04396850333702901\n",
      "Gradient Descent(5768/9999): loss=0.043968501647249976\n",
      "Gradient Descent(5769/9999): loss=0.04396849996012781\n",
      "Gradient Descent(5770/9999): loss=0.043968498275658414\n",
      "Gradient Descent(5771/9999): loss=0.04396849659383752\n",
      "Gradient Descent(5772/9999): loss=0.04396849491466101\n",
      "Gradient Descent(5773/9999): loss=0.04396849323812473\n",
      "Gradient Descent(5774/9999): loss=0.04396849156422453\n",
      "Gradient Descent(5775/9999): loss=0.04396848989295627\n",
      "Gradient Descent(5776/9999): loss=0.043968488224315774\n",
      "Gradient Descent(5777/9999): loss=0.043968486558298964\n",
      "Gradient Descent(5778/9999): loss=0.04396848489490167\n",
      "Gradient Descent(5779/9999): loss=0.043968483234119796\n",
      "Gradient Descent(5780/9999): loss=0.04396848157594922\n",
      "Gradient Descent(5781/9999): loss=0.04396847992038583\n",
      "Gradient Descent(5782/9999): loss=0.043968478267425566\n",
      "Gradient Descent(5783/9999): loss=0.043968476617064295\n",
      "Gradient Descent(5784/9999): loss=0.04396847496929794\n",
      "Gradient Descent(5785/9999): loss=0.04396847332412243\n",
      "Gradient Descent(5786/9999): loss=0.043968471681533644\n",
      "Gradient Descent(5787/9999): loss=0.0439684700415276\n",
      "Gradient Descent(5788/9999): loss=0.04396846840410016\n",
      "Gradient Descent(5789/9999): loss=0.04396846676924732\n",
      "Gradient Descent(5790/9999): loss=0.043968465136965014\n",
      "Gradient Descent(5791/9999): loss=0.04396846350724922\n",
      "Gradient Descent(5792/9999): loss=0.04396846188009585\n",
      "Gradient Descent(5793/9999): loss=0.04396846025550092\n",
      "Gradient Descent(5794/9999): loss=0.04396845863346039\n",
      "Gradient Descent(5795/9999): loss=0.04396845701397025\n",
      "Gradient Descent(5796/9999): loss=0.04396845539702647\n",
      "Gradient Descent(5797/9999): loss=0.043968453782625073\n",
      "Gradient Descent(5798/9999): loss=0.043968452170762076\n",
      "Gradient Descent(5799/9999): loss=0.04396845056143344\n",
      "Gradient Descent(5800/9999): loss=0.0439684489546352\n",
      "Gradient Descent(5801/9999): loss=0.0439684473503634\n",
      "Gradient Descent(5802/9999): loss=0.04396844574861405\n",
      "Gradient Descent(5803/9999): loss=0.04396844414938318\n",
      "Gradient Descent(5804/9999): loss=0.04396844255266683\n",
      "Gradient Descent(5805/9999): loss=0.04396844095846106\n",
      "Gradient Descent(5806/9999): loss=0.0439684393667619\n",
      "Gradient Descent(5807/9999): loss=0.04396843777756542\n",
      "Gradient Descent(5808/9999): loss=0.04396843619086769\n",
      "Gradient Descent(5809/9999): loss=0.04396843460666479\n",
      "Gradient Descent(5810/9999): loss=0.04396843302495277\n",
      "Gradient Descent(5811/9999): loss=0.04396843144572773\n",
      "Gradient Descent(5812/9999): loss=0.043968429868985755\n",
      "Gradient Descent(5813/9999): loss=0.043968428294722965\n",
      "Gradient Descent(5814/9999): loss=0.043968426722935404\n",
      "Gradient Descent(5815/9999): loss=0.043968425153619234\n",
      "Gradient Descent(5816/9999): loss=0.04396842358677056\n",
      "Gradient Descent(5817/9999): loss=0.04396842202238547\n",
      "Gradient Descent(5818/9999): loss=0.04396842046046015\n",
      "Gradient Descent(5819/9999): loss=0.04396841890099069\n",
      "Gradient Descent(5820/9999): loss=0.04396841734397323\n",
      "Gradient Descent(5821/9999): loss=0.04396841578940389\n",
      "Gradient Descent(5822/9999): loss=0.043968414237278874\n",
      "Gradient Descent(5823/9999): loss=0.04396841268759432\n",
      "Gradient Descent(5824/9999): loss=0.04396841114034638\n",
      "Gradient Descent(5825/9999): loss=0.04396840959553124\n",
      "Gradient Descent(5826/9999): loss=0.04396840805314506\n",
      "Gradient Descent(5827/9999): loss=0.04396840651318403\n",
      "Gradient Descent(5828/9999): loss=0.043968404975644304\n",
      "Gradient Descent(5829/9999): loss=0.04396840344052213\n",
      "Gradient Descent(5830/9999): loss=0.04396840190781368\n",
      "Gradient Descent(5831/9999): loss=0.04396840037751513\n",
      "Gradient Descent(5832/9999): loss=0.04396839884962274\n",
      "Gradient Descent(5833/9999): loss=0.043968397324132706\n",
      "Gradient Descent(5834/9999): loss=0.043968395801041255\n",
      "Gradient Descent(5835/9999): loss=0.0439683942803446\n",
      "Gradient Descent(5836/9999): loss=0.04396839276203899\n",
      "Gradient Descent(5837/9999): loss=0.043968391246120654\n",
      "Gradient Descent(5838/9999): loss=0.043968389732585864\n",
      "Gradient Descent(5839/9999): loss=0.04396838822143084\n",
      "Gradient Descent(5840/9999): loss=0.043968386712651866\n",
      "Gradient Descent(5841/9999): loss=0.043968385206245186\n",
      "Gradient Descent(5842/9999): loss=0.043968383702207084\n",
      "Gradient Descent(5843/9999): loss=0.04396838220053383\n",
      "Gradient Descent(5844/9999): loss=0.04396838070122172\n",
      "Gradient Descent(5845/9999): loss=0.04396837920426701\n",
      "Gradient Descent(5846/9999): loss=0.043968377709666\n",
      "Gradient Descent(5847/9999): loss=0.04396837621741503\n",
      "Gradient Descent(5848/9999): loss=0.04396837472751037\n",
      "Gradient Descent(5849/9999): loss=0.04396837323994832\n",
      "Gradient Descent(5850/9999): loss=0.04396837175472523\n",
      "Gradient Descent(5851/9999): loss=0.04396837027183739\n",
      "Gradient Descent(5852/9999): loss=0.04396836879128116\n",
      "Gradient Descent(5853/9999): loss=0.04396836731305285\n",
      "Gradient Descent(5854/9999): loss=0.04396836583714881\n",
      "Gradient Descent(5855/9999): loss=0.043968364363565354\n",
      "Gradient Descent(5856/9999): loss=0.04396836289229889\n",
      "Gradient Descent(5857/9999): loss=0.04396836142334573\n",
      "Gradient Descent(5858/9999): loss=0.043968359956702285\n",
      "Gradient Descent(5859/9999): loss=0.04396835849236487\n",
      "Gradient Descent(5860/9999): loss=0.04396835703032989\n",
      "Gradient Descent(5861/9999): loss=0.043968355570593705\n",
      "Gradient Descent(5862/9999): loss=0.043968354113152724\n",
      "Gradient Descent(5863/9999): loss=0.043968352658003314\n",
      "Gradient Descent(5864/9999): loss=0.043968351205141906\n",
      "Gradient Descent(5865/9999): loss=0.04396834975456485\n",
      "Gradient Descent(5866/9999): loss=0.04396834830626859\n",
      "Gradient Descent(5867/9999): loss=0.04396834686024955\n",
      "Gradient Descent(5868/9999): loss=0.04396834541650411\n",
      "Gradient Descent(5869/9999): loss=0.04396834397502873\n",
      "Gradient Descent(5870/9999): loss=0.04396834253581984\n",
      "Gradient Descent(5871/9999): loss=0.043968341098873875\n",
      "Gradient Descent(5872/9999): loss=0.043968339664187235\n",
      "Gradient Descent(5873/9999): loss=0.04396833823175642\n",
      "Gradient Descent(5874/9999): loss=0.04396833680157788\n",
      "Gradient Descent(5875/9999): loss=0.04396833537364803\n",
      "Gradient Descent(5876/9999): loss=0.04396833394796336\n",
      "Gradient Descent(5877/9999): loss=0.04396833252452035\n",
      "Gradient Descent(5878/9999): loss=0.04396833110331546\n",
      "Gradient Descent(5879/9999): loss=0.0439683296843452\n",
      "Gradient Descent(5880/9999): loss=0.04396832826760602\n",
      "Gradient Descent(5881/9999): loss=0.04396832685309441\n",
      "Gradient Descent(5882/9999): loss=0.04396832544080691\n",
      "Gradient Descent(5883/9999): loss=0.04396832403073996\n",
      "Gradient Descent(5884/9999): loss=0.043968322622890146\n",
      "Gradient Descent(5885/9999): loss=0.04396832121725392\n",
      "Gradient Descent(5886/9999): loss=0.04396831981382784\n",
      "Gradient Descent(5887/9999): loss=0.04396831841260838\n",
      "Gradient Descent(5888/9999): loss=0.043968317013592106\n",
      "Gradient Descent(5889/9999): loss=0.04396831561677558\n",
      "Gradient Descent(5890/9999): loss=0.043968314222155296\n",
      "Gradient Descent(5891/9999): loss=0.04396831282972784\n",
      "Gradient Descent(5892/9999): loss=0.04396831143948973\n",
      "Gradient Descent(5893/9999): loss=0.043968310051437545\n",
      "Gradient Descent(5894/9999): loss=0.043968308665567814\n",
      "Gradient Descent(5895/9999): loss=0.04396830728187716\n",
      "Gradient Descent(5896/9999): loss=0.04396830590036212\n",
      "Gradient Descent(5897/9999): loss=0.043968304521019276\n",
      "Gradient Descent(5898/9999): loss=0.04396830314384523\n",
      "Gradient Descent(5899/9999): loss=0.04396830176883655\n",
      "Gradient Descent(5900/9999): loss=0.04396830039598983\n",
      "Gradient Descent(5901/9999): loss=0.04396829902530172\n",
      "Gradient Descent(5902/9999): loss=0.043968297656768754\n",
      "Gradient Descent(5903/9999): loss=0.0439682962903876\n",
      "Gradient Descent(5904/9999): loss=0.04396829492615484\n",
      "Gradient Descent(5905/9999): loss=0.0439682935640671\n",
      "Gradient Descent(5906/9999): loss=0.04396829220412103\n",
      "Gradient Descent(5907/9999): loss=0.04396829084631324\n",
      "Gradient Descent(5908/9999): loss=0.04396828949064038\n",
      "Gradient Descent(5909/9999): loss=0.04396828813709912\n",
      "Gradient Descent(5910/9999): loss=0.043968286785686005\n",
      "Gradient Descent(5911/9999): loss=0.043968285436397817\n",
      "Gradient Descent(5912/9999): loss=0.04396828408923114\n",
      "Gradient Descent(5913/9999): loss=0.04396828274418267\n",
      "Gradient Descent(5914/9999): loss=0.04396828140124906\n",
      "Gradient Descent(5915/9999): loss=0.04396828006042699\n",
      "Gradient Descent(5916/9999): loss=0.04396827872171314\n",
      "Gradient Descent(5917/9999): loss=0.04396827738510417\n",
      "Gradient Descent(5918/9999): loss=0.04396827605059683\n",
      "Gradient Descent(5919/9999): loss=0.043968274718187754\n",
      "Gradient Descent(5920/9999): loss=0.04396827338787369\n",
      "Gradient Descent(5921/9999): loss=0.043968272059651314\n",
      "Gradient Descent(5922/9999): loss=0.04396827073351733\n",
      "Gradient Descent(5923/9999): loss=0.04396826940946849\n",
      "Gradient Descent(5924/9999): loss=0.0439682680875015\n",
      "Gradient Descent(5925/9999): loss=0.04396826676761306\n",
      "Gradient Descent(5926/9999): loss=0.04396826544979995\n",
      "Gradient Descent(5927/9999): loss=0.043968264134058875\n",
      "Gradient Descent(5928/9999): loss=0.04396826282038657\n",
      "Gradient Descent(5929/9999): loss=0.043968261508779784\n",
      "Gradient Descent(5930/9999): loss=0.043968260199235304\n",
      "Gradient Descent(5931/9999): loss=0.04396825889174986\n",
      "Gradient Descent(5932/9999): loss=0.04396825758632022\n",
      "Gradient Descent(5933/9999): loss=0.04396825628294315\n",
      "Gradient Descent(5934/9999): loss=0.04396825498161542\n",
      "Gradient Descent(5935/9999): loss=0.043968253682333806\n",
      "Gradient Descent(5936/9999): loss=0.04396825238509511\n",
      "Gradient Descent(5937/9999): loss=0.04396825108989609\n",
      "Gradient Descent(5938/9999): loss=0.04396824979673356\n",
      "Gradient Descent(5939/9999): loss=0.043968248505604296\n",
      "Gradient Descent(5940/9999): loss=0.04396824721650515\n",
      "Gradient Descent(5941/9999): loss=0.04396824592943289\n",
      "Gradient Descent(5942/9999): loss=0.043968244644384306\n",
      "Gradient Descent(5943/9999): loss=0.04396824336135627\n",
      "Gradient Descent(5944/9999): loss=0.04396824208034558\n",
      "Gradient Descent(5945/9999): loss=0.04396824080134906\n",
      "Gradient Descent(5946/9999): loss=0.043968239524363566\n",
      "Gradient Descent(5947/9999): loss=0.0439682382493859\n",
      "Gradient Descent(5948/9999): loss=0.043968236976412924\n",
      "Gradient Descent(5949/9999): loss=0.043968235705441484\n",
      "Gradient Descent(5950/9999): loss=0.04396823443646845\n",
      "Gradient Descent(5951/9999): loss=0.043968233169490646\n",
      "Gradient Descent(5952/9999): loss=0.043968231904504974\n",
      "Gradient Descent(5953/9999): loss=0.04396823064150826\n",
      "Gradient Descent(5954/9999): loss=0.04396822938049741\n",
      "Gradient Descent(5955/9999): loss=0.04396822812146928\n",
      "Gradient Descent(5956/9999): loss=0.043968226864420756\n",
      "Gradient Descent(5957/9999): loss=0.043968225609348745\n",
      "Gradient Descent(5958/9999): loss=0.04396822435625013\n",
      "Gradient Descent(5959/9999): loss=0.04396822310512181\n",
      "Gradient Descent(5960/9999): loss=0.04396822185596066\n",
      "Gradient Descent(5961/9999): loss=0.04396822060876362\n",
      "Gradient Descent(5962/9999): loss=0.04396821936352758\n",
      "Gradient Descent(5963/9999): loss=0.04396821812024946\n",
      "Gradient Descent(5964/9999): loss=0.0439682168789262\n",
      "Gradient Descent(5965/9999): loss=0.0439682156395547\n",
      "Gradient Descent(5966/9999): loss=0.043968214402131924\n",
      "Gradient Descent(5967/9999): loss=0.04396821316665478\n",
      "Gradient Descent(5968/9999): loss=0.04396821193312023\n",
      "Gradient Descent(5969/9999): loss=0.04396821070152518\n",
      "Gradient Descent(5970/9999): loss=0.04396820947186662\n",
      "Gradient Descent(5971/9999): loss=0.0439682082441415\n",
      "Gradient Descent(5972/9999): loss=0.04396820701834675\n",
      "Gradient Descent(5973/9999): loss=0.04396820579447938\n",
      "Gradient Descent(5974/9999): loss=0.04396820457253634\n",
      "Gradient Descent(5975/9999): loss=0.04396820335251459\n",
      "Gradient Descent(5976/9999): loss=0.043968202134411126\n",
      "Gradient Descent(5977/9999): loss=0.04396820091822291\n",
      "Gradient Descent(5978/9999): loss=0.04396819970394698\n",
      "Gradient Descent(5979/9999): loss=0.04396819849158028\n",
      "Gradient Descent(5980/9999): loss=0.04396819728111981\n",
      "Gradient Descent(5981/9999): loss=0.04396819607256261\n",
      "Gradient Descent(5982/9999): loss=0.04396819486590565\n",
      "Gradient Descent(5983/9999): loss=0.04396819366114596\n",
      "Gradient Descent(5984/9999): loss=0.04396819245828055\n",
      "Gradient Descent(5985/9999): loss=0.043968191257306444\n",
      "Gradient Descent(5986/9999): loss=0.04396819005822067\n",
      "Gradient Descent(5987/9999): loss=0.04396818886102028\n",
      "Gradient Descent(5988/9999): loss=0.04396818766570225\n",
      "Gradient Descent(5989/9999): loss=0.04396818647226369\n",
      "Gradient Descent(5990/9999): loss=0.043968185280701594\n",
      "Gradient Descent(5991/9999): loss=0.043968184091013045\n",
      "Gradient Descent(5992/9999): loss=0.04396818290319506\n",
      "Gradient Descent(5993/9999): loss=0.04396818171724476\n",
      "Gradient Descent(5994/9999): loss=0.04396818053315912\n",
      "Gradient Descent(5995/9999): loss=0.043968179350935284\n",
      "Gradient Descent(5996/9999): loss=0.04396817817057031\n",
      "Gradient Descent(5997/9999): loss=0.04396817699206123\n",
      "Gradient Descent(5998/9999): loss=0.043968175815405186\n",
      "Gradient Descent(5999/9999): loss=0.04396817464059922\n",
      "Gradient Descent(6000/9999): loss=0.04396817346764046\n",
      "Gradient Descent(6001/9999): loss=0.04396817229652597\n",
      "Gradient Descent(6002/9999): loss=0.043968171127252854\n",
      "Gradient Descent(6003/9999): loss=0.04396816995981825\n",
      "Gradient Descent(6004/9999): loss=0.04396816879421922\n",
      "Gradient Descent(6005/9999): loss=0.0439681676304529\n",
      "Gradient Descent(6006/9999): loss=0.043968166468516416\n",
      "Gradient Descent(6007/9999): loss=0.04396816530840687\n",
      "Gradient Descent(6008/9999): loss=0.04396816415012141\n",
      "Gradient Descent(6009/9999): loss=0.043968162993657176\n",
      "Gradient Descent(6010/9999): loss=0.04396816183901127\n",
      "Gradient Descent(6011/9999): loss=0.043968160686180854\n",
      "Gradient Descent(6012/9999): loss=0.043968159535163094\n",
      "Gradient Descent(6013/9999): loss=0.043968158385955095\n",
      "Gradient Descent(6014/9999): loss=0.04396815723855403\n",
      "Gradient Descent(6015/9999): loss=0.043968156092957095\n",
      "Gradient Descent(6016/9999): loss=0.04396815494916137\n",
      "Gradient Descent(6017/9999): loss=0.043968153807164095\n",
      "Gradient Descent(6018/9999): loss=0.04396815266696243\n",
      "Gradient Descent(6019/9999): loss=0.04396815152855352\n",
      "Gradient Descent(6020/9999): loss=0.04396815039193459\n",
      "Gradient Descent(6021/9999): loss=0.04396814925710281\n",
      "Gradient Descent(6022/9999): loss=0.04396814812405534\n",
      "Gradient Descent(6023/9999): loss=0.04396814699278942\n",
      "Gradient Descent(6024/9999): loss=0.04396814586330221\n",
      "Gradient Descent(6025/9999): loss=0.04396814473559092\n",
      "Gradient Descent(6026/9999): loss=0.04396814360965279\n",
      "Gradient Descent(6027/9999): loss=0.04396814248548501\n",
      "Gradient Descent(6028/9999): loss=0.043968141363084766\n",
      "Gradient Descent(6029/9999): loss=0.04396814024244935\n",
      "Gradient Descent(6030/9999): loss=0.04396813912357592\n",
      "Gradient Descent(6031/9999): loss=0.04396813800646174\n",
      "Gradient Descent(6032/9999): loss=0.043968136891104036\n",
      "Gradient Descent(6033/9999): loss=0.043968135777500035\n",
      "Gradient Descent(6034/9999): loss=0.04396813466564702\n",
      "Gradient Descent(6035/9999): loss=0.04396813355554216\n",
      "Gradient Descent(6036/9999): loss=0.043968132447182805\n",
      "Gradient Descent(6037/9999): loss=0.04396813134056614\n",
      "Gradient Descent(6038/9999): loss=0.04396813023568944\n",
      "Gradient Descent(6039/9999): loss=0.043968129132549987\n",
      "Gradient Descent(6040/9999): loss=0.043968128031145026\n",
      "Gradient Descent(6041/9999): loss=0.043968126931471826\n",
      "Gradient Descent(6042/9999): loss=0.0439681258335277\n",
      "Gradient Descent(6043/9999): loss=0.04396812473730989\n",
      "Gradient Descent(6044/9999): loss=0.04396812364281571\n",
      "Gradient Descent(6045/9999): loss=0.04396812255004246\n",
      "Gradient Descent(6046/9999): loss=0.04396812145898738\n",
      "Gradient Descent(6047/9999): loss=0.04396812036964781\n",
      "Gradient Descent(6048/9999): loss=0.04396811928202106\n",
      "Gradient Descent(6049/9999): loss=0.04396811819610441\n",
      "Gradient Descent(6050/9999): loss=0.04396811711189518\n",
      "Gradient Descent(6051/9999): loss=0.0439681160293907\n",
      "Gradient Descent(6052/9999): loss=0.04396811494858827\n",
      "Gradient Descent(6053/9999): loss=0.04396811386948522\n",
      "Gradient Descent(6054/9999): loss=0.0439681127920789\n",
      "Gradient Descent(6055/9999): loss=0.043968111716366574\n",
      "Gradient Descent(6056/9999): loss=0.043968110642345654\n",
      "Gradient Descent(6057/9999): loss=0.04396810957001347\n",
      "Gradient Descent(6058/9999): loss=0.04396810849936732\n",
      "Gradient Descent(6059/9999): loss=0.0439681074304046\n",
      "Gradient Descent(6060/9999): loss=0.04396810636312264\n",
      "Gradient Descent(6061/9999): loss=0.04396810529751881\n",
      "Gradient Descent(6062/9999): loss=0.04396810423359045\n",
      "Gradient Descent(6063/9999): loss=0.043968103171334955\n",
      "Gradient Descent(6064/9999): loss=0.04396810211074967\n",
      "Gradient Descent(6065/9999): loss=0.043968101051831965\n",
      "Gradient Descent(6066/9999): loss=0.04396809999457925\n",
      "Gradient Descent(6067/9999): loss=0.04396809893898891\n",
      "Gradient Descent(6068/9999): loss=0.043968097885058254\n",
      "Gradient Descent(6069/9999): loss=0.04396809683278478\n",
      "Gradient Descent(6070/9999): loss=0.0439680957821658\n",
      "Gradient Descent(6071/9999): loss=0.043968094733198744\n",
      "Gradient Descent(6072/9999): loss=0.04396809368588104\n",
      "Gradient Descent(6073/9999): loss=0.04396809264021004\n",
      "Gradient Descent(6074/9999): loss=0.04396809159618319\n",
      "Gradient Descent(6075/9999): loss=0.04396809055379792\n",
      "Gradient Descent(6076/9999): loss=0.04396808951305159\n",
      "Gradient Descent(6077/9999): loss=0.043968088473941666\n",
      "Gradient Descent(6078/9999): loss=0.04396808743646559\n",
      "Gradient Descent(6079/9999): loss=0.04396808640062078\n",
      "Gradient Descent(6080/9999): loss=0.04396808536640462\n",
      "Gradient Descent(6081/9999): loss=0.04396808433381464\n",
      "Gradient Descent(6082/9999): loss=0.04396808330284821\n",
      "Gradient Descent(6083/9999): loss=0.04396808227350281\n",
      "Gradient Descent(6084/9999): loss=0.04396808124577586\n",
      "Gradient Descent(6085/9999): loss=0.04396808021966486\n",
      "Gradient Descent(6086/9999): loss=0.04396807919516725\n",
      "Gradient Descent(6087/9999): loss=0.04396807817228049\n",
      "Gradient Descent(6088/9999): loss=0.04396807715100204\n",
      "Gradient Descent(6089/9999): loss=0.0439680761313294\n",
      "Gradient Descent(6090/9999): loss=0.04396807511326\n",
      "Gradient Descent(6091/9999): loss=0.043968074096791376\n",
      "Gradient Descent(6092/9999): loss=0.043968073081920946\n",
      "Gradient Descent(6093/9999): loss=0.04396807206864626\n",
      "Gradient Descent(6094/9999): loss=0.04396807105696476\n",
      "Gradient Descent(6095/9999): loss=0.04396807004687396\n",
      "Gradient Descent(6096/9999): loss=0.04396806903837137\n",
      "Gradient Descent(6097/9999): loss=0.04396806803145447\n",
      "Gradient Descent(6098/9999): loss=0.043968067026120786\n",
      "Gradient Descent(6099/9999): loss=0.04396806602236781\n",
      "Gradient Descent(6100/9999): loss=0.04396806502019307\n",
      "Gradient Descent(6101/9999): loss=0.0439680640195941\n",
      "Gradient Descent(6102/9999): loss=0.043968063020568396\n",
      "Gradient Descent(6103/9999): loss=0.043968062023113486\n",
      "Gradient Descent(6104/9999): loss=0.043968061027226925\n",
      "Gradient Descent(6105/9999): loss=0.0439680600329062\n",
      "Gradient Descent(6106/9999): loss=0.0439680590401489\n",
      "Gradient Descent(6107/9999): loss=0.043968058048952534\n",
      "Gradient Descent(6108/9999): loss=0.043968057059314676\n",
      "Gradient Descent(6109/9999): loss=0.043968056071232844\n",
      "Gradient Descent(6110/9999): loss=0.04396805508470459\n",
      "Gradient Descent(6111/9999): loss=0.04396805409972753\n",
      "Gradient Descent(6112/9999): loss=0.043968053116299165\n",
      "Gradient Descent(6113/9999): loss=0.043968052134417054\n",
      "Gradient Descent(6114/9999): loss=0.04396805115407879\n",
      "Gradient Descent(6115/9999): loss=0.04396805017528197\n",
      "Gradient Descent(6116/9999): loss=0.043968049198024134\n",
      "Gradient Descent(6117/9999): loss=0.04396804822230288\n",
      "Gradient Descent(6118/9999): loss=0.043968047248115774\n",
      "Gradient Descent(6119/9999): loss=0.04396804627546043\n",
      "Gradient Descent(6120/9999): loss=0.043968045304334416\n",
      "Gradient Descent(6121/9999): loss=0.04396804433473532\n",
      "Gradient Descent(6122/9999): loss=0.04396804336666077\n",
      "Gradient Descent(6123/9999): loss=0.043968042400108355\n",
      "Gradient Descent(6124/9999): loss=0.04396804143507569\n",
      "Gradient Descent(6125/9999): loss=0.04396804047156037\n",
      "Gradient Descent(6126/9999): loss=0.043968039509560006\n",
      "Gradient Descent(6127/9999): loss=0.043968038549072226\n",
      "Gradient Descent(6128/9999): loss=0.04396803759009467\n",
      "Gradient Descent(6129/9999): loss=0.04396803663262495\n",
      "Gradient Descent(6130/9999): loss=0.043968035676660674\n",
      "Gradient Descent(6131/9999): loss=0.043968034722199505\n",
      "Gradient Descent(6132/9999): loss=0.043968033769239065\n",
      "Gradient Descent(6133/9999): loss=0.04396803281777698\n",
      "Gradient Descent(6134/9999): loss=0.04396803186781094\n",
      "Gradient Descent(6135/9999): loss=0.043968030919338534\n",
      "Gradient Descent(6136/9999): loss=0.04396802997235747\n",
      "Gradient Descent(6137/9999): loss=0.043968029026865356\n",
      "Gradient Descent(6138/9999): loss=0.04396802808285988\n",
      "Gradient Descent(6139/9999): loss=0.04396802714033869\n",
      "Gradient Descent(6140/9999): loss=0.043968026199299445\n",
      "Gradient Descent(6141/9999): loss=0.043968025259739855\n",
      "Gradient Descent(6142/9999): loss=0.04396802432165752\n",
      "Gradient Descent(6143/9999): loss=0.043968023385050216\n",
      "Gradient Descent(6144/9999): loss=0.043968022449915545\n",
      "Gradient Descent(6145/9999): loss=0.04396802151625121\n",
      "Gradient Descent(6146/9999): loss=0.04396802058405493\n",
      "Gradient Descent(6147/9999): loss=0.043968019653324364\n",
      "Gradient Descent(6148/9999): loss=0.043968018724057194\n",
      "Gradient Descent(6149/9999): loss=0.04396801779625118\n",
      "Gradient Descent(6150/9999): loss=0.043968016869903946\n",
      "Gradient Descent(6151/9999): loss=0.04396801594501326\n",
      "Gradient Descent(6152/9999): loss=0.04396801502157682\n",
      "Gradient Descent(6153/9999): loss=0.04396801409959232\n",
      "Gradient Descent(6154/9999): loss=0.043968013179057464\n",
      "Gradient Descent(6155/9999): loss=0.043968012259970025\n",
      "Gradient Descent(6156/9999): loss=0.04396801134232769\n",
      "Gradient Descent(6157/9999): loss=0.04396801042612818\n",
      "Gradient Descent(6158/9999): loss=0.043968009511369276\n",
      "Gradient Descent(6159/9999): loss=0.043968008598048634\n",
      "Gradient Descent(6160/9999): loss=0.04396800768616407\n",
      "Gradient Descent(6161/9999): loss=0.04396800677571326\n",
      "Gradient Descent(6162/9999): loss=0.04396800586669402\n",
      "Gradient Descent(6163/9999): loss=0.04396800495910403\n",
      "Gradient Descent(6164/9999): loss=0.043968004052941094\n",
      "Gradient Descent(6165/9999): loss=0.04396800314820294\n",
      "Gradient Descent(6166/9999): loss=0.04396800224488733\n",
      "Gradient Descent(6167/9999): loss=0.043968001342992065\n",
      "Gradient Descent(6168/9999): loss=0.04396800044251485\n",
      "Gradient Descent(6169/9999): loss=0.04396799954345349\n",
      "Gradient Descent(6170/9999): loss=0.043967998645805736\n",
      "Gradient Descent(6171/9999): loss=0.043967997749569394\n",
      "Gradient Descent(6172/9999): loss=0.043967996854742245\n",
      "Gradient Descent(6173/9999): loss=0.04396799596132205\n",
      "Gradient Descent(6174/9999): loss=0.04396799506930663\n",
      "Gradient Descent(6175/9999): loss=0.04396799417869374\n",
      "Gradient Descent(6176/9999): loss=0.04396799328948121\n",
      "Gradient Descent(6177/9999): loss=0.04396799240166678\n",
      "Gradient Descent(6178/9999): loss=0.04396799151524832\n",
      "Gradient Descent(6179/9999): loss=0.043967990630223594\n",
      "Gradient Descent(6180/9999): loss=0.04396798974659042\n",
      "Gradient Descent(6181/9999): loss=0.043967988864346615\n",
      "Gradient Descent(6182/9999): loss=0.043967987983490005\n",
      "Gradient Descent(6183/9999): loss=0.04396798710401837\n",
      "Gradient Descent(6184/9999): loss=0.043967986225929574\n",
      "Gradient Descent(6185/9999): loss=0.04396798534922142\n",
      "Gradient Descent(6186/9999): loss=0.04396798447389175\n",
      "Gradient Descent(6187/9999): loss=0.04396798359993836\n",
      "Gradient Descent(6188/9999): loss=0.04396798272735916\n",
      "Gradient Descent(6189/9999): loss=0.04396798185615192\n",
      "Gradient Descent(6190/9999): loss=0.04396798098631451\n",
      "Gradient Descent(6191/9999): loss=0.04396798011784475\n",
      "Gradient Descent(6192/9999): loss=0.04396797925074056\n",
      "Gradient Descent(6193/9999): loss=0.04396797838499973\n",
      "Gradient Descent(6194/9999): loss=0.04396797752062013\n",
      "Gradient Descent(6195/9999): loss=0.04396797665759961\n",
      "Gradient Descent(6196/9999): loss=0.04396797579593606\n",
      "Gradient Descent(6197/9999): loss=0.043967974935627326\n",
      "Gradient Descent(6198/9999): loss=0.043967974076671294\n",
      "Gradient Descent(6199/9999): loss=0.043967973219065835\n",
      "Gradient Descent(6200/9999): loss=0.0439679723628088\n",
      "Gradient Descent(6201/9999): loss=0.0439679715078981\n",
      "Gradient Descent(6202/9999): loss=0.043967970654331584\n",
      "Gradient Descent(6203/9999): loss=0.043967969802107154\n",
      "Gradient Descent(6204/9999): loss=0.043967968951222736\n",
      "Gradient Descent(6205/9999): loss=0.043967968101676164\n",
      "Gradient Descent(6206/9999): loss=0.043967967253465384\n",
      "Gradient Descent(6207/9999): loss=0.04396796640658826\n",
      "Gradient Descent(6208/9999): loss=0.043967965561042724\n",
      "Gradient Descent(6209/9999): loss=0.04396796471682665\n",
      "Gradient Descent(6210/9999): loss=0.04396796387393796\n",
      "Gradient Descent(6211/9999): loss=0.04396796303237458\n",
      "Gradient Descent(6212/9999): loss=0.04396796219213442\n",
      "Gradient Descent(6213/9999): loss=0.04396796135321541\n",
      "Gradient Descent(6214/9999): loss=0.04396796051561545\n",
      "Gradient Descent(6215/9999): loss=0.04396795967933247\n",
      "Gradient Descent(6216/9999): loss=0.04396795884436441\n",
      "Gradient Descent(6217/9999): loss=0.0439679580107092\n",
      "Gradient Descent(6218/9999): loss=0.04396795717836476\n",
      "Gradient Descent(6219/9999): loss=0.04396795634732906\n",
      "Gradient Descent(6220/9999): loss=0.04396795551760002\n",
      "Gradient Descent(6221/9999): loss=0.043967954689175574\n",
      "Gradient Descent(6222/9999): loss=0.043967953862053724\n",
      "Gradient Descent(6223/9999): loss=0.04396795303623236\n",
      "Gradient Descent(6224/9999): loss=0.04396795221170945\n",
      "Gradient Descent(6225/9999): loss=0.043967951388483\n",
      "Gradient Descent(6226/9999): loss=0.0439679505665509\n",
      "Gradient Descent(6227/9999): loss=0.04396794974591118\n",
      "Gradient Descent(6228/9999): loss=0.043967948926561744\n",
      "Gradient Descent(6229/9999): loss=0.043967948108500646\n",
      "Gradient Descent(6230/9999): loss=0.043967947291725776\n",
      "Gradient Descent(6231/9999): loss=0.04396794647623515\n",
      "Gradient Descent(6232/9999): loss=0.04396794566202677\n",
      "Gradient Descent(6233/9999): loss=0.04396794484909857\n",
      "Gradient Descent(6234/9999): loss=0.04396794403744859\n",
      "Gradient Descent(6235/9999): loss=0.04396794322707478\n",
      "Gradient Descent(6236/9999): loss=0.043967942417975144\n",
      "Gradient Descent(6237/9999): loss=0.04396794161014769\n",
      "Gradient Descent(6238/9999): loss=0.04396794080359039\n",
      "Gradient Descent(6239/9999): loss=0.04396793999830129\n",
      "Gradient Descent(6240/9999): loss=0.04396793919427837\n",
      "Gradient Descent(6241/9999): loss=0.043967938391519634\n",
      "Gradient Descent(6242/9999): loss=0.043967937590023105\n",
      "Gradient Descent(6243/9999): loss=0.043967936789786805\n",
      "Gradient Descent(6244/9999): loss=0.043967935990808726\n",
      "Gradient Descent(6245/9999): loss=0.04396793519308692\n",
      "Gradient Descent(6246/9999): loss=0.0439679343966194\n",
      "Gradient Descent(6247/9999): loss=0.04396793360140417\n",
      "Gradient Descent(6248/9999): loss=0.04396793280743931\n",
      "Gradient Descent(6249/9999): loss=0.04396793201472282\n",
      "Gradient Descent(6250/9999): loss=0.043967931223252724\n",
      "Gradient Descent(6251/9999): loss=0.0439679304330271\n",
      "Gradient Descent(6252/9999): loss=0.043967929644043974\n",
      "Gradient Descent(6253/9999): loss=0.043967928856301394\n",
      "Gradient Descent(6254/9999): loss=0.04396792806979742\n",
      "Gradient Descent(6255/9999): loss=0.043967927284530056\n",
      "Gradient Descent(6256/9999): loss=0.043967926500497405\n",
      "Gradient Descent(6257/9999): loss=0.04396792571769756\n",
      "Gradient Descent(6258/9999): loss=0.04396792493612848\n",
      "Gradient Descent(6259/9999): loss=0.04396792415578831\n",
      "Gradient Descent(6260/9999): loss=0.04396792337667509\n",
      "Gradient Descent(6261/9999): loss=0.04396792259878691\n",
      "Gradient Descent(6262/9999): loss=0.043967921822121815\n",
      "Gradient Descent(6263/9999): loss=0.0439679210466779\n",
      "Gradient Descent(6264/9999): loss=0.04396792027245322\n",
      "Gradient Descent(6265/9999): loss=0.04396791949944588\n",
      "Gradient Descent(6266/9999): loss=0.043967918727654015\n",
      "Gradient Descent(6267/9999): loss=0.043967917957075636\n",
      "Gradient Descent(6268/9999): loss=0.04396791718770886\n",
      "Gradient Descent(6269/9999): loss=0.043967916419551764\n",
      "Gradient Descent(6270/9999): loss=0.043967915652602496\n",
      "Gradient Descent(6271/9999): loss=0.04396791488685913\n",
      "Gradient Descent(6272/9999): loss=0.04396791412231976\n",
      "Gradient Descent(6273/9999): loss=0.043967913358982494\n",
      "Gradient Descent(6274/9999): loss=0.04396791259684545\n",
      "Gradient Descent(6275/9999): loss=0.043967911835906745\n",
      "Gradient Descent(6276/9999): loss=0.04396791107616448\n",
      "Gradient Descent(6277/9999): loss=0.0439679103176168\n",
      "Gradient Descent(6278/9999): loss=0.043967909560261774\n",
      "Gradient Descent(6279/9999): loss=0.043967908804097615\n",
      "Gradient Descent(6280/9999): loss=0.043967908049122366\n",
      "Gradient Descent(6281/9999): loss=0.04396790729533417\n",
      "Gradient Descent(6282/9999): loss=0.0439679065427312\n",
      "Gradient Descent(6283/9999): loss=0.043967905791311594\n",
      "Gradient Descent(6284/9999): loss=0.04396790504107345\n",
      "Gradient Descent(6285/9999): loss=0.04396790429201492\n",
      "Gradient Descent(6286/9999): loss=0.04396790354413417\n",
      "Gradient Descent(6287/9999): loss=0.04396790279742937\n",
      "Gradient Descent(6288/9999): loss=0.04396790205189858\n",
      "Gradient Descent(6289/9999): loss=0.043967901307540046\n",
      "Gradient Descent(6290/9999): loss=0.04396790056435189\n",
      "Gradient Descent(6291/9999): loss=0.043967899822332256\n",
      "Gradient Descent(6292/9999): loss=0.04396789908147935\n",
      "Gradient Descent(6293/9999): loss=0.043967898341791295\n",
      "Gradient Descent(6294/9999): loss=0.043967897603266276\n",
      "Gradient Descent(6295/9999): loss=0.04396789686590247\n",
      "Gradient Descent(6296/9999): loss=0.04396789612969803\n",
      "Gradient Descent(6297/9999): loss=0.04396789539465116\n",
      "Gradient Descent(6298/9999): loss=0.04396789466076005\n",
      "Gradient Descent(6299/9999): loss=0.04396789392802283\n",
      "Gradient Descent(6300/9999): loss=0.04396789319643773\n",
      "Gradient Descent(6301/9999): loss=0.04396789246600291\n",
      "Gradient Descent(6302/9999): loss=0.04396789173671662\n",
      "Gradient Descent(6303/9999): loss=0.04396789100857697\n",
      "Gradient Descent(6304/9999): loss=0.043967890281582225\n",
      "Gradient Descent(6305/9999): loss=0.04396788955573053\n",
      "Gradient Descent(6306/9999): loss=0.04396788883102012\n",
      "Gradient Descent(6307/9999): loss=0.043967888107449206\n",
      "Gradient Descent(6308/9999): loss=0.043967887385015995\n",
      "Gradient Descent(6309/9999): loss=0.043967886663718675\n",
      "Gradient Descent(6310/9999): loss=0.04396788594355547\n",
      "Gradient Descent(6311/9999): loss=0.043967885224524605\n",
      "Gradient Descent(6312/9999): loss=0.04396788450662431\n",
      "Gradient Descent(6313/9999): loss=0.043967883789852794\n",
      "Gradient Descent(6314/9999): loss=0.043967883074208285\n",
      "Gradient Descent(6315/9999): loss=0.04396788235968899\n",
      "Gradient Descent(6316/9999): loss=0.043967881646293144\n",
      "Gradient Descent(6317/9999): loss=0.043967880934019016\n",
      "Gradient Descent(6318/9999): loss=0.04396788022286481\n",
      "Gradient Descent(6319/9999): loss=0.043967879512828774\n",
      "Gradient Descent(6320/9999): loss=0.04396787880390916\n",
      "Gradient Descent(6321/9999): loss=0.043967878096104224\n",
      "Gradient Descent(6322/9999): loss=0.043967877389412165\n",
      "Gradient Descent(6323/9999): loss=0.04396787668383126\n",
      "Gradient Descent(6324/9999): loss=0.04396787597935978\n",
      "Gradient Descent(6325/9999): loss=0.04396787527599596\n",
      "Gradient Descent(6326/9999): loss=0.04396787457373806\n",
      "Gradient Descent(6327/9999): loss=0.04396787387258434\n",
      "Gradient Descent(6328/9999): loss=0.04396787317253307\n",
      "Gradient Descent(6329/9999): loss=0.0439678724735825\n",
      "Gradient Descent(6330/9999): loss=0.04396787177573092\n",
      "Gradient Descent(6331/9999): loss=0.043967871078976616\n",
      "Gradient Descent(6332/9999): loss=0.04396787038331783\n",
      "Gradient Descent(6333/9999): loss=0.043967869688752846\n",
      "Gradient Descent(6334/9999): loss=0.04396786899527994\n",
      "Gradient Descent(6335/9999): loss=0.043967868302897416\n",
      "Gradient Descent(6336/9999): loss=0.04396786761160354\n",
      "Gradient Descent(6337/9999): loss=0.04396786692139662\n",
      "Gradient Descent(6338/9999): loss=0.04396786623227493\n",
      "Gradient Descent(6339/9999): loss=0.043967865544236776\n",
      "Gradient Descent(6340/9999): loss=0.04396786485728044\n",
      "Gradient Descent(6341/9999): loss=0.04396786417140422\n",
      "Gradient Descent(6342/9999): loss=0.04396786348660644\n",
      "Gradient Descent(6343/9999): loss=0.04396786280288539\n",
      "Gradient Descent(6344/9999): loss=0.04396786212023937\n",
      "Gradient Descent(6345/9999): loss=0.043967861438666704\n",
      "Gradient Descent(6346/9999): loss=0.04396786075816568\n",
      "Gradient Descent(6347/9999): loss=0.04396786007873464\n",
      "Gradient Descent(6348/9999): loss=0.0439678594003719\n",
      "Gradient Descent(6349/9999): loss=0.043967858723075764\n",
      "Gradient Descent(6350/9999): loss=0.04396785804684456\n",
      "Gradient Descent(6351/9999): loss=0.04396785737167663\n",
      "Gradient Descent(6352/9999): loss=0.04396785669757028\n",
      "Gradient Descent(6353/9999): loss=0.043967856024523834\n",
      "Gradient Descent(6354/9999): loss=0.04396785535253568\n",
      "Gradient Descent(6355/9999): loss=0.043967854681604086\n",
      "Gradient Descent(6356/9999): loss=0.043967854011727404\n",
      "Gradient Descent(6357/9999): loss=0.04396785334290404\n",
      "Gradient Descent(6358/9999): loss=0.04396785267513225\n",
      "Gradient Descent(6359/9999): loss=0.04396785200841044\n",
      "Gradient Descent(6360/9999): loss=0.04396785134273693\n",
      "Gradient Descent(6361/9999): loss=0.04396785067811008\n",
      "Gradient Descent(6362/9999): loss=0.043967850014528256\n",
      "Gradient Descent(6363/9999): loss=0.04396784935198978\n",
      "Gradient Descent(6364/9999): loss=0.04396784869049306\n",
      "Gradient Descent(6365/9999): loss=0.043967848030036435\n",
      "Gradient Descent(6366/9999): loss=0.043967847370618235\n",
      "Gradient Descent(6367/9999): loss=0.04396784671223689\n",
      "Gradient Descent(6368/9999): loss=0.04396784605489075\n",
      "Gradient Descent(6369/9999): loss=0.04396784539857813\n",
      "Gradient Descent(6370/9999): loss=0.04396784474329749\n",
      "Gradient Descent(6371/9999): loss=0.04396784408904716\n",
      "Gradient Descent(6372/9999): loss=0.043967843435825514\n",
      "Gradient Descent(6373/9999): loss=0.04396784278363098\n",
      "Gradient Descent(6374/9999): loss=0.04396784213246191\n",
      "Gradient Descent(6375/9999): loss=0.04396784148231667\n",
      "Gradient Descent(6376/9999): loss=0.043967840833193696\n",
      "Gradient Descent(6377/9999): loss=0.04396784018509134\n",
      "Gradient Descent(6378/9999): loss=0.04396783953800803\n",
      "Gradient Descent(6379/9999): loss=0.04396783889194214\n",
      "Gradient Descent(6380/9999): loss=0.043967838246892106\n",
      "Gradient Descent(6381/9999): loss=0.04396783760285628\n",
      "Gradient Descent(6382/9999): loss=0.043967836959833084\n",
      "Gradient Descent(6383/9999): loss=0.04396783631782098\n",
      "Gradient Descent(6384/9999): loss=0.043967835676818286\n",
      "Gradient Descent(6385/9999): loss=0.043967835036823485\n",
      "Gradient Descent(6386/9999): loss=0.04396783439783497\n",
      "Gradient Descent(6387/9999): loss=0.04396783375985113\n",
      "Gradient Descent(6388/9999): loss=0.04396783312287044\n",
      "Gradient Descent(6389/9999): loss=0.043967832486891296\n",
      "Gradient Descent(6390/9999): loss=0.043967831851912116\n",
      "Gradient Descent(6391/9999): loss=0.043967831217931345\n",
      "Gradient Descent(6392/9999): loss=0.04396783058494739\n",
      "Gradient Descent(6393/9999): loss=0.04396782995295871\n",
      "Gradient Descent(6394/9999): loss=0.043967829321963726\n",
      "Gradient Descent(6395/9999): loss=0.04396782869196085\n",
      "Gradient Descent(6396/9999): loss=0.043967828062948563\n",
      "Gradient Descent(6397/9999): loss=0.04396782743492531\n",
      "Gradient Descent(6398/9999): loss=0.04396782680788949\n",
      "Gradient Descent(6399/9999): loss=0.04396782618183961\n",
      "Gradient Descent(6400/9999): loss=0.04396782555677408\n",
      "Gradient Descent(6401/9999): loss=0.04396782493269133\n",
      "Gradient Descent(6402/9999): loss=0.04396782430958989\n",
      "Gradient Descent(6403/9999): loss=0.04396782368746816\n",
      "Gradient Descent(6404/9999): loss=0.04396782306632461\n",
      "Gradient Descent(6405/9999): loss=0.043967822446157684\n",
      "Gradient Descent(6406/9999): loss=0.043967821826965896\n",
      "Gradient Descent(6407/9999): loss=0.04396782120874769\n",
      "Gradient Descent(6408/9999): loss=0.04396782059150148\n",
      "Gradient Descent(6409/9999): loss=0.043967819975225815\n",
      "Gradient Descent(6410/9999): loss=0.04396781935991915\n",
      "Gradient Descent(6411/9999): loss=0.04396781874557996\n",
      "Gradient Descent(6412/9999): loss=0.043967818132206696\n",
      "Gradient Descent(6413/9999): loss=0.04396781751979786\n",
      "Gradient Descent(6414/9999): loss=0.04396781690835192\n",
      "Gradient Descent(6415/9999): loss=0.04396781629786742\n",
      "Gradient Descent(6416/9999): loss=0.043967815688342765\n",
      "Gradient Descent(6417/9999): loss=0.0439678150797765\n",
      "Gradient Descent(6418/9999): loss=0.043967814472167105\n",
      "Gradient Descent(6419/9999): loss=0.04396781386551308\n",
      "Gradient Descent(6420/9999): loss=0.04396781325981292\n",
      "Gradient Descent(6421/9999): loss=0.0439678126550651\n",
      "Gradient Descent(6422/9999): loss=0.04396781205126816\n",
      "Gradient Descent(6423/9999): loss=0.0439678114484206\n",
      "Gradient Descent(6424/9999): loss=0.0439678108465209\n",
      "Gradient Descent(6425/9999): loss=0.04396781024556758\n",
      "Gradient Descent(6426/9999): loss=0.04396780964555919\n",
      "Gradient Descent(6427/9999): loss=0.04396780904649417\n",
      "Gradient Descent(6428/9999): loss=0.04396780844837112\n",
      "Gradient Descent(6429/9999): loss=0.043967807851188506\n",
      "Gradient Descent(6430/9999): loss=0.043967807254944855\n",
      "Gradient Descent(6431/9999): loss=0.04396780665963869\n",
      "Gradient Descent(6432/9999): loss=0.043967806065268566\n",
      "Gradient Descent(6433/9999): loss=0.043967805471832974\n",
      "Gradient Descent(6434/9999): loss=0.04396780487933047\n",
      "Gradient Descent(6435/9999): loss=0.043967804287759565\n",
      "Gradient Descent(6436/9999): loss=0.043967803697118814\n",
      "Gradient Descent(6437/9999): loss=0.04396780310740676\n",
      "Gradient Descent(6438/9999): loss=0.04396780251862191\n",
      "Gradient Descent(6439/9999): loss=0.04396780193076283\n",
      "Gradient Descent(6440/9999): loss=0.04396780134382807\n",
      "Gradient Descent(6441/9999): loss=0.04396780075781617\n",
      "Gradient Descent(6442/9999): loss=0.04396780017272567\n",
      "Gradient Descent(6443/9999): loss=0.04396779958855511\n",
      "Gradient Descent(6444/9999): loss=0.043967799005303065\n",
      "Gradient Descent(6445/9999): loss=0.043967798422968093\n",
      "Gradient Descent(6446/9999): loss=0.04396779784154878\n",
      "Gradient Descent(6447/9999): loss=0.04396779726104361\n",
      "Gradient Descent(6448/9999): loss=0.043967796681451184\n",
      "Gradient Descent(6449/9999): loss=0.04396779610277008\n",
      "Gradient Descent(6450/9999): loss=0.04396779552499888\n",
      "Gradient Descent(6451/9999): loss=0.043967794948136095\n",
      "Gradient Descent(6452/9999): loss=0.043967794372180354\n",
      "Gradient Descent(6453/9999): loss=0.0439677937971302\n",
      "Gradient Descent(6454/9999): loss=0.04396779322298421\n",
      "Gradient Descent(6455/9999): loss=0.043967792649740954\n",
      "Gradient Descent(6456/9999): loss=0.043967792077399065\n",
      "Gradient Descent(6457/9999): loss=0.04396779150595705\n",
      "Gradient Descent(6458/9999): loss=0.043967790935413545\n",
      "Gradient Descent(6459/9999): loss=0.043967790365767145\n",
      "Gradient Descent(6460/9999): loss=0.04396778979701639\n",
      "Gradient Descent(6461/9999): loss=0.04396778922915992\n",
      "Gradient Descent(6462/9999): loss=0.04396778866219633\n",
      "Gradient Descent(6463/9999): loss=0.043967788096124144\n",
      "Gradient Descent(6464/9999): loss=0.04396778753094205\n",
      "Gradient Descent(6465/9999): loss=0.0439677869666486\n",
      "Gradient Descent(6466/9999): loss=0.04396778640324241\n",
      "Gradient Descent(6467/9999): loss=0.04396778584072207\n",
      "Gradient Descent(6468/9999): loss=0.043967785279086194\n",
      "Gradient Descent(6469/9999): loss=0.04396778471833343\n",
      "Gradient Descent(6470/9999): loss=0.04396778415846232\n",
      "Gradient Descent(6471/9999): loss=0.04396778359947154\n",
      "Gradient Descent(6472/9999): loss=0.04396778304135966\n",
      "Gradient Descent(6473/9999): loss=0.04396778248412533\n",
      "Gradient Descent(6474/9999): loss=0.04396778192776716\n",
      "Gradient Descent(6475/9999): loss=0.043967781372283764\n",
      "Gradient Descent(6476/9999): loss=0.04396778081767379\n",
      "Gradient Descent(6477/9999): loss=0.04396778026393583\n",
      "Gradient Descent(6478/9999): loss=0.04396777971106852\n",
      "Gradient Descent(6479/9999): loss=0.043967779159070526\n",
      "Gradient Descent(6480/9999): loss=0.04396777860794046\n",
      "Gradient Descent(6481/9999): loss=0.04396777805767691\n",
      "Gradient Descent(6482/9999): loss=0.043967777508278606\n",
      "Gradient Descent(6483/9999): loss=0.043967776959744125\n",
      "Gradient Descent(6484/9999): loss=0.04396777641207211\n",
      "Gradient Descent(6485/9999): loss=0.04396777586526122\n",
      "Gradient Descent(6486/9999): loss=0.043967775319310086\n",
      "Gradient Descent(6487/9999): loss=0.043967774774217414\n",
      "Gradient Descent(6488/9999): loss=0.04396777422998175\n",
      "Gradient Descent(6489/9999): loss=0.043967773686601846\n",
      "Gradient Descent(6490/9999): loss=0.04396777314407632\n",
      "Gradient Descent(6491/9999): loss=0.043967772602403817\n",
      "Gradient Descent(6492/9999): loss=0.043967772061583\n",
      "Gradient Descent(6493/9999): loss=0.043967771521612534\n",
      "Gradient Descent(6494/9999): loss=0.043967770982491085\n",
      "Gradient Descent(6495/9999): loss=0.0439677704442173\n",
      "Gradient Descent(6496/9999): loss=0.04396776990678987\n",
      "Gradient Descent(6497/9999): loss=0.04396776937020745\n",
      "Gradient Descent(6498/9999): loss=0.04396776883446873\n",
      "Gradient Descent(6499/9999): loss=0.04396776829957238\n",
      "Gradient Descent(6500/9999): loss=0.04396776776551702\n",
      "Gradient Descent(6501/9999): loss=0.043967767232301405\n",
      "Gradient Descent(6502/9999): loss=0.04396776669992417\n",
      "Gradient Descent(6503/9999): loss=0.043967766168384034\n",
      "Gradient Descent(6504/9999): loss=0.04396776563767963\n",
      "Gradient Descent(6505/9999): loss=0.043967765107809684\n",
      "Gradient Descent(6506/9999): loss=0.043967764578772855\n",
      "Gradient Descent(6507/9999): loss=0.04396776405056786\n",
      "Gradient Descent(6508/9999): loss=0.04396776352319337\n",
      "Gradient Descent(6509/9999): loss=0.043967762996648105\n",
      "Gradient Descent(6510/9999): loss=0.04396776247093073\n",
      "Gradient Descent(6511/9999): loss=0.04396776194603998\n",
      "Gradient Descent(6512/9999): loss=0.043967761421974505\n",
      "Gradient Descent(6513/9999): loss=0.04396776089873306\n",
      "Gradient Descent(6514/9999): loss=0.043967760376314294\n",
      "Gradient Descent(6515/9999): loss=0.04396775985471699\n",
      "Gradient Descent(6516/9999): loss=0.04396775933393975\n",
      "Gradient Descent(6517/9999): loss=0.04396775881398141\n",
      "Gradient Descent(6518/9999): loss=0.043967758294840564\n",
      "Gradient Descent(6519/9999): loss=0.04396775777651601\n",
      "Gradient Descent(6520/9999): loss=0.04396775725900642\n",
      "Gradient Descent(6521/9999): loss=0.04396775674231052\n",
      "Gradient Descent(6522/9999): loss=0.04396775622642708\n",
      "Gradient Descent(6523/9999): loss=0.04396775571135473\n",
      "Gradient Descent(6524/9999): loss=0.04396775519709228\n",
      "Gradient Descent(6525/9999): loss=0.0439677546836384\n",
      "Gradient Descent(6526/9999): loss=0.04396775417099187\n",
      "Gradient Descent(6527/9999): loss=0.043967753659151364\n",
      "Gradient Descent(6528/9999): loss=0.04396775314811566\n",
      "Gradient Descent(6529/9999): loss=0.043967752637883446\n",
      "Gradient Descent(6530/9999): loss=0.04396775212845351\n",
      "Gradient Descent(6531/9999): loss=0.04396775161982456\n",
      "Gradient Descent(6532/9999): loss=0.04396775111199536\n",
      "Gradient Descent(6533/9999): loss=0.04396775060496462\n",
      "Gradient Descent(6534/9999): loss=0.04396775009873109\n",
      "Gradient Descent(6535/9999): loss=0.043967749593293554\n",
      "Gradient Descent(6536/9999): loss=0.04396774908865071\n",
      "Gradient Descent(6537/9999): loss=0.04396774858480135\n",
      "Gradient Descent(6538/9999): loss=0.0439677480817442\n",
      "Gradient Descent(6539/9999): loss=0.04396774757947802\n",
      "Gradient Descent(6540/9999): loss=0.04396774707800161\n",
      "Gradient Descent(6541/9999): loss=0.043967746577313636\n",
      "Gradient Descent(6542/9999): loss=0.043967746077412925\n",
      "Gradient Descent(6543/9999): loss=0.04396774557829823\n",
      "Gradient Descent(6544/9999): loss=0.04396774507996832\n",
      "Gradient Descent(6545/9999): loss=0.04396774458242191\n",
      "Gradient Descent(6546/9999): loss=0.043967744085657866\n",
      "Gradient Descent(6547/9999): loss=0.04396774358967487\n",
      "Gradient Descent(6548/9999): loss=0.04396774309447172\n",
      "Gradient Descent(6549/9999): loss=0.043967742600047204\n",
      "Gradient Descent(6550/9999): loss=0.04396774210640007\n",
      "Gradient Descent(6551/9999): loss=0.04396774161352914\n",
      "Gradient Descent(6552/9999): loss=0.043967741121433146\n",
      "Gradient Descent(6553/9999): loss=0.04396774063011092\n",
      "Gradient Descent(6554/9999): loss=0.04396774013956118\n",
      "Gradient Descent(6555/9999): loss=0.043967739649782775\n",
      "Gradient Descent(6556/9999): loss=0.043967739160774455\n",
      "Gradient Descent(6557/9999): loss=0.043967738672535006\n",
      "Gradient Descent(6558/9999): loss=0.04396773818506325\n",
      "Gradient Descent(6559/9999): loss=0.04396773769835795\n",
      "Gradient Descent(6560/9999): loss=0.04396773721241791\n",
      "Gradient Descent(6561/9999): loss=0.04396773672724195\n",
      "Gradient Descent(6562/9999): loss=0.043967736242828834\n",
      "Gradient Descent(6563/9999): loss=0.04396773575917738\n",
      "Gradient Descent(6564/9999): loss=0.04396773527628639\n",
      "Gradient Descent(6565/9999): loss=0.04396773479415466\n",
      "Gradient Descent(6566/9999): loss=0.043967734312781\n",
      "Gradient Descent(6567/9999): loss=0.04396773383216423\n",
      "Gradient Descent(6568/9999): loss=0.04396773335230313\n",
      "Gradient Descent(6569/9999): loss=0.043967732873196566\n",
      "Gradient Descent(6570/9999): loss=0.043967732394843285\n",
      "Gradient Descent(6571/9999): loss=0.04396773191724214\n",
      "Gradient Descent(6572/9999): loss=0.04396773144039194\n",
      "Gradient Descent(6573/9999): loss=0.04396773096429154\n",
      "Gradient Descent(6574/9999): loss=0.043967730488939705\n",
      "Gradient Descent(6575/9999): loss=0.04396773001433526\n",
      "Gradient Descent(6576/9999): loss=0.04396772954047709\n",
      "Gradient Descent(6577/9999): loss=0.04396772906736396\n",
      "Gradient Descent(6578/9999): loss=0.04396772859499472\n",
      "Gradient Descent(6579/9999): loss=0.043967728123368216\n",
      "Gradient Descent(6580/9999): loss=0.04396772765248325\n",
      "Gradient Descent(6581/9999): loss=0.04396772718233868\n",
      "Gradient Descent(6582/9999): loss=0.04396772671293334\n",
      "Gradient Descent(6583/9999): loss=0.043967726244266064\n",
      "Gradient Descent(6584/9999): loss=0.04396772577633566\n",
      "Gradient Descent(6585/9999): loss=0.04396772530914104\n",
      "Gradient Descent(6586/9999): loss=0.04396772484268099\n",
      "Gradient Descent(6587/9999): loss=0.04396772437695434\n",
      "Gradient Descent(6588/9999): loss=0.04396772391196\n",
      "Gradient Descent(6589/9999): loss=0.04396772344769678\n",
      "Gradient Descent(6590/9999): loss=0.043967722984163536\n",
      "Gradient Descent(6591/9999): loss=0.043967722521359116\n",
      "Gradient Descent(6592/9999): loss=0.043967722059282385\n",
      "Gradient Descent(6593/9999): loss=0.04396772159793219\n",
      "Gradient Descent(6594/9999): loss=0.04396772113730739\n",
      "Gradient Descent(6595/9999): loss=0.04396772067740685\n",
      "Gradient Descent(6596/9999): loss=0.04396772021822942\n",
      "Gradient Descent(6597/9999): loss=0.043967719759773975\n",
      "Gradient Descent(6598/9999): loss=0.04396771930203936\n",
      "Gradient Descent(6599/9999): loss=0.04396771884502448\n",
      "Gradient Descent(6600/9999): loss=0.04396771838872818\n",
      "Gradient Descent(6601/9999): loss=0.043967717933149304\n",
      "Gradient Descent(6602/9999): loss=0.043967717478286775\n",
      "Gradient Descent(6603/9999): loss=0.04396771702413942\n",
      "Gradient Descent(6604/9999): loss=0.04396771657070613\n",
      "Gradient Descent(6605/9999): loss=0.04396771611798582\n",
      "Gradient Descent(6606/9999): loss=0.043967715665977326\n",
      "Gradient Descent(6607/9999): loss=0.043967715214679536\n",
      "Gradient Descent(6608/9999): loss=0.04396771476409133\n",
      "Gradient Descent(6609/9999): loss=0.04396771431421161\n",
      "Gradient Descent(6610/9999): loss=0.043967713865039254\n",
      "Gradient Descent(6611/9999): loss=0.04396771341657312\n",
      "Gradient Descent(6612/9999): loss=0.04396771296881215\n",
      "Gradient Descent(6613/9999): loss=0.043967712521755214\n",
      "Gradient Descent(6614/9999): loss=0.04396771207540118\n",
      "Gradient Descent(6615/9999): loss=0.04396771162974897\n",
      "Gradient Descent(6616/9999): loss=0.04396771118479746\n",
      "Gradient Descent(6617/9999): loss=0.04396771074054559\n",
      "Gradient Descent(6618/9999): loss=0.043967710296992236\n",
      "Gradient Descent(6619/9999): loss=0.04396770985413626\n",
      "Gradient Descent(6620/9999): loss=0.04396770941197663\n",
      "Gradient Descent(6621/9999): loss=0.0439677089705122\n",
      "Gradient Descent(6622/9999): loss=0.04396770852974192\n",
      "Gradient Descent(6623/9999): loss=0.043967708089664666\n",
      "Gradient Descent(6624/9999): loss=0.043967707650279374\n",
      "Gradient Descent(6625/9999): loss=0.04396770721158492\n",
      "Gradient Descent(6626/9999): loss=0.04396770677358026\n",
      "Gradient Descent(6627/9999): loss=0.04396770633626427\n",
      "Gradient Descent(6628/9999): loss=0.0439677058996359\n",
      "Gradient Descent(6629/9999): loss=0.04396770546369404\n",
      "Gradient Descent(6630/9999): loss=0.04396770502843766\n",
      "Gradient Descent(6631/9999): loss=0.0439677045938656\n",
      "Gradient Descent(6632/9999): loss=0.04396770415997687\n",
      "Gradient Descent(6633/9999): loss=0.043967703726770335\n",
      "Gradient Descent(6634/9999): loss=0.043967703294244966\n",
      "Gradient Descent(6635/9999): loss=0.04396770286239965\n",
      "Gradient Descent(6636/9999): loss=0.04396770243123336\n",
      "Gradient Descent(6637/9999): loss=0.04396770200074498\n",
      "Gradient Descent(6638/9999): loss=0.0439677015709335\n",
      "Gradient Descent(6639/9999): loss=0.043967701141797814\n",
      "Gradient Descent(6640/9999): loss=0.04396770071333687\n",
      "Gradient Descent(6641/9999): loss=0.0439677002855496\n",
      "Gradient Descent(6642/9999): loss=0.04396769985843498\n",
      "Gradient Descent(6643/9999): loss=0.043967699431991907\n",
      "Gradient Descent(6644/9999): loss=0.04396769900621934\n",
      "Gradient Descent(6645/9999): loss=0.043967698581116245\n",
      "Gradient Descent(6646/9999): loss=0.04396769815668155\n",
      "Gradient Descent(6647/9999): loss=0.0439676977329142\n",
      "Gradient Descent(6648/9999): loss=0.043967697309813154\n",
      "Gradient Descent(6649/9999): loss=0.04396769688737738\n",
      "Gradient Descent(6650/9999): loss=0.043967696465605804\n",
      "Gradient Descent(6651/9999): loss=0.04396769604449738\n",
      "Gradient Descent(6652/9999): loss=0.04396769562405112\n",
      "Gradient Descent(6653/9999): loss=0.04396769520426589\n",
      "Gradient Descent(6654/9999): loss=0.043967694785140725\n",
      "Gradient Descent(6655/9999): loss=0.04396769436667457\n",
      "Gradient Descent(6656/9999): loss=0.043967693948866376\n",
      "Gradient Descent(6657/9999): loss=0.043967693531715125\n",
      "Gradient Descent(6658/9999): loss=0.04396769311521977\n",
      "Gradient Descent(6659/9999): loss=0.04396769269937928\n",
      "Gradient Descent(6660/9999): loss=0.04396769228419263\n",
      "Gradient Descent(6661/9999): loss=0.043967691869658806\n",
      "Gradient Descent(6662/9999): loss=0.04396769145577673\n",
      "Gradient Descent(6663/9999): loss=0.04396769104254544\n",
      "Gradient Descent(6664/9999): loss=0.043967690629963904\n",
      "Gradient Descent(6665/9999): loss=0.04396769021803106\n",
      "Gradient Descent(6666/9999): loss=0.04396768980674592\n",
      "Gradient Descent(6667/9999): loss=0.04396768939610744\n",
      "Gradient Descent(6668/9999): loss=0.04396768898611465\n",
      "Gradient Descent(6669/9999): loss=0.04396768857676648\n",
      "Gradient Descent(6670/9999): loss=0.04396768816806196\n",
      "Gradient Descent(6671/9999): loss=0.043967687760000033\n",
      "Gradient Descent(6672/9999): loss=0.04396768735257974\n",
      "Gradient Descent(6673/9999): loss=0.04396768694580004\n",
      "Gradient Descent(6674/9999): loss=0.04396768653965994\n",
      "Gradient Descent(6675/9999): loss=0.04396768613415844\n",
      "Gradient Descent(6676/9999): loss=0.04396768572929449\n",
      "Gradient Descent(6677/9999): loss=0.04396768532506714\n",
      "Gradient Descent(6678/9999): loss=0.04396768492147535\n",
      "Gradient Descent(6679/9999): loss=0.04396768451851817\n",
      "Gradient Descent(6680/9999): loss=0.04396768411619457\n",
      "Gradient Descent(6681/9999): loss=0.04396768371450354\n",
      "Gradient Descent(6682/9999): loss=0.04396768331344412\n",
      "Gradient Descent(6683/9999): loss=0.043967682913015275\n",
      "Gradient Descent(6684/9999): loss=0.04396768251321606\n",
      "Gradient Descent(6685/9999): loss=0.04396768211404544\n",
      "Gradient Descent(6686/9999): loss=0.043967681715502464\n",
      "Gradient Descent(6687/9999): loss=0.04396768131758613\n",
      "Gradient Descent(6688/9999): loss=0.04396768092029544\n",
      "Gradient Descent(6689/9999): loss=0.04396768052362945\n",
      "Gradient Descent(6690/9999): loss=0.04396768012758713\n",
      "Gradient Descent(6691/9999): loss=0.04396767973216753\n",
      "Gradient Descent(6692/9999): loss=0.043967679337369656\n",
      "Gradient Descent(6693/9999): loss=0.04396767894319255\n",
      "Gradient Descent(6694/9999): loss=0.0439676785496352\n",
      "Gradient Descent(6695/9999): loss=0.04396767815669666\n",
      "Gradient Descent(6696/9999): loss=0.04396767776437596\n",
      "Gradient Descent(6697/9999): loss=0.043967677372672084\n",
      "Gradient Descent(6698/9999): loss=0.04396767698158415\n",
      "Gradient Descent(6699/9999): loss=0.0439676765911111\n",
      "Gradient Descent(6700/9999): loss=0.04396767620125201\n",
      "Gradient Descent(6701/9999): loss=0.04396767581200592\n",
      "Gradient Descent(6702/9999): loss=0.043967675423371856\n",
      "Gradient Descent(6703/9999): loss=0.04396767503534885\n",
      "Gradient Descent(6704/9999): loss=0.04396767464793593\n",
      "Gradient Descent(6705/9999): loss=0.04396767426113216\n",
      "Gradient Descent(6706/9999): loss=0.04396767387493659\n",
      "Gradient Descent(6707/9999): loss=0.04396767348934822\n",
      "Gradient Descent(6708/9999): loss=0.04396767310436614\n",
      "Gradient Descent(6709/9999): loss=0.043967672719989394\n",
      "Gradient Descent(6710/9999): loss=0.043967672336217005\n",
      "Gradient Descent(6711/9999): loss=0.04396767195304804\n",
      "Gradient Descent(6712/9999): loss=0.04396767157048151\n",
      "Gradient Descent(6713/9999): loss=0.04396767118851655\n",
      "Gradient Descent(6714/9999): loss=0.043967670807152116\n",
      "Gradient Descent(6715/9999): loss=0.04396767042638736\n",
      "Gradient Descent(6716/9999): loss=0.04396767004622126\n",
      "Gradient Descent(6717/9999): loss=0.04396766966665293\n",
      "Gradient Descent(6718/9999): loss=0.043967669287681393\n",
      "Gradient Descent(6719/9999): loss=0.04396766890930573\n",
      "Gradient Descent(6720/9999): loss=0.04396766853152501\n",
      "Gradient Descent(6721/9999): loss=0.04396766815433829\n",
      "Gradient Descent(6722/9999): loss=0.04396766777774461\n",
      "Gradient Descent(6723/9999): loss=0.04396766740174307\n",
      "Gradient Descent(6724/9999): loss=0.04396766702633272\n",
      "Gradient Descent(6725/9999): loss=0.043967666651512666\n",
      "Gradient Descent(6726/9999): loss=0.043967666277281936\n",
      "Gradient Descent(6727/9999): loss=0.04396766590363962\n",
      "Gradient Descent(6728/9999): loss=0.0439676655305848\n",
      "Gradient Descent(6729/9999): loss=0.043967665158116544\n",
      "Gradient Descent(6730/9999): loss=0.04396766478623393\n",
      "Gradient Descent(6731/9999): loss=0.043967664414936054\n",
      "Gradient Descent(6732/9999): loss=0.043967664044221956\n",
      "Gradient Descent(6733/9999): loss=0.04396766367409079\n",
      "Gradient Descent(6734/9999): loss=0.04396766330454155\n",
      "Gradient Descent(6735/9999): loss=0.04396766293557339\n",
      "Gradient Descent(6736/9999): loss=0.043967662567185345\n",
      "Gradient Descent(6737/9999): loss=0.04396766219937654\n",
      "Gradient Descent(6738/9999): loss=0.04396766183214606\n",
      "Gradient Descent(6739/9999): loss=0.043967661465492976\n",
      "Gradient Descent(6740/9999): loss=0.043967661099416404\n",
      "Gradient Descent(6741/9999): loss=0.04396766073391543\n",
      "Gradient Descent(6742/9999): loss=0.04396766036898914\n",
      "Gradient Descent(6743/9999): loss=0.04396766000463663\n",
      "Gradient Descent(6744/9999): loss=0.04396765964085701\n",
      "Gradient Descent(6745/9999): loss=0.04396765927764937\n",
      "Gradient Descent(6746/9999): loss=0.043967658915012794\n",
      "Gradient Descent(6747/9999): loss=0.04396765855294643\n",
      "Gradient Descent(6748/9999): loss=0.04396765819144934\n",
      "Gradient Descent(6749/9999): loss=0.04396765783052066\n",
      "Gradient Descent(6750/9999): loss=0.043967657470159474\n",
      "Gradient Descent(6751/9999): loss=0.0439676571103649\n",
      "Gradient Descent(6752/9999): loss=0.04396765675113603\n",
      "Gradient Descent(6753/9999): loss=0.043967656392471986\n",
      "Gradient Descent(6754/9999): loss=0.0439676560343719\n",
      "Gradient Descent(6755/9999): loss=0.04396765567683486\n",
      "Gradient Descent(6756/9999): loss=0.043967655319859966\n",
      "Gradient Descent(6757/9999): loss=0.043967654963446366\n",
      "Gradient Descent(6758/9999): loss=0.04396765460759319\n",
      "Gradient Descent(6759/9999): loss=0.043967654252299516\n",
      "Gradient Descent(6760/9999): loss=0.043967653897564477\n",
      "Gradient Descent(6761/9999): loss=0.04396765354338721\n",
      "Gradient Descent(6762/9999): loss=0.043967653189766816\n",
      "Gradient Descent(6763/9999): loss=0.04396765283670244\n",
      "Gradient Descent(6764/9999): loss=0.0439676524841932\n",
      "Gradient Descent(6765/9999): loss=0.043967652132238216\n",
      "Gradient Descent(6766/9999): loss=0.043967651780836635\n",
      "Gradient Descent(6767/9999): loss=0.04396765142998756\n",
      "Gradient Descent(6768/9999): loss=0.04396765107969013\n",
      "Gradient Descent(6769/9999): loss=0.04396765072994352\n",
      "Gradient Descent(6770/9999): loss=0.043967650380746785\n",
      "Gradient Descent(6771/9999): loss=0.04396765003209915\n",
      "Gradient Descent(6772/9999): loss=0.043967649683999674\n",
      "Gradient Descent(6773/9999): loss=0.043967649336447506\n",
      "Gradient Descent(6774/9999): loss=0.04396764898944183\n",
      "Gradient Descent(6775/9999): loss=0.04396764864298177\n",
      "Gradient Descent(6776/9999): loss=0.043967648297066446\n",
      "Gradient Descent(6777/9999): loss=0.04396764795169505\n",
      "Gradient Descent(6778/9999): loss=0.04396764760686666\n",
      "Gradient Descent(6779/9999): loss=0.04396764726258045\n",
      "Gradient Descent(6780/9999): loss=0.043967646918835576\n",
      "Gradient Descent(6781/9999): loss=0.04396764657563119\n",
      "Gradient Descent(6782/9999): loss=0.04396764623296646\n",
      "Gradient Descent(6783/9999): loss=0.043967645890840484\n",
      "Gradient Descent(6784/9999): loss=0.04396764554925243\n",
      "Gradient Descent(6785/9999): loss=0.04396764520820149\n",
      "Gradient Descent(6786/9999): loss=0.04396764486768678\n",
      "Gradient Descent(6787/9999): loss=0.04396764452770751\n",
      "Gradient Descent(6788/9999): loss=0.04396764418826276\n",
      "Gradient Descent(6789/9999): loss=0.04396764384935176\n",
      "Gradient Descent(6790/9999): loss=0.04396764351097362\n",
      "Gradient Descent(6791/9999): loss=0.043967643173127534\n",
      "Gradient Descent(6792/9999): loss=0.043967642835812636\n",
      "Gradient Descent(6793/9999): loss=0.04396764249902811\n",
      "Gradient Descent(6794/9999): loss=0.04396764216277314\n",
      "Gradient Descent(6795/9999): loss=0.0439676418270469\n",
      "Gradient Descent(6796/9999): loss=0.043967641491848494\n",
      "Gradient Descent(6797/9999): loss=0.04396764115717712\n",
      "Gradient Descent(6798/9999): loss=0.04396764082303198\n",
      "Gradient Descent(6799/9999): loss=0.04396764048941224\n",
      "Gradient Descent(6800/9999): loss=0.04396764015631705\n",
      "Gradient Descent(6801/9999): loss=0.0439676398237456\n",
      "Gradient Descent(6802/9999): loss=0.043967639491697054\n",
      "Gradient Descent(6803/9999): loss=0.0439676391601706\n",
      "Gradient Descent(6804/9999): loss=0.043967638829165434\n",
      "Gradient Descent(6805/9999): loss=0.04396763849868071\n",
      "Gradient Descent(6806/9999): loss=0.0439676381687156\n",
      "Gradient Descent(6807/9999): loss=0.043967637839269325\n",
      "Gradient Descent(6808/9999): loss=0.04396763751034103\n",
      "Gradient Descent(6809/9999): loss=0.04396763718192994\n",
      "Gradient Descent(6810/9999): loss=0.043967636854035215\n",
      "Gradient Descent(6811/9999): loss=0.04396763652665604\n",
      "Gradient Descent(6812/9999): loss=0.04396763619979164\n",
      "Gradient Descent(6813/9999): loss=0.04396763587344116\n",
      "Gradient Descent(6814/9999): loss=0.04396763554760381\n",
      "Gradient Descent(6815/9999): loss=0.043967635222278784\n",
      "Gradient Descent(6816/9999): loss=0.043967634897465295\n",
      "Gradient Descent(6817/9999): loss=0.043967634573162495\n",
      "Gradient Descent(6818/9999): loss=0.04396763424936964\n",
      "Gradient Descent(6819/9999): loss=0.04396763392608587\n",
      "Gradient Descent(6820/9999): loss=0.04396763360331041\n",
      "Gradient Descent(6821/9999): loss=0.04396763328104246\n",
      "Gradient Descent(6822/9999): loss=0.04396763295928124\n",
      "Gradient Descent(6823/9999): loss=0.04396763263802592\n",
      "Gradient Descent(6824/9999): loss=0.043967632317275705\n",
      "Gradient Descent(6825/9999): loss=0.04396763199702985\n",
      "Gradient Descent(6826/9999): loss=0.04396763167728752\n",
      "Gradient Descent(6827/9999): loss=0.04396763135804791\n",
      "Gradient Descent(6828/9999): loss=0.04396763103931027\n",
      "Gradient Descent(6829/9999): loss=0.043967630721073785\n",
      "Gradient Descent(6830/9999): loss=0.04396763040333769\n",
      "Gradient Descent(6831/9999): loss=0.04396763008610116\n",
      "Gradient Descent(6832/9999): loss=0.04396762976936344\n",
      "Gradient Descent(6833/9999): loss=0.04396762945312375\n",
      "Gradient Descent(6834/9999): loss=0.04396762913738127\n",
      "Gradient Descent(6835/9999): loss=0.043967628822135243\n",
      "Gradient Descent(6836/9999): loss=0.04396762850738492\n",
      "Gradient Descent(6837/9999): loss=0.04396762819312947\n",
      "Gradient Descent(6838/9999): loss=0.04396762787936812\n",
      "Gradient Descent(6839/9999): loss=0.04396762756610012\n",
      "Gradient Descent(6840/9999): loss=0.043967627253324676\n",
      "Gradient Descent(6841/9999): loss=0.04396762694104103\n",
      "Gradient Descent(6842/9999): loss=0.043967626629248385\n",
      "Gradient Descent(6843/9999): loss=0.043967626317946\n",
      "Gradient Descent(6844/9999): loss=0.043967626007133054\n",
      "Gradient Descent(6845/9999): loss=0.04396762569680884\n",
      "Gradient Descent(6846/9999): loss=0.04396762538697255\n",
      "Gradient Descent(6847/9999): loss=0.04396762507762341\n",
      "Gradient Descent(6848/9999): loss=0.0439676247687607\n",
      "Gradient Descent(6849/9999): loss=0.04396762446038362\n",
      "Gradient Descent(6850/9999): loss=0.04396762415249142\n",
      "Gradient Descent(6851/9999): loss=0.04396762384508327\n",
      "Gradient Descent(6852/9999): loss=0.04396762353815853\n",
      "Gradient Descent(6853/9999): loss=0.043967623231716345\n",
      "Gradient Descent(6854/9999): loss=0.04396762292575602\n",
      "Gradient Descent(6855/9999): loss=0.04396762262027676\n",
      "Gradient Descent(6856/9999): loss=0.043967622315277774\n",
      "Gradient Descent(6857/9999): loss=0.04396762201075839\n",
      "Gradient Descent(6858/9999): loss=0.04396762170671779\n",
      "Gradient Descent(6859/9999): loss=0.04396762140315525\n",
      "Gradient Descent(6860/9999): loss=0.043967621100070034\n",
      "Gradient Descent(6861/9999): loss=0.04396762079746133\n",
      "Gradient Descent(6862/9999): loss=0.04396762049532845\n",
      "Gradient Descent(6863/9999): loss=0.04396762019367063\n",
      "Gradient Descent(6864/9999): loss=0.043967619892487105\n",
      "Gradient Descent(6865/9999): loss=0.04396761959177712\n",
      "Gradient Descent(6866/9999): loss=0.043967619291539986\n",
      "Gradient Descent(6867/9999): loss=0.043967618991774905\n",
      "Gradient Descent(6868/9999): loss=0.04396761869248117\n",
      "Gradient Descent(6869/9999): loss=0.04396761839365802\n",
      "Gradient Descent(6870/9999): loss=0.04396761809530471\n",
      "Gradient Descent(6871/9999): loss=0.04396761779742052\n",
      "Gradient Descent(6872/9999): loss=0.04396761750000469\n",
      "Gradient Descent(6873/9999): loss=0.0439676172030565\n",
      "Gradient Descent(6874/9999): loss=0.04396761690657522\n",
      "Gradient Descent(6875/9999): loss=0.043967616610560104\n",
      "Gradient Descent(6876/9999): loss=0.04396761631501042\n",
      "Gradient Descent(6877/9999): loss=0.04396761601992544\n",
      "Gradient Descent(6878/9999): loss=0.04396761572530443\n",
      "Gradient Descent(6879/9999): loss=0.043967615431146675\n",
      "Gradient Descent(6880/9999): loss=0.04396761513745144\n",
      "Gradient Descent(6881/9999): loss=0.04396761484421797\n",
      "Gradient Descent(6882/9999): loss=0.043967614551445565\n",
      "Gradient Descent(6883/9999): loss=0.04396761425913351\n",
      "Gradient Descent(6884/9999): loss=0.04396761396728104\n",
      "Gradient Descent(6885/9999): loss=0.04396761367588747\n",
      "Gradient Descent(6886/9999): loss=0.04396761338495207\n",
      "Gradient Descent(6887/9999): loss=0.04396761309447414\n",
      "Gradient Descent(6888/9999): loss=0.043967612804452896\n",
      "Gradient Descent(6889/9999): loss=0.04396761251488769\n",
      "Gradient Descent(6890/9999): loss=0.04396761222577778\n",
      "Gradient Descent(6891/9999): loss=0.043967611937122425\n",
      "Gradient Descent(6892/9999): loss=0.04396761164892094\n",
      "Gradient Descent(6893/9999): loss=0.04396761136117262\n",
      "Gradient Descent(6894/9999): loss=0.043967611073876735\n",
      "Gradient Descent(6895/9999): loss=0.04396761078703256\n",
      "Gradient Descent(6896/9999): loss=0.0439676105006394\n",
      "Gradient Descent(6897/9999): loss=0.04396761021469654\n",
      "Gradient Descent(6898/9999): loss=0.0439676099292033\n",
      "Gradient Descent(6899/9999): loss=0.043967609644158925\n",
      "Gradient Descent(6900/9999): loss=0.043967609359562745\n",
      "Gradient Descent(6901/9999): loss=0.04396760907541406\n",
      "Gradient Descent(6902/9999): loss=0.04396760879171214\n",
      "Gradient Descent(6903/9999): loss=0.04396760850845628\n",
      "Gradient Descent(6904/9999): loss=0.04396760822564581\n",
      "Gradient Descent(6905/9999): loss=0.04396760794328\n",
      "Gradient Descent(6906/9999): loss=0.04396760766135818\n",
      "Gradient Descent(6907/9999): loss=0.04396760737987963\n",
      "Gradient Descent(6908/9999): loss=0.04396760709884365\n",
      "Gradient Descent(6909/9999): loss=0.04396760681824956\n",
      "Gradient Descent(6910/9999): loss=0.04396760653809664\n",
      "Gradient Descent(6911/9999): loss=0.04396760625838423\n",
      "Gradient Descent(6912/9999): loss=0.04396760597911164\n",
      "Gradient Descent(6913/9999): loss=0.04396760570027811\n",
      "Gradient Descent(6914/9999): loss=0.04396760542188303\n",
      "Gradient Descent(6915/9999): loss=0.04396760514392569\n",
      "Gradient Descent(6916/9999): loss=0.04396760486640537\n",
      "Gradient Descent(6917/9999): loss=0.043967604589321405\n",
      "Gradient Descent(6918/9999): loss=0.04396760431267311\n",
      "Gradient Descent(6919/9999): loss=0.04396760403645982\n",
      "Gradient Descent(6920/9999): loss=0.043967603760680804\n",
      "Gradient Descent(6921/9999): loss=0.04396760348533542\n",
      "Gradient Descent(6922/9999): loss=0.043967603210422965\n",
      "Gradient Descent(6923/9999): loss=0.04396760293594276\n",
      "Gradient Descent(6924/9999): loss=0.04396760266189414\n",
      "Gradient Descent(6925/9999): loss=0.043967602388276406\n",
      "Gradient Descent(6926/9999): loss=0.04396760211508889\n",
      "Gradient Descent(6927/9999): loss=0.043967601842330915\n",
      "Gradient Descent(6928/9999): loss=0.043967601570001805\n",
      "Gradient Descent(6929/9999): loss=0.0439676012981009\n",
      "Gradient Descent(6930/9999): loss=0.04396760102662751\n",
      "Gradient Descent(6931/9999): loss=0.043967600755580954\n",
      "Gradient Descent(6932/9999): loss=0.04396760048496058\n",
      "Gradient Descent(6933/9999): loss=0.043967600214765715\n",
      "Gradient Descent(6934/9999): loss=0.043967599944995675\n",
      "Gradient Descent(6935/9999): loss=0.043967599675649824\n",
      "Gradient Descent(6936/9999): loss=0.043967599406727474\n",
      "Gradient Descent(6937/9999): loss=0.04396759913822794\n",
      "Gradient Descent(6938/9999): loss=0.04396759887015059\n",
      "Gradient Descent(6939/9999): loss=0.043967598602494745\n",
      "Gradient Descent(6940/9999): loss=0.04396759833525972\n",
      "Gradient Descent(6941/9999): loss=0.0439675980684449\n",
      "Gradient Descent(6942/9999): loss=0.043967597802049604\n",
      "Gradient Descent(6943/9999): loss=0.04396759753607316\n",
      "Gradient Descent(6944/9999): loss=0.043967597270514934\n",
      "Gradient Descent(6945/9999): loss=0.04396759700537425\n",
      "Gradient Descent(6946/9999): loss=0.04396759674065045\n",
      "Gradient Descent(6947/9999): loss=0.04396759647634287\n",
      "Gradient Descent(6948/9999): loss=0.0439675962124509\n",
      "Gradient Descent(6949/9999): loss=0.043967595948973856\n",
      "Gradient Descent(6950/9999): loss=0.04396759568591107\n",
      "Gradient Descent(6951/9999): loss=0.04396759542326189\n",
      "Gradient Descent(6952/9999): loss=0.0439675951610257\n",
      "Gradient Descent(6953/9999): loss=0.04396759489920185\n",
      "Gradient Descent(6954/9999): loss=0.04396759463778964\n",
      "Gradient Descent(6955/9999): loss=0.043967594376788474\n",
      "Gradient Descent(6956/9999): loss=0.04396759411619769\n",
      "Gradient Descent(6957/9999): loss=0.04396759385601663\n",
      "Gradient Descent(6958/9999): loss=0.043967593596244685\n",
      "Gradient Descent(6959/9999): loss=0.043967593336881175\n",
      "Gradient Descent(6960/9999): loss=0.04396759307792547\n",
      "Gradient Descent(6961/9999): loss=0.04396759281937692\n",
      "Gradient Descent(6962/9999): loss=0.043967592561234906\n",
      "Gradient Descent(6963/9999): loss=0.04396759230349876\n",
      "Gradient Descent(6964/9999): loss=0.043967592046167896\n",
      "Gradient Descent(6965/9999): loss=0.04396759178924163\n",
      "Gradient Descent(6966/9999): loss=0.04396759153271931\n",
      "Gradient Descent(6967/9999): loss=0.04396759127660034\n",
      "Gradient Descent(6968/9999): loss=0.04396759102088409\n",
      "Gradient Descent(6969/9999): loss=0.04396759076556989\n",
      "Gradient Descent(6970/9999): loss=0.04396759051065717\n",
      "Gradient Descent(6971/9999): loss=0.04396759025614522\n",
      "Gradient Descent(6972/9999): loss=0.04396759000203346\n",
      "Gradient Descent(6973/9999): loss=0.043967589748321254\n",
      "Gradient Descent(6974/9999): loss=0.04396758949500794\n",
      "Gradient Descent(6975/9999): loss=0.04396758924209296\n",
      "Gradient Descent(6976/9999): loss=0.043967588989575604\n",
      "Gradient Descent(6977/9999): loss=0.043967588737455314\n",
      "Gradient Descent(6978/9999): loss=0.04396758848573145\n",
      "Gradient Descent(6979/9999): loss=0.043967588234403374\n",
      "Gradient Descent(6980/9999): loss=0.04396758798347045\n",
      "Gradient Descent(6981/9999): loss=0.0439675877329321\n",
      "Gradient Descent(6982/9999): loss=0.04396758748278765\n",
      "Gradient Descent(6983/9999): loss=0.04396758723303653\n",
      "Gradient Descent(6984/9999): loss=0.0439675869836781\n",
      "Gradient Descent(6985/9999): loss=0.04396758673471177\n",
      "Gradient Descent(6986/9999): loss=0.043967586486136856\n",
      "Gradient Descent(6987/9999): loss=0.043967586237952806\n",
      "Gradient Descent(6988/9999): loss=0.04396758599015897\n",
      "Gradient Descent(6989/9999): loss=0.04396758574275478\n",
      "Gradient Descent(6990/9999): loss=0.04396758549573956\n",
      "Gradient Descent(6991/9999): loss=0.04396758524911273\n",
      "Gradient Descent(6992/9999): loss=0.0439675850028737\n",
      "Gradient Descent(6993/9999): loss=0.04396758475702184\n",
      "Gradient Descent(6994/9999): loss=0.04396758451155652\n",
      "Gradient Descent(6995/9999): loss=0.04396758426647717\n",
      "Gradient Descent(6996/9999): loss=0.04396758402178315\n",
      "Gradient Descent(6997/9999): loss=0.0439675837774739\n",
      "Gradient Descent(6998/9999): loss=0.04396758353354877\n",
      "Gradient Descent(6999/9999): loss=0.04396758329000715\n",
      "Gradient Descent(7000/9999): loss=0.043967583046848474\n",
      "Gradient Descent(7001/9999): loss=0.04396758280407215\n",
      "Gradient Descent(7002/9999): loss=0.04396758256167752\n",
      "Gradient Descent(7003/9999): loss=0.043967582319664016\n",
      "Gradient Descent(7004/9999): loss=0.04396758207803104\n",
      "Gradient Descent(7005/9999): loss=0.043967581836778\n",
      "Gradient Descent(7006/9999): loss=0.04396758159590429\n",
      "Gradient Descent(7007/9999): loss=0.0439675813554093\n",
      "Gradient Descent(7008/9999): loss=0.043967581115292476\n",
      "Gradient Descent(7009/9999): loss=0.04396758087555317\n",
      "Gradient Descent(7010/9999): loss=0.04396758063619083\n",
      "Gradient Descent(7011/9999): loss=0.04396758039720484\n",
      "Gradient Descent(7012/9999): loss=0.043967580158594616\n",
      "Gradient Descent(7013/9999): loss=0.04396757992035956\n",
      "Gradient Descent(7014/9999): loss=0.043967579682499106\n",
      "Gradient Descent(7015/9999): loss=0.043967579445012624\n",
      "Gradient Descent(7016/9999): loss=0.043967579207899565\n",
      "Gradient Descent(7017/9999): loss=0.04396757897115933\n",
      "Gradient Descent(7018/9999): loss=0.04396757873479133\n",
      "Gradient Descent(7019/9999): loss=0.043967578498794964\n",
      "Gradient Descent(7020/9999): loss=0.04396757826316967\n",
      "Gradient Descent(7021/9999): loss=0.04396757802791487\n",
      "Gradient Descent(7022/9999): loss=0.043967577793029955\n",
      "Gradient Descent(7023/9999): loss=0.043967577558514354\n",
      "Gradient Descent(7024/9999): loss=0.043967577324367506\n",
      "Gradient Descent(7025/9999): loss=0.04396757709058881\n",
      "Gradient Descent(7026/9999): loss=0.04396757685717768\n",
      "Gradient Descent(7027/9999): loss=0.04396757662413356\n",
      "Gradient Descent(7028/9999): loss=0.04396757639145587\n",
      "Gradient Descent(7029/9999): loss=0.04396757615914401\n",
      "Gradient Descent(7030/9999): loss=0.043967575927197425\n",
      "Gradient Descent(7031/9999): loss=0.043967575695615546\n",
      "Gradient Descent(7032/9999): loss=0.04396757546439778\n",
      "Gradient Descent(7033/9999): loss=0.043967575233543575\n",
      "Gradient Descent(7034/9999): loss=0.04396757500305234\n",
      "Gradient Descent(7035/9999): loss=0.04396757477292351\n",
      "Gradient Descent(7036/9999): loss=0.04396757454315654\n",
      "Gradient Descent(7037/9999): loss=0.043967574313750826\n",
      "Gradient Descent(7038/9999): loss=0.04396757408470583\n",
      "Gradient Descent(7039/9999): loss=0.043967573856020944\n",
      "Gradient Descent(7040/9999): loss=0.043967573627695634\n",
      "Gradient Descent(7041/9999): loss=0.04396757339972934\n",
      "Gradient Descent(7042/9999): loss=0.04396757317212148\n",
      "Gradient Descent(7043/9999): loss=0.043967572944871504\n",
      "Gradient Descent(7044/9999): loss=0.04396757271797882\n",
      "Gradient Descent(7045/9999): loss=0.04396757249144288\n",
      "Gradient Descent(7046/9999): loss=0.04396757226526316\n",
      "Gradient Descent(7047/9999): loss=0.043967572039439046\n",
      "Gradient Descent(7048/9999): loss=0.043967571813970034\n",
      "Gradient Descent(7049/9999): loss=0.04396757158885552\n",
      "Gradient Descent(7050/9999): loss=0.043967571364094946\n",
      "Gradient Descent(7051/9999): loss=0.04396757113968777\n",
      "Gradient Descent(7052/9999): loss=0.043967570915633435\n",
      "Gradient Descent(7053/9999): loss=0.04396757069193141\n",
      "Gradient Descent(7054/9999): loss=0.04396757046858111\n",
      "Gradient Descent(7055/9999): loss=0.04396757024558199\n",
      "Gradient Descent(7056/9999): loss=0.04396757002293351\n",
      "Gradient Descent(7057/9999): loss=0.043967569800635084\n",
      "Gradient Descent(7058/9999): loss=0.04396756957868621\n",
      "Gradient Descent(7059/9999): loss=0.043967569357086296\n",
      "Gradient Descent(7060/9999): loss=0.04396756913583481\n",
      "Gradient Descent(7061/9999): loss=0.0439675689149312\n",
      "Gradient Descent(7062/9999): loss=0.043967568694374924\n",
      "Gradient Descent(7063/9999): loss=0.04396756847416547\n",
      "Gradient Descent(7064/9999): loss=0.04396756825430223\n",
      "Gradient Descent(7065/9999): loss=0.043967568034784694\n",
      "Gradient Descent(7066/9999): loss=0.043967567815612306\n",
      "Gradient Descent(7067/9999): loss=0.043967567596784544\n",
      "Gradient Descent(7068/9999): loss=0.04396756737830085\n",
      "Gradient Descent(7069/9999): loss=0.04396756716016066\n",
      "Gradient Descent(7070/9999): loss=0.043967566942363485\n",
      "Gradient Descent(7071/9999): loss=0.043967566724908765\n",
      "Gradient Descent(7072/9999): loss=0.043967566507795944\n",
      "Gradient Descent(7073/9999): loss=0.04396756629102448\n",
      "Gradient Descent(7074/9999): loss=0.04396756607459387\n",
      "Gradient Descent(7075/9999): loss=0.04396756585850357\n",
      "Gradient Descent(7076/9999): loss=0.04396756564275302\n",
      "Gradient Descent(7077/9999): loss=0.0439675654273417\n",
      "Gradient Descent(7078/9999): loss=0.0439675652122691\n",
      "Gradient Descent(7079/9999): loss=0.04396756499753467\n",
      "Gradient Descent(7080/9999): loss=0.04396756478313781\n",
      "Gradient Descent(7081/9999): loss=0.043967564569078126\n",
      "Gradient Descent(7082/9999): loss=0.04396756435535499\n",
      "Gradient Descent(7083/9999): loss=0.0439675641419679\n",
      "Gradient Descent(7084/9999): loss=0.04396756392891631\n",
      "Gradient Descent(7085/9999): loss=0.043967563716199726\n",
      "Gradient Descent(7086/9999): loss=0.0439675635038176\n",
      "Gradient Descent(7087/9999): loss=0.0439675632917694\n",
      "Gradient Descent(7088/9999): loss=0.04396756308005464\n",
      "Gradient Descent(7089/9999): loss=0.04396756286867273\n",
      "Gradient Descent(7090/9999): loss=0.04396756265762319\n",
      "Gradient Descent(7091/9999): loss=0.043967562446905514\n",
      "Gradient Descent(7092/9999): loss=0.043967562236519125\n",
      "Gradient Descent(7093/9999): loss=0.04396756202646355\n",
      "Gradient Descent(7094/9999): loss=0.04396756181673825\n",
      "Gradient Descent(7095/9999): loss=0.04396756160734272\n",
      "Gradient Descent(7096/9999): loss=0.04396756139827642\n",
      "Gradient Descent(7097/9999): loss=0.04396756118953883\n",
      "Gradient Descent(7098/9999): loss=0.04396756098112946\n",
      "Gradient Descent(7099/9999): loss=0.04396756077304777\n",
      "Gradient Descent(7100/9999): loss=0.04396756056529325\n",
      "Gradient Descent(7101/9999): loss=0.04396756035786539\n",
      "Gradient Descent(7102/9999): loss=0.043967560150763675\n",
      "Gradient Descent(7103/9999): loss=0.04396755994398762\n",
      "Gradient Descent(7104/9999): loss=0.04396755973753665\n",
      "Gradient Descent(7105/9999): loss=0.043967559531410315\n",
      "Gradient Descent(7106/9999): loss=0.04396755932560806\n",
      "Gradient Descent(7107/9999): loss=0.04396755912012942\n",
      "Gradient Descent(7108/9999): loss=0.043967558914973844\n",
      "Gradient Descent(7109/9999): loss=0.043967558710140825\n",
      "Gradient Descent(7110/9999): loss=0.04396755850562992\n",
      "Gradient Descent(7111/9999): loss=0.043967558301440536\n",
      "Gradient Descent(7112/9999): loss=0.04396755809757219\n",
      "Gradient Descent(7113/9999): loss=0.04396755789402443\n",
      "Gradient Descent(7114/9999): loss=0.04396755769079669\n",
      "Gradient Descent(7115/9999): loss=0.04396755748788851\n",
      "Gradient Descent(7116/9999): loss=0.04396755728529936\n",
      "Gradient Descent(7117/9999): loss=0.04396755708302875\n",
      "Gradient Descent(7118/9999): loss=0.043967556881076175\n",
      "Gradient Descent(7119/9999): loss=0.04396755667944114\n",
      "Gradient Descent(7120/9999): loss=0.04396755647812313\n",
      "Gradient Descent(7121/9999): loss=0.04396755627712167\n",
      "Gradient Descent(7122/9999): loss=0.04396755607643626\n",
      "Gradient Descent(7123/9999): loss=0.04396755587606637\n",
      "Gradient Descent(7124/9999): loss=0.04396755567601152\n",
      "Gradient Descent(7125/9999): loss=0.043967555476271285\n",
      "Gradient Descent(7126/9999): loss=0.043967555276845066\n",
      "Gradient Descent(7127/9999): loss=0.04396755507773241\n",
      "Gradient Descent(7128/9999): loss=0.043967554878932835\n",
      "Gradient Descent(7129/9999): loss=0.043967554680445824\n",
      "Gradient Descent(7130/9999): loss=0.04396755448227092\n",
      "Gradient Descent(7131/9999): loss=0.0439675542844076\n",
      "Gradient Descent(7132/9999): loss=0.04396755408685538\n",
      "Gradient Descent(7133/9999): loss=0.0439675538896138\n",
      "Gradient Descent(7134/9999): loss=0.04396755369268234\n",
      "Gradient Descent(7135/9999): loss=0.04396755349606051\n",
      "Gradient Descent(7136/9999): loss=0.04396755329974785\n",
      "Gradient Descent(7137/9999): loss=0.04396755310374387\n",
      "Gradient Descent(7138/9999): loss=0.043967552908048047\n",
      "Gradient Descent(7139/9999): loss=0.04396755271265993\n",
      "Gradient Descent(7140/9999): loss=0.04396755251757904\n",
      "Gradient Descent(7141/9999): loss=0.04396755232280486\n",
      "Gradient Descent(7142/9999): loss=0.04396755212833695\n",
      "Gradient Descent(7143/9999): loss=0.043967551934174794\n",
      "Gradient Descent(7144/9999): loss=0.043967551740317946\n",
      "Gradient Descent(7145/9999): loss=0.043967551546765876\n",
      "Gradient Descent(7146/9999): loss=0.04396755135351817\n",
      "Gradient Descent(7147/9999): loss=0.04396755116057429\n",
      "Gradient Descent(7148/9999): loss=0.04396755096793378\n",
      "Gradient Descent(7149/9999): loss=0.04396755077559617\n",
      "Gradient Descent(7150/9999): loss=0.04396755058356099\n",
      "Gradient Descent(7151/9999): loss=0.04396755039182773\n",
      "Gradient Descent(7152/9999): loss=0.043967550200395965\n",
      "Gradient Descent(7153/9999): loss=0.04396755000926518\n",
      "Gradient Descent(7154/9999): loss=0.043967549818434905\n",
      "Gradient Descent(7155/9999): loss=0.04396754962790471\n",
      "Gradient Descent(7156/9999): loss=0.04396754943767406\n",
      "Gradient Descent(7157/9999): loss=0.04396754924774255\n",
      "Gradient Descent(7158/9999): loss=0.043967549058109644\n",
      "Gradient Descent(7159/9999): loss=0.04396754886877493\n",
      "Gradient Descent(7160/9999): loss=0.0439675486797379\n",
      "Gradient Descent(7161/9999): loss=0.04396754849099808\n",
      "Gradient Descent(7162/9999): loss=0.043967548302555046\n",
      "Gradient Descent(7163/9999): loss=0.0439675481144083\n",
      "Gradient Descent(7164/9999): loss=0.04396754792655738\n",
      "Gradient Descent(7165/9999): loss=0.043967547739001835\n",
      "Gradient Descent(7166/9999): loss=0.043967547551741204\n",
      "Gradient Descent(7167/9999): loss=0.043967547364774984\n",
      "Gradient Descent(7168/9999): loss=0.04396754717810274\n",
      "Gradient Descent(7169/9999): loss=0.043967546991724\n",
      "Gradient Descent(7170/9999): loss=0.043967546805638316\n",
      "Gradient Descent(7171/9999): loss=0.04396754661984522\n",
      "Gradient Descent(7172/9999): loss=0.04396754643434425\n",
      "Gradient Descent(7173/9999): loss=0.043967546249134966\n",
      "Gradient Descent(7174/9999): loss=0.043967546064216885\n",
      "Gradient Descent(7175/9999): loss=0.04396754587958954\n",
      "Gradient Descent(7176/9999): loss=0.043967545695252505\n",
      "Gradient Descent(7177/9999): loss=0.04396754551120532\n",
      "Gradient Descent(7178/9999): loss=0.0439675453274475\n",
      "Gradient Descent(7179/9999): loss=0.04396754514397861\n",
      "Gradient Descent(7180/9999): loss=0.043967544960798216\n",
      "Gradient Descent(7181/9999): loss=0.04396754477790582\n",
      "Gradient Descent(7182/9999): loss=0.043967544595301\n",
      "Gradient Descent(7183/9999): loss=0.04396754441298329\n",
      "Gradient Descent(7184/9999): loss=0.043967544230952234\n",
      "Gradient Descent(7185/9999): loss=0.04396754404920742\n",
      "Gradient Descent(7186/9999): loss=0.04396754386774835\n",
      "Gradient Descent(7187/9999): loss=0.04396754368657458\n",
      "Gradient Descent(7188/9999): loss=0.04396754350568571\n",
      "Gradient Descent(7189/9999): loss=0.04396754332508124\n",
      "Gradient Descent(7190/9999): loss=0.04396754314476074\n",
      "Gradient Descent(7191/9999): loss=0.04396754296472377\n",
      "Gradient Descent(7192/9999): loss=0.04396754278496988\n",
      "Gradient Descent(7193/9999): loss=0.043967542605498615\n",
      "Gradient Descent(7194/9999): loss=0.04396754242630955\n",
      "Gradient Descent(7195/9999): loss=0.04396754224740219\n",
      "Gradient Descent(7196/9999): loss=0.0439675420687762\n",
      "Gradient Descent(7197/9999): loss=0.043967541890431006\n",
      "Gradient Descent(7198/9999): loss=0.04396754171236626\n",
      "Gradient Descent(7199/9999): loss=0.04396754153458149\n",
      "Gradient Descent(7200/9999): loss=0.04396754135707625\n",
      "Gradient Descent(7201/9999): loss=0.043967541179850106\n",
      "Gradient Descent(7202/9999): loss=0.04396754100290263\n",
      "Gradient Descent(7203/9999): loss=0.04396754082623336\n",
      "Gradient Descent(7204/9999): loss=0.04396754064984188\n",
      "Gradient Descent(7205/9999): loss=0.043967540473727765\n",
      "Gradient Descent(7206/9999): loss=0.043967540297890545\n",
      "Gradient Descent(7207/9999): loss=0.043967540122329805\n",
      "Gradient Descent(7208/9999): loss=0.04396753994704511\n",
      "Gradient Descent(7209/9999): loss=0.043967539772036004\n",
      "Gradient Descent(7210/9999): loss=0.04396753959730207\n",
      "Gradient Descent(7211/9999): loss=0.043967539422842886\n",
      "Gradient Descent(7212/9999): loss=0.043967539248658\n",
      "Gradient Descent(7213/9999): loss=0.043967539074747014\n",
      "Gradient Descent(7214/9999): loss=0.043967538901109444\n",
      "Gradient Descent(7215/9999): loss=0.043967538727744906\n",
      "Gradient Descent(7216/9999): loss=0.04396753855465294\n",
      "Gradient Descent(7217/9999): loss=0.04396753838183314\n",
      "Gradient Descent(7218/9999): loss=0.04396753820928507\n",
      "Gradient Descent(7219/9999): loss=0.043967538037008334\n",
      "Gradient Descent(7220/9999): loss=0.04396753786500243\n",
      "Gradient Descent(7221/9999): loss=0.043967537693266986\n",
      "Gradient Descent(7222/9999): loss=0.04396753752180158\n",
      "Gradient Descent(7223/9999): loss=0.043967537350605775\n",
      "Gradient Descent(7224/9999): loss=0.04396753717967915\n",
      "Gradient Descent(7225/9999): loss=0.04396753700902128\n",
      "Gradient Descent(7226/9999): loss=0.0439675368386317\n",
      "Gradient Descent(7227/9999): loss=0.043967536668510075\n",
      "Gradient Descent(7228/9999): loss=0.04396753649865592\n",
      "Gradient Descent(7229/9999): loss=0.04396753632906886\n",
      "Gradient Descent(7230/9999): loss=0.043967536159748404\n",
      "Gradient Descent(7231/9999): loss=0.0439675359906942\n",
      "Gradient Descent(7232/9999): loss=0.0439675358219058\n",
      "Gradient Descent(7233/9999): loss=0.04396753565338279\n",
      "Gradient Descent(7234/9999): loss=0.04396753548512474\n",
      "Gradient Descent(7235/9999): loss=0.043967535317131275\n",
      "Gradient Descent(7236/9999): loss=0.04396753514940195\n",
      "Gradient Descent(7237/9999): loss=0.04396753498193634\n",
      "Gradient Descent(7238/9999): loss=0.04396753481473406\n",
      "Gradient Descent(7239/9999): loss=0.04396753464779466\n",
      "Gradient Descent(7240/9999): loss=0.043967534481117745\n",
      "Gradient Descent(7241/9999): loss=0.04396753431470289\n",
      "Gradient Descent(7242/9999): loss=0.043967534148549724\n",
      "Gradient Descent(7243/9999): loss=0.04396753398265778\n",
      "Gradient Descent(7244/9999): loss=0.04396753381702669\n",
      "Gradient Descent(7245/9999): loss=0.04396753365165601\n",
      "Gradient Descent(7246/9999): loss=0.043967533486545356\n",
      "Gradient Descent(7247/9999): loss=0.04396753332169432\n",
      "Gradient Descent(7248/9999): loss=0.04396753315710247\n",
      "Gradient Descent(7249/9999): loss=0.04396753299276943\n",
      "Gradient Descent(7250/9999): loss=0.04396753282869477\n",
      "Gradient Descent(7251/9999): loss=0.04396753266487808\n",
      "Gradient Descent(7252/9999): loss=0.04396753250131899\n",
      "Gradient Descent(7253/9999): loss=0.043967532338017035\n",
      "Gradient Descent(7254/9999): loss=0.04396753217497187\n",
      "Gradient Descent(7255/9999): loss=0.043967532012183064\n",
      "Gradient Descent(7256/9999): loss=0.043967531849650195\n",
      "Gradient Descent(7257/9999): loss=0.043967531687372906\n",
      "Gradient Descent(7258/9999): loss=0.04396753152535077\n",
      "Gradient Descent(7259/9999): loss=0.04396753136358336\n",
      "Gradient Descent(7260/9999): loss=0.04396753120207033\n",
      "Gradient Descent(7261/9999): loss=0.043967531040811234\n",
      "Gradient Descent(7262/9999): loss=0.043967530879805705\n",
      "Gradient Descent(7263/9999): loss=0.04396753071905332\n",
      "Gradient Descent(7264/9999): loss=0.043967530558553694\n",
      "Gradient Descent(7265/9999): loss=0.043967530398306434\n",
      "Gradient Descent(7266/9999): loss=0.04396753023831115\n",
      "Gradient Descent(7267/9999): loss=0.04396753007856741\n",
      "Gradient Descent(7268/9999): loss=0.04396752991907484\n",
      "Gradient Descent(7269/9999): loss=0.04396752975983305\n",
      "Gradient Descent(7270/9999): loss=0.04396752960084165\n",
      "Gradient Descent(7271/9999): loss=0.04396752944210021\n",
      "Gradient Descent(7272/9999): loss=0.043967529283608385\n",
      "Gradient Descent(7273/9999): loss=0.04396752912536575\n",
      "Gradient Descent(7274/9999): loss=0.04396752896737194\n",
      "Gradient Descent(7275/9999): loss=0.04396752880962655\n",
      "Gradient Descent(7276/9999): loss=0.04396752865212918\n",
      "Gradient Descent(7277/9999): loss=0.04396752849487944\n",
      "Gradient Descent(7278/9999): loss=0.04396752833787696\n",
      "Gradient Descent(7279/9999): loss=0.043967528181121346\n",
      "Gradient Descent(7280/9999): loss=0.04396752802461219\n",
      "Gradient Descent(7281/9999): loss=0.04396752786834915\n",
      "Gradient Descent(7282/9999): loss=0.043967527712331755\n",
      "Gradient Descent(7283/9999): loss=0.04396752755655972\n",
      "Gradient Descent(7284/9999): loss=0.04396752740103258\n",
      "Gradient Descent(7285/9999): loss=0.043967527245749996\n",
      "Gradient Descent(7286/9999): loss=0.04396752709071155\n",
      "Gradient Descent(7287/9999): loss=0.0439675269359169\n",
      "Gradient Descent(7288/9999): loss=0.043967526781365625\n",
      "Gradient Descent(7289/9999): loss=0.04396752662705735\n",
      "Gradient Descent(7290/9999): loss=0.04396752647299171\n",
      "Gradient Descent(7291/9999): loss=0.04396752631916833\n",
      "Gradient Descent(7292/9999): loss=0.04396752616558678\n",
      "Gradient Descent(7293/9999): loss=0.04396752601224672\n",
      "Gradient Descent(7294/9999): loss=0.04396752585914777\n",
      "Gradient Descent(7295/9999): loss=0.043967525706289534\n",
      "Gradient Descent(7296/9999): loss=0.04396752555367163\n",
      "Gradient Descent(7297/9999): loss=0.043967525401293735\n",
      "Gradient Descent(7298/9999): loss=0.04396752524915539\n",
      "Gradient Descent(7299/9999): loss=0.043967525097256266\n",
      "Gradient Descent(7300/9999): loss=0.04396752494559599\n",
      "Gradient Descent(7301/9999): loss=0.043967524794174144\n",
      "Gradient Descent(7302/9999): loss=0.04396752464299042\n",
      "Gradient Descent(7303/9999): loss=0.043967524492044385\n",
      "Gradient Descent(7304/9999): loss=0.043967524341335704\n",
      "Gradient Descent(7305/9999): loss=0.04396752419086397\n",
      "Gradient Descent(7306/9999): loss=0.04396752404062884\n",
      "Gradient Descent(7307/9999): loss=0.04396752389062992\n",
      "Gradient Descent(7308/9999): loss=0.04396752374086685\n",
      "Gradient Descent(7309/9999): loss=0.04396752359133927\n",
      "Gradient Descent(7310/9999): loss=0.04396752344204678\n",
      "Gradient Descent(7311/9999): loss=0.043967523292989\n",
      "Gradient Descent(7312/9999): loss=0.04396752314416565\n",
      "Gradient Descent(7313/9999): loss=0.043967522995576276\n",
      "Gradient Descent(7314/9999): loss=0.043967522847220525\n",
      "Gradient Descent(7315/9999): loss=0.04396752269909806\n",
      "Gradient Descent(7316/9999): loss=0.04396752255120848\n",
      "Gradient Descent(7317/9999): loss=0.04396752240355142\n",
      "Gradient Descent(7318/9999): loss=0.043967522256126525\n",
      "Gradient Descent(7319/9999): loss=0.043967522108933454\n",
      "Gradient Descent(7320/9999): loss=0.0439675219619718\n",
      "Gradient Descent(7321/9999): loss=0.04396752181524124\n",
      "Gradient Descent(7322/9999): loss=0.04396752166874137\n",
      "Gradient Descent(7323/9999): loss=0.043967521522471846\n",
      "Gradient Descent(7324/9999): loss=0.04396752137643232\n",
      "Gradient Descent(7325/9999): loss=0.0439675212306224\n",
      "Gradient Descent(7326/9999): loss=0.04396752108504177\n",
      "Gradient Descent(7327/9999): loss=0.04396752093968999\n",
      "Gradient Descent(7328/9999): loss=0.0439675207945668\n",
      "Gradient Descent(7329/9999): loss=0.04396752064967177\n",
      "Gradient Descent(7330/9999): loss=0.043967520505004576\n",
      "Gradient Descent(7331/9999): loss=0.043967520360564845\n",
      "Gradient Descent(7332/9999): loss=0.0439675202163522\n",
      "Gradient Descent(7333/9999): loss=0.04396752007236633\n",
      "Gradient Descent(7334/9999): loss=0.043967519928606834\n",
      "Gradient Descent(7335/9999): loss=0.04396751978507339\n",
      "Gradient Descent(7336/9999): loss=0.043967519641765616\n",
      "Gradient Descent(7337/9999): loss=0.04396751949868319\n",
      "Gradient Descent(7338/9999): loss=0.04396751935582573\n",
      "Gradient Descent(7339/9999): loss=0.04396751921319288\n",
      "Gradient Descent(7340/9999): loss=0.04396751907078429\n",
      "Gradient Descent(7341/9999): loss=0.043967518928599646\n",
      "Gradient Descent(7342/9999): loss=0.04396751878663853\n",
      "Gradient Descent(7343/9999): loss=0.043967518644900644\n",
      "Gradient Descent(7344/9999): loss=0.043967518503385615\n",
      "Gradient Descent(7345/9999): loss=0.043967518362093096\n",
      "Gradient Descent(7346/9999): loss=0.04396751822102272\n",
      "Gradient Descent(7347/9999): loss=0.043967518080174156\n",
      "Gradient Descent(7348/9999): loss=0.043967517939547056\n",
      "Gradient Descent(7349/9999): loss=0.04396751779914107\n",
      "Gradient Descent(7350/9999): loss=0.043967517658955874\n",
      "Gradient Descent(7351/9999): loss=0.04396751751899108\n",
      "Gradient Descent(7352/9999): loss=0.04396751737924634\n",
      "Gradient Descent(7353/9999): loss=0.04396751723972133\n",
      "Gradient Descent(7354/9999): loss=0.04396751710041571\n",
      "Gradient Descent(7355/9999): loss=0.043967516961329126\n",
      "Gradient Descent(7356/9999): loss=0.04396751682246122\n",
      "Gradient Descent(7357/9999): loss=0.043967516683811686\n",
      "Gradient Descent(7358/9999): loss=0.04396751654538012\n",
      "Gradient Descent(7359/9999): loss=0.04396751640716625\n",
      "Gradient Descent(7360/9999): loss=0.04396751626916967\n",
      "Gradient Descent(7361/9999): loss=0.04396751613139007\n",
      "Gradient Descent(7362/9999): loss=0.04396751599382711\n",
      "Gradient Descent(7363/9999): loss=0.043967515856480455\n",
      "Gradient Descent(7364/9999): loss=0.04396751571934974\n",
      "Gradient Descent(7365/9999): loss=0.04396751558243465\n",
      "Gradient Descent(7366/9999): loss=0.04396751544573483\n",
      "Gradient Descent(7367/9999): loss=0.043967515309249944\n",
      "Gradient Descent(7368/9999): loss=0.04396751517297968\n",
      "Gradient Descent(7369/9999): loss=0.043967515036923664\n",
      "Gradient Descent(7370/9999): loss=0.04396751490108157\n",
      "Gradient Descent(7371/9999): loss=0.04396751476545306\n",
      "Gradient Descent(7372/9999): loss=0.043967514630037806\n",
      "Gradient Descent(7373/9999): loss=0.0439675144948355\n",
      "Gradient Descent(7374/9999): loss=0.04396751435984572\n",
      "Gradient Descent(7375/9999): loss=0.04396751422506821\n",
      "Gradient Descent(7376/9999): loss=0.04396751409050264\n",
      "Gradient Descent(7377/9999): loss=0.043967513956148646\n",
      "Gradient Descent(7378/9999): loss=0.043967513822005894\n",
      "Gradient Descent(7379/9999): loss=0.04396751368807405\n",
      "Gradient Descent(7380/9999): loss=0.043967513554352806\n",
      "Gradient Descent(7381/9999): loss=0.0439675134208418\n",
      "Gradient Descent(7382/9999): loss=0.04396751328754074\n",
      "Gradient Descent(7383/9999): loss=0.043967513154449245\n",
      "Gradient Descent(7384/9999): loss=0.04396751302156703\n",
      "Gradient Descent(7385/9999): loss=0.043967512888893764\n",
      "Gradient Descent(7386/9999): loss=0.04396751275642908\n",
      "Gradient Descent(7387/9999): loss=0.043967512624172686\n",
      "Gradient Descent(7388/9999): loss=0.043967512492124244\n",
      "Gradient Descent(7389/9999): loss=0.04396751236028342\n",
      "Gradient Descent(7390/9999): loss=0.043967512228649905\n",
      "Gradient Descent(7391/9999): loss=0.043967512097223355\n",
      "Gradient Descent(7392/9999): loss=0.04396751196600346\n",
      "Gradient Descent(7393/9999): loss=0.04396751183498988\n",
      "Gradient Descent(7394/9999): loss=0.04396751170418228\n",
      "Gradient Descent(7395/9999): loss=0.04396751157358037\n",
      "Gradient Descent(7396/9999): loss=0.0439675114431838\n",
      "Gradient Descent(7397/9999): loss=0.043967511312992265\n",
      "Gradient Descent(7398/9999): loss=0.04396751118300544\n",
      "Gradient Descent(7399/9999): loss=0.043967511053222996\n",
      "Gradient Descent(7400/9999): loss=0.0439675109236446\n",
      "Gradient Descent(7401/9999): loss=0.04396751079426994\n",
      "Gradient Descent(7402/9999): loss=0.04396751066509872\n",
      "Gradient Descent(7403/9999): loss=0.0439675105361306\n",
      "Gradient Descent(7404/9999): loss=0.043967510407365254\n",
      "Gradient Descent(7405/9999): loss=0.04396751027880237\n",
      "Gradient Descent(7406/9999): loss=0.04396751015044164\n",
      "Gradient Descent(7407/9999): loss=0.04396751002228271\n",
      "Gradient Descent(7408/9999): loss=0.0439675098943253\n",
      "Gradient Descent(7409/9999): loss=0.04396750976656909\n",
      "Gradient Descent(7410/9999): loss=0.04396750963901375\n",
      "Gradient Descent(7411/9999): loss=0.04396750951165899\n",
      "Gradient Descent(7412/9999): loss=0.043967509384504444\n",
      "Gradient Descent(7413/9999): loss=0.04396750925754983\n",
      "Gradient Descent(7414/9999): loss=0.04396750913079484\n",
      "Gradient Descent(7415/9999): loss=0.04396750900423915\n",
      "Gradient Descent(7416/9999): loss=0.04396750887788243\n",
      "Gradient Descent(7417/9999): loss=0.043967508751724435\n",
      "Gradient Descent(7418/9999): loss=0.04396750862576475\n",
      "Gradient Descent(7419/9999): loss=0.043967508500003125\n",
      "Gradient Descent(7420/9999): loss=0.04396750837443927\n",
      "Gradient Descent(7421/9999): loss=0.04396750824907282\n",
      "Gradient Descent(7422/9999): loss=0.04396750812390351\n",
      "Gradient Descent(7423/9999): loss=0.043967507998930976\n",
      "Gradient Descent(7424/9999): loss=0.04396750787415494\n",
      "Gradient Descent(7425/9999): loss=0.04396750774957511\n",
      "Gradient Descent(7426/9999): loss=0.043967507625191145\n",
      "Gradient Descent(7427/9999): loss=0.04396750750100277\n",
      "Gradient Descent(7428/9999): loss=0.04396750737700965\n",
      "Gradient Descent(7429/9999): loss=0.043967507253211524\n",
      "Gradient Descent(7430/9999): loss=0.043967507129607994\n",
      "Gradient Descent(7431/9999): loss=0.04396750700619885\n",
      "Gradient Descent(7432/9999): loss=0.04396750688298374\n",
      "Gradient Descent(7433/9999): loss=0.04396750675996233\n",
      "Gradient Descent(7434/9999): loss=0.04396750663713439\n",
      "Gradient Descent(7435/9999): loss=0.043967506514499564\n",
      "Gradient Descent(7436/9999): loss=0.043967506392057557\n",
      "Gradient Descent(7437/9999): loss=0.04396750626980808\n",
      "Gradient Descent(7438/9999): loss=0.04396750614775082\n",
      "Gradient Descent(7439/9999): loss=0.043967506025885464\n",
      "Gradient Descent(7440/9999): loss=0.043967505904211716\n",
      "Gradient Descent(7441/9999): loss=0.04396750578272929\n",
      "Gradient Descent(7442/9999): loss=0.04396750566143787\n",
      "Gradient Descent(7443/9999): loss=0.04396750554033718\n",
      "Gradient Descent(7444/9999): loss=0.04396750541942688\n",
      "Gradient Descent(7445/9999): loss=0.04396750529870668\n",
      "Gradient Descent(7446/9999): loss=0.043967505178176325\n",
      "Gradient Descent(7447/9999): loss=0.04396750505783546\n",
      "Gradient Descent(7448/9999): loss=0.043967504937683824\n",
      "Gradient Descent(7449/9999): loss=0.04396750481772109\n",
      "Gradient Descent(7450/9999): loss=0.043967504697946996\n",
      "Gradient Descent(7451/9999): loss=0.04396750457836122\n",
      "Gradient Descent(7452/9999): loss=0.04396750445896348\n",
      "Gradient Descent(7453/9999): loss=0.04396750433975345\n",
      "Gradient Descent(7454/9999): loss=0.043967504220730876\n",
      "Gradient Descent(7455/9999): loss=0.043967504101895455\n",
      "Gradient Descent(7456/9999): loss=0.043967503983246864\n",
      "Gradient Descent(7457/9999): loss=0.04396750386478484\n",
      "Gradient Descent(7458/9999): loss=0.04396750374650906\n",
      "Gradient Descent(7459/9999): loss=0.04396750362841926\n",
      "Gradient Descent(7460/9999): loss=0.04396750351051513\n",
      "Gradient Descent(7461/9999): loss=0.0439675033927964\n",
      "Gradient Descent(7462/9999): loss=0.04396750327526277\n",
      "Gradient Descent(7463/9999): loss=0.04396750315791391\n",
      "Gradient Descent(7464/9999): loss=0.04396750304074958\n",
      "Gradient Descent(7465/9999): loss=0.04396750292376948\n",
      "Gradient Descent(7466/9999): loss=0.043967502806973285\n",
      "Gradient Descent(7467/9999): loss=0.04396750269036075\n",
      "Gradient Descent(7468/9999): loss=0.043967502573931574\n",
      "Gradient Descent(7469/9999): loss=0.04396750245768545\n",
      "Gradient Descent(7470/9999): loss=0.04396750234162211\n",
      "Gradient Descent(7471/9999): loss=0.04396750222574125\n",
      "Gradient Descent(7472/9999): loss=0.043967502110042606\n",
      "Gradient Descent(7473/9999): loss=0.0439675019945259\n",
      "Gradient Descent(7474/9999): loss=0.043967501879190796\n",
      "Gradient Descent(7475/9999): loss=0.04396750176403704\n",
      "Gradient Descent(7476/9999): loss=0.043967501649064336\n",
      "Gradient Descent(7477/9999): loss=0.04396750153427243\n",
      "Gradient Descent(7478/9999): loss=0.043967501419661\n",
      "Gradient Descent(7479/9999): loss=0.04396750130522978\n",
      "Gradient Descent(7480/9999): loss=0.04396750119097849\n",
      "Gradient Descent(7481/9999): loss=0.04396750107690683\n",
      "Gradient Descent(7482/9999): loss=0.04396750096301453\n",
      "Gradient Descent(7483/9999): loss=0.043967500849301316\n",
      "Gradient Descent(7484/9999): loss=0.043967500735766885\n",
      "Gradient Descent(7485/9999): loss=0.043967500622410984\n",
      "Gradient Descent(7486/9999): loss=0.043967500509233294\n",
      "Gradient Descent(7487/9999): loss=0.043967500396233566\n",
      "Gradient Descent(7488/9999): loss=0.04396750028341154\n",
      "Gradient Descent(7489/9999): loss=0.043967500170766866\n",
      "Gradient Descent(7490/9999): loss=0.04396750005829933\n",
      "Gradient Descent(7491/9999): loss=0.04396749994600862\n",
      "Gradient Descent(7492/9999): loss=0.04396749983389449\n",
      "Gradient Descent(7493/9999): loss=0.04396749972195661\n",
      "Gradient Descent(7494/9999): loss=0.043967499610194756\n",
      "Gradient Descent(7495/9999): loss=0.04396749949860862\n",
      "Gradient Descent(7496/9999): loss=0.043967499387197934\n",
      "Gradient Descent(7497/9999): loss=0.04396749927596241\n",
      "Gradient Descent(7498/9999): loss=0.04396749916490182\n",
      "Gradient Descent(7499/9999): loss=0.0439674990540158\n",
      "Gradient Descent(7500/9999): loss=0.04396749894330416\n",
      "Gradient Descent(7501/9999): loss=0.04396749883276662\n",
      "Gradient Descent(7502/9999): loss=0.043967498722402854\n",
      "Gradient Descent(7503/9999): loss=0.04396749861221265\n",
      "Gradient Descent(7504/9999): loss=0.04396749850219565\n",
      "Gradient Descent(7505/9999): loss=0.043967498392351645\n",
      "Gradient Descent(7506/9999): loss=0.04396749828268037\n",
      "Gradient Descent(7507/9999): loss=0.04396749817318153\n",
      "Gradient Descent(7508/9999): loss=0.04396749806385486\n",
      "Gradient Descent(7509/9999): loss=0.0439674979547001\n",
      "Gradient Descent(7510/9999): loss=0.043967497845716956\n",
      "Gradient Descent(7511/9999): loss=0.043967497736905156\n",
      "Gradient Descent(7512/9999): loss=0.04396749762826444\n",
      "Gradient Descent(7513/9999): loss=0.043967497519794556\n",
      "Gradient Descent(7514/9999): loss=0.04396749741149525\n",
      "Gradient Descent(7515/9999): loss=0.0439674973033662\n",
      "Gradient Descent(7516/9999): loss=0.04396749719540716\n",
      "Gradient Descent(7517/9999): loss=0.04396749708761786\n",
      "Gradient Descent(7518/9999): loss=0.04396749697999807\n",
      "Gradient Descent(7519/9999): loss=0.04396749687254749\n",
      "Gradient Descent(7520/9999): loss=0.043967496765265845\n",
      "Gradient Descent(7521/9999): loss=0.04396749665815289\n",
      "Gradient Descent(7522/9999): loss=0.043967496551208324\n",
      "Gradient Descent(7523/9999): loss=0.04396749644443194\n",
      "Gradient Descent(7524/9999): loss=0.04396749633782344\n",
      "Gradient Descent(7525/9999): loss=0.04396749623138256\n",
      "Gradient Descent(7526/9999): loss=0.04396749612510904\n",
      "Gradient Descent(7527/9999): loss=0.04396749601900263\n",
      "Gradient Descent(7528/9999): loss=0.04396749591306303\n",
      "Gradient Descent(7529/9999): loss=0.04396749580729002\n",
      "Gradient Descent(7530/9999): loss=0.04396749570168332\n",
      "Gradient Descent(7531/9999): loss=0.04396749559624267\n",
      "Gradient Descent(7532/9999): loss=0.04396749549096781\n",
      "Gradient Descent(7533/9999): loss=0.04396749538585848\n",
      "Gradient Descent(7534/9999): loss=0.0439674952809144\n",
      "Gradient Descent(7535/9999): loss=0.04396749517613534\n",
      "Gradient Descent(7536/9999): loss=0.04396749507152102\n",
      "Gradient Descent(7537/9999): loss=0.04396749496707119\n",
      "Gradient Descent(7538/9999): loss=0.04396749486278559\n",
      "Gradient Descent(7539/9999): loss=0.04396749475866397\n",
      "Gradient Descent(7540/9999): loss=0.04396749465470605\n",
      "Gradient Descent(7541/9999): loss=0.04396749455091159\n",
      "Gradient Descent(7542/9999): loss=0.04396749444728033\n",
      "Gradient Descent(7543/9999): loss=0.043967494343812016\n",
      "Gradient Descent(7544/9999): loss=0.043967494240506395\n",
      "Gradient Descent(7545/9999): loss=0.043967494137363186\n",
      "Gradient Descent(7546/9999): loss=0.04396749403438217\n",
      "Gradient Descent(7547/9999): loss=0.04396749393156305\n",
      "Gradient Descent(7548/9999): loss=0.04396749382890563\n",
      "Gradient Descent(7549/9999): loss=0.0439674937264096\n",
      "Gradient Descent(7550/9999): loss=0.04396749362407473\n",
      "Gradient Descent(7551/9999): loss=0.04396749352190077\n",
      "Gradient Descent(7552/9999): loss=0.04396749341988746\n",
      "Gradient Descent(7553/9999): loss=0.04396749331803455\n",
      "Gradient Descent(7554/9999): loss=0.043967493216341795\n",
      "Gradient Descent(7555/9999): loss=0.043967493114808937\n",
      "Gradient Descent(7556/9999): loss=0.0439674930134357\n",
      "Gradient Descent(7557/9999): loss=0.043967492912221864\n",
      "Gradient Descent(7558/9999): loss=0.043967492811167165\n",
      "Gradient Descent(7559/9999): loss=0.04396749271027137\n",
      "Gradient Descent(7560/9999): loss=0.043967492609534226\n",
      "Gradient Descent(7561/9999): loss=0.04396749250895545\n",
      "Gradient Descent(7562/9999): loss=0.04396749240853483\n",
      "Gradient Descent(7563/9999): loss=0.043967492308272085\n",
      "Gradient Descent(7564/9999): loss=0.043967492208167015\n",
      "Gradient Descent(7565/9999): loss=0.04396749210821935\n",
      "Gradient Descent(7566/9999): loss=0.043967492008428796\n",
      "Gradient Descent(7567/9999): loss=0.04396749190879519\n",
      "Gradient Descent(7568/9999): loss=0.04396749180931819\n",
      "Gradient Descent(7569/9999): loss=0.04396749170999764\n",
      "Gradient Descent(7570/9999): loss=0.04396749161083326\n",
      "Gradient Descent(7571/9999): loss=0.04396749151182479\n",
      "Gradient Descent(7572/9999): loss=0.04396749141297199\n",
      "Gradient Descent(7573/9999): loss=0.04396749131427462\n",
      "Gradient Descent(7574/9999): loss=0.04396749121573242\n",
      "Gradient Descent(7575/9999): loss=0.04396749111734519\n",
      "Gradient Descent(7576/9999): loss=0.04396749101911261\n",
      "Gradient Descent(7577/9999): loss=0.04396749092103454\n",
      "Gradient Descent(7578/9999): loss=0.043967490823110654\n",
      "Gradient Descent(7579/9999): loss=0.04396749072534074\n",
      "Gradient Descent(7580/9999): loss=0.043967490627724565\n",
      "Gradient Descent(7581/9999): loss=0.04396749053026185\n",
      "Gradient Descent(7582/9999): loss=0.04396749043295238\n",
      "Gradient Descent(7583/9999): loss=0.04396749033579595\n",
      "Gradient Descent(7584/9999): loss=0.043967490238792234\n",
      "Gradient Descent(7585/9999): loss=0.04396749014194108\n",
      "Gradient Descent(7586/9999): loss=0.04396749004524218\n",
      "Gradient Descent(7587/9999): loss=0.04396748994869534\n",
      "Gradient Descent(7588/9999): loss=0.04396748985230031\n",
      "Gradient Descent(7589/9999): loss=0.04396748975605682\n",
      "Gradient Descent(7590/9999): loss=0.04396748965996467\n",
      "Gradient Descent(7591/9999): loss=0.043967489564023625\n",
      "Gradient Descent(7592/9999): loss=0.04396748946823341\n",
      "Gradient Descent(7593/9999): loss=0.043967489372593815\n",
      "Gradient Descent(7594/9999): loss=0.0439674892771046\n",
      "Gradient Descent(7595/9999): loss=0.043967489181765525\n",
      "Gradient Descent(7596/9999): loss=0.043967489086576356\n",
      "Gradient Descent(7597/9999): loss=0.043967488991536845\n",
      "Gradient Descent(7598/9999): loss=0.04396748889664678\n",
      "Gradient Descent(7599/9999): loss=0.04396748880190592\n",
      "Gradient Descent(7600/9999): loss=0.043967488707314005\n",
      "Gradient Descent(7601/9999): loss=0.04396748861287083\n",
      "Gradient Descent(7602/9999): loss=0.043967488518576146\n",
      "Gradient Descent(7603/9999): loss=0.04396748842442974\n",
      "Gradient Descent(7604/9999): loss=0.04396748833043135\n",
      "Gradient Descent(7605/9999): loss=0.043967488236580754\n",
      "Gradient Descent(7606/9999): loss=0.04396748814287774\n",
      "Gradient Descent(7607/9999): loss=0.04396748804932204\n",
      "Gradient Descent(7608/9999): loss=0.04396748795591345\n",
      "Gradient Descent(7609/9999): loss=0.04396748786265172\n",
      "Gradient Descent(7610/9999): loss=0.04396748776953663\n",
      "Gradient Descent(7611/9999): loss=0.043967487676567975\n",
      "Gradient Descent(7612/9999): loss=0.04396748758374547\n",
      "Gradient Descent(7613/9999): loss=0.043967487491068914\n",
      "Gradient Descent(7614/9999): loss=0.043967487398538056\n",
      "Gradient Descent(7615/9999): loss=0.04396748730615273\n",
      "Gradient Descent(7616/9999): loss=0.04396748721391264\n",
      "Gradient Descent(7617/9999): loss=0.0439674871218176\n",
      "Gradient Descent(7618/9999): loss=0.043967487029867344\n",
      "Gradient Descent(7619/9999): loss=0.04396748693806167\n",
      "Gradient Descent(7620/9999): loss=0.043967486846400367\n",
      "Gradient Descent(7621/9999): loss=0.043967486754883135\n",
      "Gradient Descent(7622/9999): loss=0.04396748666350986\n",
      "Gradient Descent(7623/9999): loss=0.04396748657228021\n",
      "Gradient Descent(7624/9999): loss=0.04396748648119403\n",
      "Gradient Descent(7625/9999): loss=0.04396748639025105\n",
      "Gradient Descent(7626/9999): loss=0.043967486299451075\n",
      "Gradient Descent(7627/9999): loss=0.04396748620879386\n",
      "Gradient Descent(7628/9999): loss=0.043967486118279185\n",
      "Gradient Descent(7629/9999): loss=0.04396748602790683\n",
      "Gradient Descent(7630/9999): loss=0.04396748593767659\n",
      "Gradient Descent(7631/9999): loss=0.04396748584758818\n",
      "Gradient Descent(7632/9999): loss=0.043967485757641464\n",
      "Gradient Descent(7633/9999): loss=0.043967485667836155\n",
      "Gradient Descent(7634/9999): loss=0.04396748557817204\n",
      "Gradient Descent(7635/9999): loss=0.043967485488648934\n",
      "Gradient Descent(7636/9999): loss=0.043967485399266565\n",
      "Gradient Descent(7637/9999): loss=0.04396748531002473\n",
      "Gradient Descent(7638/9999): loss=0.043967485220923246\n",
      "Gradient Descent(7639/9999): loss=0.04396748513196183\n",
      "Gradient Descent(7640/9999): loss=0.043967485043140304\n",
      "Gradient Descent(7641/9999): loss=0.04396748495445842\n",
      "Gradient Descent(7642/9999): loss=0.043967484865915986\n",
      "Gradient Descent(7643/9999): loss=0.04396748477751277\n",
      "Gradient Descent(7644/9999): loss=0.04396748468924854\n",
      "Gradient Descent(7645/9999): loss=0.04396748460112312\n",
      "Gradient Descent(7646/9999): loss=0.04396748451313623\n",
      "Gradient Descent(7647/9999): loss=0.04396748442528769\n",
      "Gradient Descent(7648/9999): loss=0.04396748433757729\n",
      "Gradient Descent(7649/9999): loss=0.043967484250004796\n",
      "Gradient Descent(7650/9999): loss=0.04396748416257\n",
      "Gradient Descent(7651/9999): loss=0.043967484075272664\n",
      "Gradient Descent(7652/9999): loss=0.04396748398811261\n",
      "Gradient Descent(7653/9999): loss=0.043967483901089593\n",
      "Gradient Descent(7654/9999): loss=0.0439674838142034\n",
      "Gradient Descent(7655/9999): loss=0.04396748372745382\n",
      "Gradient Descent(7656/9999): loss=0.04396748364084066\n",
      "Gradient Descent(7657/9999): loss=0.04396748355436366\n",
      "Gradient Descent(7658/9999): loss=0.04396748346802266\n",
      "Gradient Descent(7659/9999): loss=0.043967483381817364\n",
      "Gradient Descent(7660/9999): loss=0.04396748329574765\n",
      "Gradient Descent(7661/9999): loss=0.043967483209813266\n",
      "Gradient Descent(7662/9999): loss=0.043967483124013974\n",
      "Gradient Descent(7663/9999): loss=0.04396748303834964\n",
      "Gradient Descent(7664/9999): loss=0.04396748295281994\n",
      "Gradient Descent(7665/9999): loss=0.043967482867424766\n",
      "Gradient Descent(7666/9999): loss=0.04396748278216383\n",
      "Gradient Descent(7667/9999): loss=0.04396748269703696\n",
      "Gradient Descent(7668/9999): loss=0.04396748261204394\n",
      "Gradient Descent(7669/9999): loss=0.04396748252718456\n",
      "Gradient Descent(7670/9999): loss=0.043967482442458614\n",
      "Gradient Descent(7671/9999): loss=0.04396748235786586\n",
      "Gradient Descent(7672/9999): loss=0.043967482273406126\n",
      "Gradient Descent(7673/9999): loss=0.043967482189079206\n",
      "Gradient Descent(7674/9999): loss=0.04396748210488487\n",
      "Gradient Descent(7675/9999): loss=0.04396748202082291\n",
      "Gradient Descent(7676/9999): loss=0.04396748193689313\n",
      "Gradient Descent(7677/9999): loss=0.04396748185309532\n",
      "Gradient Descent(7678/9999): loss=0.04396748176942924\n",
      "Gradient Descent(7679/9999): loss=0.04396748168589473\n",
      "Gradient Descent(7680/9999): loss=0.04396748160249159\n",
      "Gradient Descent(7681/9999): loss=0.043967481519219545\n",
      "Gradient Descent(7682/9999): loss=0.04396748143607843\n",
      "Gradient Descent(7683/9999): loss=0.04396748135306809\n",
      "Gradient Descent(7684/9999): loss=0.04396748127018823\n",
      "Gradient Descent(7685/9999): loss=0.04396748118743867\n",
      "Gradient Descent(7686/9999): loss=0.04396748110481925\n",
      "Gradient Descent(7687/9999): loss=0.04396748102232974\n",
      "Gradient Descent(7688/9999): loss=0.04396748093996991\n",
      "Gradient Descent(7689/9999): loss=0.0439674808577396\n",
      "Gradient Descent(7690/9999): loss=0.043967480775638564\n",
      "Gradient Descent(7691/9999): loss=0.043967480693666613\n",
      "Gradient Descent(7692/9999): loss=0.04396748061182357\n",
      "Gradient Descent(7693/9999): loss=0.04396748053010921\n",
      "Gradient Descent(7694/9999): loss=0.04396748044852331\n",
      "Gradient Descent(7695/9999): loss=0.043967480367065714\n",
      "Gradient Descent(7696/9999): loss=0.04396748028573619\n",
      "Gradient Descent(7697/9999): loss=0.04396748020453454\n",
      "Gradient Descent(7698/9999): loss=0.043967480123460564\n",
      "Gradient Descent(7699/9999): loss=0.04396748004251405\n",
      "Gradient Descent(7700/9999): loss=0.04396747996169483\n",
      "Gradient Descent(7701/9999): loss=0.04396747988100268\n",
      "Gradient Descent(7702/9999): loss=0.043967479800437415\n",
      "Gradient Descent(7703/9999): loss=0.04396747971999882\n",
      "Gradient Descent(7704/9999): loss=0.04396747963968669\n",
      "Gradient Descent(7705/9999): loss=0.04396747955950084\n",
      "Gradient Descent(7706/9999): loss=0.04396747947944109\n",
      "Gradient Descent(7707/9999): loss=0.0439674793995072\n",
      "Gradient Descent(7708/9999): loss=0.043967479319699\n",
      "Gradient Descent(7709/9999): loss=0.04396747924001627\n",
      "Gradient Descent(7710/9999): loss=0.04396747916045884\n",
      "Gradient Descent(7711/9999): loss=0.043967479081026495\n",
      "Gradient Descent(7712/9999): loss=0.043967479001719066\n",
      "Gradient Descent(7713/9999): loss=0.0439674789225363\n",
      "Gradient Descent(7714/9999): loss=0.04396747884347807\n",
      "Gradient Descent(7715/9999): loss=0.04396747876454412\n",
      "Gradient Descent(7716/9999): loss=0.04396747868573429\n",
      "Gradient Descent(7717/9999): loss=0.04396747860704837\n",
      "Gradient Descent(7718/9999): loss=0.04396747852848617\n",
      "Gradient Descent(7719/9999): loss=0.04396747845004752\n",
      "Gradient Descent(7720/9999): loss=0.04396747837173218\n",
      "Gradient Descent(7721/9999): loss=0.04396747829353998\n",
      "Gradient Descent(7722/9999): loss=0.043967478215470715\n",
      "Gradient Descent(7723/9999): loss=0.04396747813752421\n",
      "Gradient Descent(7724/9999): loss=0.04396747805970028\n",
      "Gradient Descent(7725/9999): loss=0.043967477981998696\n",
      "Gradient Descent(7726/9999): loss=0.043967477904419294\n",
      "Gradient Descent(7727/9999): loss=0.04396747782696186\n",
      "Gradient Descent(7728/9999): loss=0.04396747774962621\n",
      "Gradient Descent(7729/9999): loss=0.04396747767241218\n",
      "Gradient Descent(7730/9999): loss=0.04396747759531954\n",
      "Gradient Descent(7731/9999): loss=0.04396747751834815\n",
      "Gradient Descent(7732/9999): loss=0.04396747744149772\n",
      "Gradient Descent(7733/9999): loss=0.04396747736476816\n",
      "Gradient Descent(7734/9999): loss=0.043967477288159244\n",
      "Gradient Descent(7735/9999): loss=0.043967477211670804\n",
      "Gradient Descent(7736/9999): loss=0.04396747713530261\n",
      "Gradient Descent(7737/9999): loss=0.043967477059054474\n",
      "Gradient Descent(7738/9999): loss=0.04396747698292625\n",
      "Gradient Descent(7739/9999): loss=0.04396747690691773\n",
      "Gradient Descent(7740/9999): loss=0.043967476831028696\n",
      "Gradient Descent(7741/9999): loss=0.043967476755258986\n",
      "Gradient Descent(7742/9999): loss=0.04396747667960842\n",
      "Gradient Descent(7743/9999): loss=0.04396747660407681\n",
      "Gradient Descent(7744/9999): loss=0.04396747652866396\n",
      "Gradient Descent(7745/9999): loss=0.043967476453369674\n",
      "Gradient Descent(7746/9999): loss=0.04396747637819378\n",
      "Gradient Descent(7747/9999): loss=0.04396747630313609\n",
      "Gradient Descent(7748/9999): loss=0.043967476228196405\n",
      "Gradient Descent(7749/9999): loss=0.043967476153374575\n",
      "Gradient Descent(7750/9999): loss=0.04396747607867035\n",
      "Gradient Descent(7751/9999): loss=0.043967476004083614\n",
      "Gradient Descent(7752/9999): loss=0.04396747592961417\n",
      "Gradient Descent(7753/9999): loss=0.04396747585526177\n",
      "Gradient Descent(7754/9999): loss=0.043967475781026326\n",
      "Gradient Descent(7755/9999): loss=0.04396747570690757\n",
      "Gradient Descent(7756/9999): loss=0.043967475632905366\n",
      "Gradient Descent(7757/9999): loss=0.04396747555901952\n",
      "Gradient Descent(7758/9999): loss=0.043967475485249825\n",
      "Gradient Descent(7759/9999): loss=0.043967475411596145\n",
      "Gradient Descent(7760/9999): loss=0.04396747533805827\n",
      "Gradient Descent(7761/9999): loss=0.04396747526463602\n",
      "Gradient Descent(7762/9999): loss=0.04396747519132921\n",
      "Gradient Descent(7763/9999): loss=0.04396747511813765\n",
      "Gradient Descent(7764/9999): loss=0.04396747504506119\n",
      "Gradient Descent(7765/9999): loss=0.04396747497209962\n",
      "Gradient Descent(7766/9999): loss=0.043967474899252776\n",
      "Gradient Descent(7767/9999): loss=0.043967474826520477\n",
      "Gradient Descent(7768/9999): loss=0.04396747475390256\n",
      "Gradient Descent(7769/9999): loss=0.04396747468139878\n",
      "Gradient Descent(7770/9999): loss=0.043967474609009016\n",
      "Gradient Descent(7771/9999): loss=0.04396747453673305\n",
      "Gradient Descent(7772/9999): loss=0.04396747446457075\n",
      "Gradient Descent(7773/9999): loss=0.043967474392521924\n",
      "Gradient Descent(7774/9999): loss=0.04396747432058637\n",
      "Gradient Descent(7775/9999): loss=0.04396747424876391\n",
      "Gradient Descent(7776/9999): loss=0.043967474177054405\n",
      "Gradient Descent(7777/9999): loss=0.04396747410545764\n",
      "Gradient Descent(7778/9999): loss=0.043967474033973435\n",
      "Gradient Descent(7779/9999): loss=0.043967473962601616\n",
      "Gradient Descent(7780/9999): loss=0.04396747389134206\n",
      "Gradient Descent(7781/9999): loss=0.04396747382019453\n",
      "Gradient Descent(7782/9999): loss=0.04396747374915886\n",
      "Gradient Descent(7783/9999): loss=0.043967473678234906\n",
      "Gradient Descent(7784/9999): loss=0.043967473607422446\n",
      "Gradient Descent(7785/9999): loss=0.04396747353672131\n",
      "Gradient Descent(7786/9999): loss=0.04396747346613137\n",
      "Gradient Descent(7787/9999): loss=0.043967473395652414\n",
      "Gradient Descent(7788/9999): loss=0.04396747332528426\n",
      "Gradient Descent(7789/9999): loss=0.04396747325502677\n",
      "Gradient Descent(7790/9999): loss=0.04396747318487973\n",
      "Gradient Descent(7791/9999): loss=0.043967473114842975\n",
      "Gradient Descent(7792/9999): loss=0.043967473044916376\n",
      "Gradient Descent(7793/9999): loss=0.04396747297509971\n",
      "Gradient Descent(7794/9999): loss=0.043967472905392796\n",
      "Gradient Descent(7795/9999): loss=0.043967472835795524\n",
      "Gradient Descent(7796/9999): loss=0.04396747276630765\n",
      "Gradient Descent(7797/9999): loss=0.04396747269692905\n",
      "Gradient Descent(7798/9999): loss=0.04396747262765952\n",
      "Gradient Descent(7799/9999): loss=0.04396747255849892\n",
      "Gradient Descent(7800/9999): loss=0.043967472489447074\n",
      "Gradient Descent(7801/9999): loss=0.04396747242050378\n",
      "Gradient Descent(7802/9999): loss=0.04396747235166889\n",
      "Gradient Descent(7803/9999): loss=0.043967472282942247\n",
      "Gradient Descent(7804/9999): loss=0.04396747221432365\n",
      "Gradient Descent(7805/9999): loss=0.04396747214581295\n",
      "Gradient Descent(7806/9999): loss=0.043967472077409975\n",
      "Gradient Descent(7807/9999): loss=0.04396747200911455\n",
      "Gradient Descent(7808/9999): loss=0.043967471940926484\n",
      "Gradient Descent(7809/9999): loss=0.04396747187284566\n",
      "Gradient Descent(7810/9999): loss=0.04396747180487187\n",
      "Gradient Descent(7811/9999): loss=0.04396747173700497\n",
      "Gradient Descent(7812/9999): loss=0.04396747166924478\n",
      "Gradient Descent(7813/9999): loss=0.04396747160159112\n",
      "Gradient Descent(7814/9999): loss=0.043967471534043845\n",
      "Gradient Descent(7815/9999): loss=0.04396747146660277\n",
      "Gradient Descent(7816/9999): loss=0.04396747139926774\n",
      "Gradient Descent(7817/9999): loss=0.04396747133203856\n",
      "Gradient Descent(7818/9999): loss=0.04396747126491512\n",
      "Gradient Descent(7819/9999): loss=0.0439674711978972\n",
      "Gradient Descent(7820/9999): loss=0.04396747113098467\n",
      "Gradient Descent(7821/9999): loss=0.04396747106417732\n",
      "Gradient Descent(7822/9999): loss=0.04396747099747505\n",
      "Gradient Descent(7823/9999): loss=0.04396747093087763\n",
      "Gradient Descent(7824/9999): loss=0.043967470864384926\n",
      "Gradient Descent(7825/9999): loss=0.04396747079799677\n",
      "Gradient Descent(7826/9999): loss=0.043967470731713025\n",
      "Gradient Descent(7827/9999): loss=0.0439674706655335\n",
      "Gradient Descent(7828/9999): loss=0.04396747059945798\n",
      "Gradient Descent(7829/9999): loss=0.04396747053348639\n",
      "Gradient Descent(7830/9999): loss=0.04396747046761853\n",
      "Gradient Descent(7831/9999): loss=0.043967470401854225\n",
      "Gradient Descent(7832/9999): loss=0.043967470336193346\n",
      "Gradient Descent(7833/9999): loss=0.043967470270635675\n",
      "Gradient Descent(7834/9999): loss=0.0439674702051811\n",
      "Gradient Descent(7835/9999): loss=0.04396747013982944\n",
      "Gradient Descent(7836/9999): loss=0.043967470074580524\n",
      "Gradient Descent(7837/9999): loss=0.04396747000943423\n",
      "Gradient Descent(7838/9999): loss=0.04396746994439034\n",
      "Gradient Descent(7839/9999): loss=0.04396746987944874\n",
      "Gradient Descent(7840/9999): loss=0.043967469814609236\n",
      "Gradient Descent(7841/9999): loss=0.04396746974987167\n",
      "Gradient Descent(7842/9999): loss=0.043967469685235926\n",
      "Gradient Descent(7843/9999): loss=0.04396746962070178\n",
      "Gradient Descent(7844/9999): loss=0.04396746955626912\n",
      "Gradient Descent(7845/9999): loss=0.04396746949193776\n",
      "Gradient Descent(7846/9999): loss=0.04396746942770756\n",
      "Gradient Descent(7847/9999): loss=0.04396746936357834\n",
      "Gradient Descent(7848/9999): loss=0.04396746929954997\n",
      "Gradient Descent(7849/9999): loss=0.04396746923562226\n",
      "Gradient Descent(7850/9999): loss=0.043967469171795066\n",
      "Gradient Descent(7851/9999): loss=0.04396746910806824\n",
      "Gradient Descent(7852/9999): loss=0.04396746904444159\n",
      "Gradient Descent(7853/9999): loss=0.04396746898091501\n",
      "Gradient Descent(7854/9999): loss=0.04396746891748832\n",
      "Gradient Descent(7855/9999): loss=0.043967468854161335\n",
      "Gradient Descent(7856/9999): loss=0.04396746879093392\n",
      "Gradient Descent(7857/9999): loss=0.043967468727805936\n",
      "Gradient Descent(7858/9999): loss=0.043967468664777194\n",
      "Gradient Descent(7859/9999): loss=0.043967468601847554\n",
      "Gradient Descent(7860/9999): loss=0.043967468539016875\n",
      "Gradient Descent(7861/9999): loss=0.043967468476284986\n",
      "Gradient Descent(7862/9999): loss=0.04396746841365171\n",
      "Gradient Descent(7863/9999): loss=0.043967468351116964\n",
      "Gradient Descent(7864/9999): loss=0.0439674682886805\n",
      "Gradient Descent(7865/9999): loss=0.043967468226342205\n",
      "Gradient Descent(7866/9999): loss=0.04396746816410195\n",
      "Gradient Descent(7867/9999): loss=0.043967468101959534\n",
      "Gradient Descent(7868/9999): loss=0.04396746803991485\n",
      "Gradient Descent(7869/9999): loss=0.043967467977967696\n",
      "Gradient Descent(7870/9999): loss=0.04396746791611796\n",
      "Gradient Descent(7871/9999): loss=0.04396746785436546\n",
      "Gradient Descent(7872/9999): loss=0.04396746779271008\n",
      "Gradient Descent(7873/9999): loss=0.043967467731151615\n",
      "Gradient Descent(7874/9999): loss=0.04396746766968997\n",
      "Gradient Descent(7875/9999): loss=0.04396746760832494\n",
      "Gradient Descent(7876/9999): loss=0.0439674675470564\n",
      "Gradient Descent(7877/9999): loss=0.0439674674858842\n",
      "Gradient Descent(7878/9999): loss=0.04396746742480818\n",
      "Gradient Descent(7879/9999): loss=0.0439674673638282\n",
      "Gradient Descent(7880/9999): loss=0.04396746730294411\n",
      "Gradient Descent(7881/9999): loss=0.043967467242155715\n",
      "Gradient Descent(7882/9999): loss=0.0439674671814629\n",
      "Gradient Descent(7883/9999): loss=0.043967467120865564\n",
      "Gradient Descent(7884/9999): loss=0.043967467060363453\n",
      "Gradient Descent(7885/9999): loss=0.04396746699995649\n",
      "Gradient Descent(7886/9999): loss=0.043967466939644505\n",
      "Gradient Descent(7887/9999): loss=0.04396746687942737\n",
      "Gradient Descent(7888/9999): loss=0.04396746681930491\n",
      "Gradient Descent(7889/9999): loss=0.043967466759276964\n",
      "Gradient Descent(7890/9999): loss=0.04396746669934341\n",
      "Gradient Descent(7891/9999): loss=0.0439674666395041\n",
      "Gradient Descent(7892/9999): loss=0.043967466579758856\n",
      "Gradient Descent(7893/9999): loss=0.043967466520107586\n",
      "Gradient Descent(7894/9999): loss=0.043967466460550095\n",
      "Gradient Descent(7895/9999): loss=0.04396746640108623\n",
      "Gradient Descent(7896/9999): loss=0.043967466341715875\n",
      "Gradient Descent(7897/9999): loss=0.04396746628243887\n",
      "Gradient Descent(7898/9999): loss=0.04396746622325507\n",
      "Gradient Descent(7899/9999): loss=0.04396746616416431\n",
      "Gradient Descent(7900/9999): loss=0.04396746610516648\n",
      "Gradient Descent(7901/9999): loss=0.04396746604626141\n",
      "Gradient Descent(7902/9999): loss=0.043967465987448946\n",
      "Gradient Descent(7903/9999): loss=0.04396746592872899\n",
      "Gradient Descent(7904/9999): loss=0.04396746587010134\n",
      "Gradient Descent(7905/9999): loss=0.04396746581156586\n",
      "Gradient Descent(7906/9999): loss=0.043967465753122416\n",
      "Gradient Descent(7907/9999): loss=0.04396746569477088\n",
      "Gradient Descent(7908/9999): loss=0.043967465636511084\n",
      "Gradient Descent(7909/9999): loss=0.04396746557834291\n",
      "Gradient Descent(7910/9999): loss=0.04396746552026618\n",
      "Gradient Descent(7911/9999): loss=0.043967465462280746\n",
      "Gradient Descent(7912/9999): loss=0.04396746540438652\n",
      "Gradient Descent(7913/9999): loss=0.043967465346583294\n",
      "Gradient Descent(7914/9999): loss=0.04396746528887098\n",
      "Gradient Descent(7915/9999): loss=0.0439674652312494\n",
      "Gradient Descent(7916/9999): loss=0.043967465173718405\n",
      "Gradient Descent(7917/9999): loss=0.043967465116277894\n",
      "Gradient Descent(7918/9999): loss=0.04396746505892768\n",
      "Gradient Descent(7919/9999): loss=0.04396746500166764\n",
      "Gradient Descent(7920/9999): loss=0.04396746494449764\n",
      "Gradient Descent(7921/9999): loss=0.04396746488741753\n",
      "Gradient Descent(7922/9999): loss=0.04396746483042716\n",
      "Gradient Descent(7923/9999): loss=0.0439674647735264\n",
      "Gradient Descent(7924/9999): loss=0.04396746471671513\n",
      "Gradient Descent(7925/9999): loss=0.04396746465999315\n",
      "Gradient Descent(7926/9999): loss=0.04396746460336038\n",
      "Gradient Descent(7927/9999): loss=0.04396746454681663\n",
      "Gradient Descent(7928/9999): loss=0.04396746449036182\n",
      "Gradient Descent(7929/9999): loss=0.04396746443399574\n",
      "Gradient Descent(7930/9999): loss=0.043967464377718306\n",
      "Gradient Descent(7931/9999): loss=0.04396746432152937\n",
      "Gradient Descent(7932/9999): loss=0.043967464265428774\n",
      "Gradient Descent(7933/9999): loss=0.043967464209416364\n",
      "Gradient Descent(7934/9999): loss=0.04396746415349205\n",
      "Gradient Descent(7935/9999): loss=0.04396746409765567\n",
      "Gradient Descent(7936/9999): loss=0.04396746404190708\n",
      "Gradient Descent(7937/9999): loss=0.04396746398624614\n",
      "Gradient Descent(7938/9999): loss=0.043967463930672704\n",
      "Gradient Descent(7939/9999): loss=0.04396746387518667\n",
      "Gradient Descent(7940/9999): loss=0.04396746381978787\n",
      "Gradient Descent(7941/9999): loss=0.043967463764476154\n",
      "Gradient Descent(7942/9999): loss=0.04396746370925144\n",
      "Gradient Descent(7943/9999): loss=0.04396746365411357\n",
      "Gradient Descent(7944/9999): loss=0.04396746359906235\n",
      "Gradient Descent(7945/9999): loss=0.04396746354409771\n",
      "Gradient Descent(7946/9999): loss=0.0439674634892195\n",
      "Gradient Descent(7947/9999): loss=0.04396746343442756\n",
      "Gradient Descent(7948/9999): loss=0.043967463379721794\n",
      "Gradient Descent(7949/9999): loss=0.04396746332510202\n",
      "Gradient Descent(7950/9999): loss=0.04396746327056813\n",
      "Gradient Descent(7951/9999): loss=0.04396746321611999\n",
      "Gradient Descent(7952/9999): loss=0.043967463161757474\n",
      "Gradient Descent(7953/9999): loss=0.04396746310748042\n",
      "Gradient Descent(7954/9999): loss=0.04396746305328871\n",
      "Gradient Descent(7955/9999): loss=0.043967462999182215\n",
      "Gradient Descent(7956/9999): loss=0.04396746294516077\n",
      "Gradient Descent(7957/9999): loss=0.04396746289122427\n",
      "Gradient Descent(7958/9999): loss=0.04396746283737259\n",
      "Gradient Descent(7959/9999): loss=0.04396746278360558\n",
      "Gradient Descent(7960/9999): loss=0.043967462729923114\n",
      "Gradient Descent(7961/9999): loss=0.04396746267632505\n",
      "Gradient Descent(7962/9999): loss=0.04396746262281125\n",
      "Gradient Descent(7963/9999): loss=0.04396746256938162\n",
      "Gradient Descent(7964/9999): loss=0.04396746251603597\n",
      "Gradient Descent(7965/9999): loss=0.04396746246277421\n",
      "Gradient Descent(7966/9999): loss=0.04396746240959619\n",
      "Gradient Descent(7967/9999): loss=0.043967462356501785\n",
      "Gradient Descent(7968/9999): loss=0.04396746230349088\n",
      "Gradient Descent(7969/9999): loss=0.043967462250563284\n",
      "Gradient Descent(7970/9999): loss=0.04396746219771894\n",
      "Gradient Descent(7971/9999): loss=0.04396746214495766\n",
      "Gradient Descent(7972/9999): loss=0.043967462092279364\n",
      "Gradient Descent(7973/9999): loss=0.04396746203968388\n",
      "Gradient Descent(7974/9999): loss=0.04396746198717111\n",
      "Gradient Descent(7975/9999): loss=0.043967461934740895\n",
      "Gradient Descent(7976/9999): loss=0.04396746188239313\n",
      "Gradient Descent(7977/9999): loss=0.04396746183012768\n",
      "Gradient Descent(7978/9999): loss=0.04396746177794438\n",
      "Gradient Descent(7979/9999): loss=0.043967461725843135\n",
      "Gradient Descent(7980/9999): loss=0.04396746167382381\n",
      "Gradient Descent(7981/9999): loss=0.043967461621886306\n",
      "Gradient Descent(7982/9999): loss=0.04396746157003044\n",
      "Gradient Descent(7983/9999): loss=0.043967461518256125\n",
      "Gradient Descent(7984/9999): loss=0.04396746146656319\n",
      "Gradient Descent(7985/9999): loss=0.043967461414951545\n",
      "Gradient Descent(7986/9999): loss=0.043967461363421056\n",
      "Gradient Descent(7987/9999): loss=0.04396746131197158\n",
      "Gradient Descent(7988/9999): loss=0.04396746126060301\n",
      "Gradient Descent(7989/9999): loss=0.04396746120931523\n",
      "Gradient Descent(7990/9999): loss=0.043967461158108054\n",
      "Gradient Descent(7991/9999): loss=0.04396746110698139\n",
      "Gradient Descent(7992/9999): loss=0.04396746105593513\n",
      "Gradient Descent(7993/9999): loss=0.04396746100496914\n",
      "Gradient Descent(7994/9999): loss=0.04396746095408327\n",
      "Gradient Descent(7995/9999): loss=0.04396746090327743\n",
      "Gradient Descent(7996/9999): loss=0.04396746085255146\n",
      "Gradient Descent(7997/9999): loss=0.04396746080190525\n",
      "Gradient Descent(7998/9999): loss=0.04396746075133867\n",
      "Gradient Descent(7999/9999): loss=0.04396746070085159\n",
      "Gradient Descent(8000/9999): loss=0.04396746065044392\n",
      "Gradient Descent(8001/9999): loss=0.04396746060011548\n",
      "Gradient Descent(8002/9999): loss=0.043967460549866194\n",
      "Gradient Descent(8003/9999): loss=0.043967460499695896\n",
      "Gradient Descent(8004/9999): loss=0.0439674604496045\n",
      "Gradient Descent(8005/9999): loss=0.043967460399591846\n",
      "Gradient Descent(8006/9999): loss=0.043967460349657844\n",
      "Gradient Descent(8007/9999): loss=0.04396746029980234\n",
      "Gradient Descent(8008/9999): loss=0.04396746025002523\n",
      "Gradient Descent(8009/9999): loss=0.04396746020032637\n",
      "Gradient Descent(8010/9999): loss=0.0439674601507057\n",
      "Gradient Descent(8011/9999): loss=0.043967460101163015\n",
      "Gradient Descent(8012/9999): loss=0.043967460051698264\n",
      "Gradient Descent(8013/9999): loss=0.04396746000231123\n",
      "Gradient Descent(8014/9999): loss=0.0439674599530019\n",
      "Gradient Descent(8015/9999): loss=0.04396745990377007\n",
      "Gradient Descent(8016/9999): loss=0.04396745985461565\n",
      "Gradient Descent(8017/9999): loss=0.04396745980553853\n",
      "Gradient Descent(8018/9999): loss=0.04396745975653859\n",
      "Gradient Descent(8019/9999): loss=0.043967459707615654\n",
      "Gradient Descent(8020/9999): loss=0.04396745965876968\n",
      "Gradient Descent(8021/9999): loss=0.043967459610000495\n",
      "Gradient Descent(8022/9999): loss=0.04396745956130798\n",
      "Gradient Descent(8023/9999): loss=0.04396745951269204\n",
      "Gradient Descent(8024/9999): loss=0.04396745946415251\n",
      "Gradient Descent(8025/9999): loss=0.04396745941568934\n",
      "Gradient Descent(8026/9999): loss=0.04396745936730235\n",
      "Gradient Descent(8027/9999): loss=0.043967459318991445\n",
      "Gradient Descent(8028/9999): loss=0.04396745927075653\n",
      "Gradient Descent(8029/9999): loss=0.04396745922259741\n",
      "Gradient Descent(8030/9999): loss=0.04396745917451403\n",
      "Gradient Descent(8031/9999): loss=0.04396745912650624\n",
      "Gradient Descent(8032/9999): loss=0.04396745907857397\n",
      "Gradient Descent(8033/9999): loss=0.04396745903071702\n",
      "Gradient Descent(8034/9999): loss=0.04396745898293536\n",
      "Gradient Descent(8035/9999): loss=0.0439674589352288\n",
      "Gradient Descent(8036/9999): loss=0.04396745888759725\n",
      "Gradient Descent(8037/9999): loss=0.043967458840040614\n",
      "Gradient Descent(8038/9999): loss=0.04396745879255872\n",
      "Gradient Descent(8039/9999): loss=0.04396745874515153\n",
      "Gradient Descent(8040/9999): loss=0.043967458697818845\n",
      "Gradient Descent(8041/9999): loss=0.04396745865056058\n",
      "Gradient Descent(8042/9999): loss=0.04396745860337663\n",
      "Gradient Descent(8043/9999): loss=0.043967458556266895\n",
      "Gradient Descent(8044/9999): loss=0.0439674585092312\n",
      "Gradient Descent(8045/9999): loss=0.04396745846226947\n",
      "Gradient Descent(8046/9999): loss=0.04396745841538158\n",
      "Gradient Descent(8047/9999): loss=0.04396745836856739\n",
      "Gradient Descent(8048/9999): loss=0.04396745832182683\n",
      "Gradient Descent(8049/9999): loss=0.04396745827515976\n",
      "Gradient Descent(8050/9999): loss=0.04396745822856608\n",
      "Gradient Descent(8051/9999): loss=0.04396745818204564\n",
      "Gradient Descent(8052/9999): loss=0.04396745813559835\n",
      "Gradient Descent(8053/9999): loss=0.04396745808922411\n",
      "Gradient Descent(8054/9999): loss=0.043967458042922763\n",
      "Gradient Descent(8055/9999): loss=0.04396745799669422\n",
      "Gradient Descent(8056/9999): loss=0.04396745795053838\n",
      "Gradient Descent(8057/9999): loss=0.0439674579044551\n",
      "Gradient Descent(8058/9999): loss=0.043967457858444275\n",
      "Gradient Descent(8059/9999): loss=0.04396745781250579\n",
      "Gradient Descent(8060/9999): loss=0.04396745776663954\n",
      "Gradient Descent(8061/9999): loss=0.04396745772084543\n",
      "Gradient Descent(8062/9999): loss=0.043967457675123285\n",
      "Gradient Descent(8063/9999): loss=0.04396745762947307\n",
      "Gradient Descent(8064/9999): loss=0.04396745758389458\n",
      "Gradient Descent(8065/9999): loss=0.0439674575383878\n",
      "Gradient Descent(8066/9999): loss=0.04396745749295256\n",
      "Gradient Descent(8067/9999): loss=0.04396745744758876\n",
      "Gradient Descent(8068/9999): loss=0.04396745740229628\n",
      "Gradient Descent(8069/9999): loss=0.04396745735707504\n",
      "Gradient Descent(8070/9999): loss=0.04396745731192486\n",
      "Gradient Descent(8071/9999): loss=0.04396745726684569\n",
      "Gradient Descent(8072/9999): loss=0.043967457221837404\n",
      "Gradient Descent(8073/9999): loss=0.0439674571768999\n",
      "Gradient Descent(8074/9999): loss=0.04396745713203301\n",
      "Gradient Descent(8075/9999): loss=0.04396745708723672\n",
      "Gradient Descent(8076/9999): loss=0.043967457042510816\n",
      "Gradient Descent(8077/9999): loss=0.043967456997855266\n",
      "Gradient Descent(8078/9999): loss=0.04396745695326992\n",
      "Gradient Descent(8079/9999): loss=0.04396745690875468\n",
      "Gradient Descent(8080/9999): loss=0.04396745686430943\n",
      "Gradient Descent(8081/9999): loss=0.04396745681993406\n",
      "Gradient Descent(8082/9999): loss=0.04396745677562849\n",
      "Gradient Descent(8083/9999): loss=0.04396745673139255\n",
      "Gradient Descent(8084/9999): loss=0.04396745668722617\n",
      "Gradient Descent(8085/9999): loss=0.043967456643129246\n",
      "Gradient Descent(8086/9999): loss=0.04396745659910164\n",
      "Gradient Descent(8087/9999): loss=0.043967456555143274\n",
      "Gradient Descent(8088/9999): loss=0.043967456511254006\n",
      "Gradient Descent(8089/9999): loss=0.04396745646743376\n",
      "Gradient Descent(8090/9999): loss=0.043967456423682424\n",
      "Gradient Descent(8091/9999): loss=0.04396745637999988\n",
      "Gradient Descent(8092/9999): loss=0.043967456336386004\n",
      "Gradient Descent(8093/9999): loss=0.04396745629284069\n",
      "Gradient Descent(8094/9999): loss=0.04396745624936385\n",
      "Gradient Descent(8095/9999): loss=0.043967456205955414\n",
      "Gradient Descent(8096/9999): loss=0.04396745616261519\n",
      "Gradient Descent(8097/9999): loss=0.043967456119343135\n",
      "Gradient Descent(8098/9999): loss=0.04396745607613911\n",
      "Gradient Descent(8099/9999): loss=0.043967456033003\n",
      "Gradient Descent(8100/9999): loss=0.04396745598993472\n",
      "Gradient Descent(8101/9999): loss=0.04396745594693416\n",
      "Gradient Descent(8102/9999): loss=0.04396745590400122\n",
      "Gradient Descent(8103/9999): loss=0.043967455861135776\n",
      "Gradient Descent(8104/9999): loss=0.043967455818337706\n",
      "Gradient Descent(8105/9999): loss=0.04396745577560697\n",
      "Gradient Descent(8106/9999): loss=0.0439674557329434\n",
      "Gradient Descent(8107/9999): loss=0.043967455690346935\n",
      "Gradient Descent(8108/9999): loss=0.04396745564781743\n",
      "Gradient Descent(8109/9999): loss=0.0439674556053548\n",
      "Gradient Descent(8110/9999): loss=0.04396745556295893\n",
      "Gradient Descent(8111/9999): loss=0.043967455520629715\n",
      "Gradient Descent(8112/9999): loss=0.04396745547836705\n",
      "Gradient Descent(8113/9999): loss=0.04396745543617086\n",
      "Gradient Descent(8114/9999): loss=0.043967455394041004\n",
      "Gradient Descent(8115/9999): loss=0.04396745535197738\n",
      "Gradient Descent(8116/9999): loss=0.043967455309979926\n",
      "Gradient Descent(8117/9999): loss=0.04396745526804844\n",
      "Gradient Descent(8118/9999): loss=0.043967455226182964\n",
      "Gradient Descent(8119/9999): loss=0.04396745518438326\n",
      "Gradient Descent(8120/9999): loss=0.04396745514264931\n",
      "Gradient Descent(8121/9999): loss=0.04396745510098097\n",
      "Gradient Descent(8122/9999): loss=0.04396745505937814\n",
      "Gradient Descent(8123/9999): loss=0.043967455017840724\n",
      "Gradient Descent(8124/9999): loss=0.043967454976368614\n",
      "Gradient Descent(8125/9999): loss=0.04396745493496173\n",
      "Gradient Descent(8126/9999): loss=0.04396745489361992\n",
      "Gradient Descent(8127/9999): loss=0.04396745485234315\n",
      "Gradient Descent(8128/9999): loss=0.04396745481113126\n",
      "Gradient Descent(8129/9999): loss=0.043967454769984184\n",
      "Gradient Descent(8130/9999): loss=0.043967454728901796\n",
      "Gradient Descent(8131/9999): loss=0.043967454687884\n",
      "Gradient Descent(8132/9999): loss=0.0439674546469307\n",
      "Gradient Descent(8133/9999): loss=0.04396745460604179\n",
      "Gradient Descent(8134/9999): loss=0.043967454565217175\n",
      "Gradient Descent(8135/9999): loss=0.04396745452445674\n",
      "Gradient Descent(8136/9999): loss=0.0439674544837604\n",
      "Gradient Descent(8137/9999): loss=0.04396745444312805\n",
      "Gradient Descent(8138/9999): loss=0.04396745440255958\n",
      "Gradient Descent(8139/9999): loss=0.04396745436205492\n",
      "Gradient Descent(8140/9999): loss=0.04396745432161391\n",
      "Gradient Descent(8141/9999): loss=0.04396745428123653\n",
      "Gradient Descent(8142/9999): loss=0.043967454240922604\n",
      "Gradient Descent(8143/9999): loss=0.04396745420067207\n",
      "Gradient Descent(8144/9999): loss=0.04396745416048484\n",
      "Gradient Descent(8145/9999): loss=0.0439674541203608\n",
      "Gradient Descent(8146/9999): loss=0.043967454080299805\n",
      "Gradient Descent(8147/9999): loss=0.04396745404030183\n",
      "Gradient Descent(8148/9999): loss=0.04396745400036676\n",
      "Gradient Descent(8149/9999): loss=0.043967453960494465\n",
      "Gradient Descent(8150/9999): loss=0.043967453920684865\n",
      "Gradient Descent(8151/9999): loss=0.04396745388093787\n",
      "Gradient Descent(8152/9999): loss=0.04396745384125334\n",
      "Gradient Descent(8153/9999): loss=0.04396745380163123\n",
      "Gradient Descent(8154/9999): loss=0.0439674537620714\n",
      "Gradient Descent(8155/9999): loss=0.0439674537225738\n",
      "Gradient Descent(8156/9999): loss=0.043967453683138286\n",
      "Gradient Descent(8157/9999): loss=0.04396745364376477\n",
      "Gradient Descent(8158/9999): loss=0.0439674536044532\n",
      "Gradient Descent(8159/9999): loss=0.04396745356520339\n",
      "Gradient Descent(8160/9999): loss=0.04396745352601532\n",
      "Gradient Descent(8161/9999): loss=0.04396745348688886\n",
      "Gradient Descent(8162/9999): loss=0.04396745344782392\n",
      "Gradient Descent(8163/9999): loss=0.04396745340882041\n",
      "Gradient Descent(8164/9999): loss=0.04396745336987823\n",
      "Gradient Descent(8165/9999): loss=0.043967453330997275\n",
      "Gradient Descent(8166/9999): loss=0.04396745329217746\n",
      "Gradient Descent(8167/9999): loss=0.043967453253418665\n",
      "Gradient Descent(8168/9999): loss=0.043967453214720835\n",
      "Gradient Descent(8169/9999): loss=0.04396745317608382\n",
      "Gradient Descent(8170/9999): loss=0.04396745313750757\n",
      "Gradient Descent(8171/9999): loss=0.04396745309899197\n",
      "Gradient Descent(8172/9999): loss=0.04396745306053695\n",
      "Gradient Descent(8173/9999): loss=0.04396745302214236\n",
      "Gradient Descent(8174/9999): loss=0.04396745298380818\n",
      "Gradient Descent(8175/9999): loss=0.04396745294553425\n",
      "Gradient Descent(8176/9999): loss=0.04396745290732052\n",
      "Gradient Descent(8177/9999): loss=0.04396745286916684\n",
      "Gradient Descent(8178/9999): loss=0.043967452831073194\n",
      "Gradient Descent(8179/9999): loss=0.0439674527930394\n",
      "Gradient Descent(8180/9999): loss=0.043967452755065424\n",
      "Gradient Descent(8181/9999): loss=0.04396745271715117\n",
      "Gradient Descent(8182/9999): loss=0.04396745267929651\n",
      "Gradient Descent(8183/9999): loss=0.04396745264150138\n",
      "Gradient Descent(8184/9999): loss=0.04396745260376567\n",
      "Gradient Descent(8185/9999): loss=0.043967452566089305\n",
      "Gradient Descent(8186/9999): loss=0.04396745252847216\n",
      "Gradient Descent(8187/9999): loss=0.04396745249091419\n",
      "Gradient Descent(8188/9999): loss=0.04396745245341526\n",
      "Gradient Descent(8189/9999): loss=0.04396745241597528\n",
      "Gradient Descent(8190/9999): loss=0.04396745237859419\n",
      "Gradient Descent(8191/9999): loss=0.043967452341271834\n",
      "Gradient Descent(8192/9999): loss=0.0439674523040082\n",
      "Gradient Descent(8193/9999): loss=0.043967452266803146\n",
      "Gradient Descent(8194/9999): loss=0.043967452229656596\n",
      "Gradient Descent(8195/9999): loss=0.04396745219256843\n",
      "Gradient Descent(8196/9999): loss=0.0439674521555386\n",
      "Gradient Descent(8197/9999): loss=0.043967452118566994\n",
      "Gradient Descent(8198/9999): loss=0.043967452081653514\n",
      "Gradient Descent(8199/9999): loss=0.043967452044798086\n",
      "Gradient Descent(8200/9999): loss=0.043967452008000604\n",
      "Gradient Descent(8201/9999): loss=0.04396745197126098\n",
      "Gradient Descent(8202/9999): loss=0.0439674519345791\n",
      "Gradient Descent(8203/9999): loss=0.043967451897954916\n",
      "Gradient Descent(8204/9999): loss=0.043967451861388306\n",
      "Gradient Descent(8205/9999): loss=0.04396745182487919\n",
      "Gradient Descent(8206/9999): loss=0.043967451788427495\n",
      "Gradient Descent(8207/9999): loss=0.04396745175203312\n",
      "Gradient Descent(8208/9999): loss=0.043967451715695945\n",
      "Gradient Descent(8209/9999): loss=0.043967451679415924\n",
      "Gradient Descent(8210/9999): loss=0.04396745164319293\n",
      "Gradient Descent(8211/9999): loss=0.04396745160702689\n",
      "Gradient Descent(8212/9999): loss=0.043967451570917734\n",
      "Gradient Descent(8213/9999): loss=0.04396745153486534\n",
      "Gradient Descent(8214/9999): loss=0.04396745149886963\n",
      "Gradient Descent(8215/9999): loss=0.04396745146293053\n",
      "Gradient Descent(8216/9999): loss=0.04396745142704792\n",
      "Gradient Descent(8217/9999): loss=0.043967451391221754\n",
      "Gradient Descent(8218/9999): loss=0.04396745135545187\n",
      "Gradient Descent(8219/9999): loss=0.04396745131973828\n",
      "Gradient Descent(8220/9999): loss=0.043967451284080826\n",
      "Gradient Descent(8221/9999): loss=0.043967451248479436\n",
      "Gradient Descent(8222/9999): loss=0.043967451212934036\n",
      "Gradient Descent(8223/9999): loss=0.04396745117744452\n",
      "Gradient Descent(8224/9999): loss=0.04396745114201079\n",
      "Gradient Descent(8225/9999): loss=0.043967451106632784\n",
      "Gradient Descent(8226/9999): loss=0.0439674510713104\n",
      "Gradient Descent(8227/9999): loss=0.04396745103604356\n",
      "Gradient Descent(8228/9999): loss=0.04396745100083217\n",
      "Gradient Descent(8229/9999): loss=0.04396745096567614\n",
      "Gradient Descent(8230/9999): loss=0.04396745093057539\n",
      "Gradient Descent(8231/9999): loss=0.043967450895529836\n",
      "Gradient Descent(8232/9999): loss=0.04396745086053937\n",
      "Gradient Descent(8233/9999): loss=0.043967450825603945\n",
      "Gradient Descent(8234/9999): loss=0.04396745079072342\n",
      "Gradient Descent(8235/9999): loss=0.043967450755897766\n",
      "Gradient Descent(8236/9999): loss=0.04396745072112686\n",
      "Gradient Descent(8237/9999): loss=0.04396745068641061\n",
      "Gradient Descent(8238/9999): loss=0.04396745065174896\n",
      "Gradient Descent(8239/9999): loss=0.043967450617141815\n",
      "Gradient Descent(8240/9999): loss=0.043967450582589065\n",
      "Gradient Descent(8241/9999): loss=0.04396745054809065\n",
      "Gradient Descent(8242/9999): loss=0.0439674505136465\n",
      "Gradient Descent(8243/9999): loss=0.043967450479256494\n",
      "Gradient Descent(8244/9999): loss=0.04396745044492056\n",
      "Gradient Descent(8245/9999): loss=0.0439674504106386\n",
      "Gradient Descent(8246/9999): loss=0.04396745037641053\n",
      "Gradient Descent(8247/9999): loss=0.04396745034223632\n",
      "Gradient Descent(8248/9999): loss=0.04396745030811583\n",
      "Gradient Descent(8249/9999): loss=0.04396745027404899\n",
      "Gradient Descent(8250/9999): loss=0.04396745024003572\n",
      "Gradient Descent(8251/9999): loss=0.04396745020607591\n",
      "Gradient Descent(8252/9999): loss=0.04396745017216949\n",
      "Gradient Descent(8253/9999): loss=0.04396745013831642\n",
      "Gradient Descent(8254/9999): loss=0.043967450104516546\n",
      "Gradient Descent(8255/9999): loss=0.04396745007076982\n",
      "Gradient Descent(8256/9999): loss=0.04396745003707618\n",
      "Gradient Descent(8257/9999): loss=0.04396745000343549\n",
      "Gradient Descent(8258/9999): loss=0.0439674499698477\n",
      "Gradient Descent(8259/9999): loss=0.043967449936312714\n",
      "Gradient Descent(8260/9999): loss=0.04396744990283047\n",
      "Gradient Descent(8261/9999): loss=0.04396744986940085\n",
      "Gradient Descent(8262/9999): loss=0.04396744983602383\n",
      "Gradient Descent(8263/9999): loss=0.043967449802699274\n",
      "Gradient Descent(8264/9999): loss=0.0439674497694271\n",
      "Gradient Descent(8265/9999): loss=0.04396744973620725\n",
      "Gradient Descent(8266/9999): loss=0.04396744970303963\n",
      "Gradient Descent(8267/9999): loss=0.04396744966992418\n",
      "Gradient Descent(8268/9999): loss=0.04396744963686078\n",
      "Gradient Descent(8269/9999): loss=0.04396744960384936\n",
      "Gradient Descent(8270/9999): loss=0.04396744957088987\n",
      "Gradient Descent(8271/9999): loss=0.04396744953798218\n",
      "Gradient Descent(8272/9999): loss=0.04396744950512624\n",
      "Gradient Descent(8273/9999): loss=0.04396744947232196\n",
      "Gradient Descent(8274/9999): loss=0.04396744943956926\n",
      "Gradient Descent(8275/9999): loss=0.04396744940686805\n",
      "Gradient Descent(8276/9999): loss=0.04396744937421827\n",
      "Gradient Descent(8277/9999): loss=0.043967449341619835\n",
      "Gradient Descent(8278/9999): loss=0.04396744930907265\n",
      "Gradient Descent(8279/9999): loss=0.04396744927657663\n",
      "Gradient Descent(8280/9999): loss=0.04396744924413173\n",
      "Gradient Descent(8281/9999): loss=0.04396744921173779\n",
      "Gradient Descent(8282/9999): loss=0.043967449179394796\n",
      "Gradient Descent(8283/9999): loss=0.04396744914710269\n",
      "Gradient Descent(8284/9999): loss=0.04396744911486135\n",
      "Gradient Descent(8285/9999): loss=0.04396744908267069\n",
      "Gradient Descent(8286/9999): loss=0.04396744905053067\n",
      "Gradient Descent(8287/9999): loss=0.04396744901844115\n",
      "Gradient Descent(8288/9999): loss=0.04396744898640211\n",
      "Gradient Descent(8289/9999): loss=0.04396744895441342\n",
      "Gradient Descent(8290/9999): loss=0.04396744892247506\n",
      "Gradient Descent(8291/9999): loss=0.04396744889058688\n",
      "Gradient Descent(8292/9999): loss=0.043967448858748884\n",
      "Gradient Descent(8293/9999): loss=0.04396744882696093\n",
      "Gradient Descent(8294/9999): loss=0.04396744879522295\n",
      "Gradient Descent(8295/9999): loss=0.04396744876353487\n",
      "Gradient Descent(8296/9999): loss=0.043967448731896626\n",
      "Gradient Descent(8297/9999): loss=0.0439674487003081\n",
      "Gradient Descent(8298/9999): loss=0.04396744866876928\n",
      "Gradient Descent(8299/9999): loss=0.04396744863728003\n",
      "Gradient Descent(8300/9999): loss=0.0439674486058403\n",
      "Gradient Descent(8301/9999): loss=0.043967448574449994\n",
      "Gradient Descent(8302/9999): loss=0.04396744854310905\n",
      "Gradient Descent(8303/9999): loss=0.043967448511817386\n",
      "Gradient Descent(8304/9999): loss=0.043967448480574926\n",
      "Gradient Descent(8305/9999): loss=0.04396744844938156\n",
      "Gradient Descent(8306/9999): loss=0.04396744841823728\n",
      "Gradient Descent(8307/9999): loss=0.04396744838714195\n",
      "Gradient Descent(8308/9999): loss=0.04396744835609551\n",
      "Gradient Descent(8309/9999): loss=0.04396744832509787\n",
      "Gradient Descent(8310/9999): loss=0.04396744829414901\n",
      "Gradient Descent(8311/9999): loss=0.04396744826324879\n",
      "Gradient Descent(8312/9999): loss=0.04396744823239716\n",
      "Gradient Descent(8313/9999): loss=0.04396744820159403\n",
      "Gradient Descent(8314/9999): loss=0.04396744817083933\n",
      "Gradient Descent(8315/9999): loss=0.043967448140133\n",
      "Gradient Descent(8316/9999): loss=0.04396744810947495\n",
      "Gradient Descent(8317/9999): loss=0.043967448078865094\n",
      "Gradient Descent(8318/9999): loss=0.043967448048303374\n",
      "Gradient Descent(8319/9999): loss=0.043967448017789706\n",
      "Gradient Descent(8320/9999): loss=0.043967447987324014\n",
      "Gradient Descent(8321/9999): loss=0.04396744795690623\n",
      "Gradient Descent(8322/9999): loss=0.04396744792653627\n",
      "Gradient Descent(8323/9999): loss=0.04396744789621407\n",
      "Gradient Descent(8324/9999): loss=0.04396744786593954\n",
      "Gradient Descent(8325/9999): loss=0.043967447835712596\n",
      "Gradient Descent(8326/9999): loss=0.04396744780553319\n",
      "Gradient Descent(8327/9999): loss=0.04396744777540121\n",
      "Gradient Descent(8328/9999): loss=0.04396744774531667\n",
      "Gradient Descent(8329/9999): loss=0.043967447715279384\n",
      "Gradient Descent(8330/9999): loss=0.04396744768528935\n",
      "Gradient Descent(8331/9999): loss=0.043967447655346464\n",
      "Gradient Descent(8332/9999): loss=0.043967447625450656\n",
      "Gradient Descent(8333/9999): loss=0.043967447595601845\n",
      "Gradient Descent(8334/9999): loss=0.04396744756579996\n",
      "Gradient Descent(8335/9999): loss=0.04396744753604497\n",
      "Gradient Descent(8336/9999): loss=0.043967447506336715\n",
      "Gradient Descent(8337/9999): loss=0.043967447476675206\n",
      "Gradient Descent(8338/9999): loss=0.04396744744706034\n",
      "Gradient Descent(8339/9999): loss=0.04396744741749203\n",
      "Gradient Descent(8340/9999): loss=0.04396744738797019\n",
      "Gradient Descent(8341/9999): loss=0.04396744735849479\n",
      "Gradient Descent(8342/9999): loss=0.04396744732906574\n",
      "Gradient Descent(8343/9999): loss=0.04396744729968294\n",
      "Gradient Descent(8344/9999): loss=0.04396744727034637\n",
      "Gradient Descent(8345/9999): loss=0.043967447241055904\n",
      "Gradient Descent(8346/9999): loss=0.043967447211811485\n",
      "Gradient Descent(8347/9999): loss=0.04396744718261307\n",
      "Gradient Descent(8348/9999): loss=0.04396744715346054\n",
      "Gradient Descent(8349/9999): loss=0.043967447124353874\n",
      "Gradient Descent(8350/9999): loss=0.04396744709529297\n",
      "Gradient Descent(8351/9999): loss=0.04396744706627775\n",
      "Gradient Descent(8352/9999): loss=0.04396744703730815\n",
      "Gradient Descent(8353/9999): loss=0.04396744700838412\n",
      "Gradient Descent(8354/9999): loss=0.043967446979505526\n",
      "Gradient Descent(8355/9999): loss=0.043967446950672374\n",
      "Gradient Descent(8356/9999): loss=0.04396744692188455\n",
      "Gradient Descent(8357/9999): loss=0.04396744689314198\n",
      "Gradient Descent(8358/9999): loss=0.043967446864444606\n",
      "Gradient Descent(8359/9999): loss=0.04396744683579236\n",
      "Gradient Descent(8360/9999): loss=0.04396744680718516\n",
      "Gradient Descent(8361/9999): loss=0.043967446778622944\n",
      "Gradient Descent(8362/9999): loss=0.04396744675010564\n",
      "Gradient Descent(8363/9999): loss=0.04396744672163317\n",
      "Gradient Descent(8364/9999): loss=0.04396744669320547\n",
      "Gradient Descent(8365/9999): loss=0.04396744666482247\n",
      "Gradient Descent(8366/9999): loss=0.04396744663648409\n",
      "Gradient Descent(8367/9999): loss=0.043967446608190276\n",
      "Gradient Descent(8368/9999): loss=0.04396744657994096\n",
      "Gradient Descent(8369/9999): loss=0.04396744655173603\n",
      "Gradient Descent(8370/9999): loss=0.043967446523575454\n",
      "Gradient Descent(8371/9999): loss=0.04396744649545918\n",
      "Gradient Descent(8372/9999): loss=0.04396744646738711\n",
      "Gradient Descent(8373/9999): loss=0.043967446439359154\n",
      "Gradient Descent(8374/9999): loss=0.04396744641137529\n",
      "Gradient Descent(8375/9999): loss=0.043967446383435416\n",
      "Gradient Descent(8376/9999): loss=0.04396744635553947\n",
      "Gradient Descent(8377/9999): loss=0.043967446327687384\n",
      "Gradient Descent(8378/9999): loss=0.0439674462998791\n",
      "Gradient Descent(8379/9999): loss=0.043967446272114546\n",
      "Gradient Descent(8380/9999): loss=0.04396744624439365\n",
      "Gradient Descent(8381/9999): loss=0.04396744621671631\n",
      "Gradient Descent(8382/9999): loss=0.04396744618908251\n",
      "Gradient Descent(8383/9999): loss=0.04396744616149215\n",
      "Gradient Descent(8384/9999): loss=0.0439674461339452\n",
      "Gradient Descent(8385/9999): loss=0.043967446106441516\n",
      "Gradient Descent(8386/9999): loss=0.04396744607898111\n",
      "Gradient Descent(8387/9999): loss=0.04396744605156386\n",
      "Gradient Descent(8388/9999): loss=0.043967446024189734\n",
      "Gradient Descent(8389/9999): loss=0.04396744599685865\n",
      "Gradient Descent(8390/9999): loss=0.04396744596957054\n",
      "Gradient Descent(8391/9999): loss=0.043967445942325344\n",
      "Gradient Descent(8392/9999): loss=0.04396744591512297\n",
      "Gradient Descent(8393/9999): loss=0.04396744588796337\n",
      "Gradient Descent(8394/9999): loss=0.043967445860846464\n",
      "Gradient Descent(8395/9999): loss=0.04396744583377221\n",
      "Gradient Descent(8396/9999): loss=0.04396744580674052\n",
      "Gradient Descent(8397/9999): loss=0.04396744577975134\n",
      "Gradient Descent(8398/9999): loss=0.0439674457528046\n",
      "Gradient Descent(8399/9999): loss=0.04396744572590022\n",
      "Gradient Descent(8400/9999): loss=0.04396744569903815\n",
      "Gradient Descent(8401/9999): loss=0.043967445672218296\n",
      "Gradient Descent(8402/9999): loss=0.04396744564544061\n",
      "Gradient Descent(8403/9999): loss=0.043967445618705046\n",
      "Gradient Descent(8404/9999): loss=0.04396744559201153\n",
      "Gradient Descent(8405/9999): loss=0.04396744556535997\n",
      "Gradient Descent(8406/9999): loss=0.0439674455387503\n",
      "Gradient Descent(8407/9999): loss=0.0439674455121825\n",
      "Gradient Descent(8408/9999): loss=0.04396744548565645\n",
      "Gradient Descent(8409/9999): loss=0.04396744545917211\n",
      "Gradient Descent(8410/9999): loss=0.04396744543272943\n",
      "Gradient Descent(8411/9999): loss=0.04396744540632832\n",
      "Gradient Descent(8412/9999): loss=0.04396744537996872\n",
      "Gradient Descent(8413/9999): loss=0.043967445353650575\n",
      "Gradient Descent(8414/9999): loss=0.0439674453273738\n",
      "Gradient Descent(8415/9999): loss=0.043967445301138317\n",
      "Gradient Descent(8416/9999): loss=0.04396744527494411\n",
      "Gradient Descent(8417/9999): loss=0.04396744524879108\n",
      "Gradient Descent(8418/9999): loss=0.043967445222679175\n",
      "Gradient Descent(8419/9999): loss=0.04396744519660833\n",
      "Gradient Descent(8420/9999): loss=0.04396744517057849\n",
      "Gradient Descent(8421/9999): loss=0.04396744514458956\n",
      "Gradient Descent(8422/9999): loss=0.04396744511864149\n",
      "Gradient Descent(8423/9999): loss=0.043967445092734224\n",
      "Gradient Descent(8424/9999): loss=0.04396744506686769\n",
      "Gradient Descent(8425/9999): loss=0.04396744504104183\n",
      "Gradient Descent(8426/9999): loss=0.04396744501525658\n",
      "Gradient Descent(8427/9999): loss=0.04396744498951189\n",
      "Gradient Descent(8428/9999): loss=0.043967444963807646\n",
      "Gradient Descent(8429/9999): loss=0.043967444938143835\n",
      "Gradient Descent(8430/9999): loss=0.04396744491252038\n",
      "Gradient Descent(8431/9999): loss=0.04396744488693721\n",
      "Gradient Descent(8432/9999): loss=0.04396744486139426\n",
      "Gradient Descent(8433/9999): loss=0.043967444835891475\n",
      "Gradient Descent(8434/9999): loss=0.043967444810428795\n",
      "Gradient Descent(8435/9999): loss=0.043967444785006124\n",
      "Gradient Descent(8436/9999): loss=0.04396744475962346\n",
      "Gradient Descent(8437/9999): loss=0.04396744473428069\n",
      "Gradient Descent(8438/9999): loss=0.043967444708977774\n",
      "Gradient Descent(8439/9999): loss=0.04396744468371463\n",
      "Gradient Descent(8440/9999): loss=0.04396744465849123\n",
      "Gradient Descent(8441/9999): loss=0.043967444633307456\n",
      "Gradient Descent(8442/9999): loss=0.043967444608163306\n",
      "Gradient Descent(8443/9999): loss=0.043967444583058686\n",
      "Gradient Descent(8444/9999): loss=0.04396744455799353\n",
      "Gradient Descent(8445/9999): loss=0.043967444532967796\n",
      "Gradient Descent(8446/9999): loss=0.0439674445079814\n",
      "Gradient Descent(8447/9999): loss=0.04396744448303428\n",
      "Gradient Descent(8448/9999): loss=0.04396744445812642\n",
      "Gradient Descent(8449/9999): loss=0.0439674444332577\n",
      "Gradient Descent(8450/9999): loss=0.04396744440842809\n",
      "Gradient Descent(8451/9999): loss=0.043967444383637526\n",
      "Gradient Descent(8452/9999): loss=0.043967444358885915\n",
      "Gradient Descent(8453/9999): loss=0.04396744433417325\n",
      "Gradient Descent(8454/9999): loss=0.04396744430949942\n",
      "Gradient Descent(8455/9999): loss=0.04396744428486441\n",
      "Gradient Descent(8456/9999): loss=0.04396744426026813\n",
      "Gradient Descent(8457/9999): loss=0.0439674442357105\n",
      "Gradient Descent(8458/9999): loss=0.04396744421119151\n",
      "Gradient Descent(8459/9999): loss=0.04396744418671105\n",
      "Gradient Descent(8460/9999): loss=0.04396744416226911\n",
      "Gradient Descent(8461/9999): loss=0.04396744413786556\n",
      "Gradient Descent(8462/9999): loss=0.04396744411350042\n",
      "Gradient Descent(8463/9999): loss=0.04396744408917355\n",
      "Gradient Descent(8464/9999): loss=0.04396744406488498\n",
      "Gradient Descent(8465/9999): loss=0.043967444040634546\n",
      "Gradient Descent(8466/9999): loss=0.04396744401642228\n",
      "Gradient Descent(8467/9999): loss=0.04396744399224807\n",
      "Gradient Descent(8468/9999): loss=0.04396744396811188\n",
      "Gradient Descent(8469/9999): loss=0.04396744394401362\n",
      "Gradient Descent(8470/9999): loss=0.04396744391995325\n",
      "Gradient Descent(8471/9999): loss=0.04396744389593074\n",
      "Gradient Descent(8472/9999): loss=0.04396744387194598\n",
      "Gradient Descent(8473/9999): loss=0.043967443847998944\n",
      "Gradient Descent(8474/9999): loss=0.04396744382408955\n",
      "Gradient Descent(8475/9999): loss=0.043967443800217755\n",
      "Gradient Descent(8476/9999): loss=0.04396744377638349\n",
      "Gradient Descent(8477/9999): loss=0.043967443752586696\n",
      "Gradient Descent(8478/9999): loss=0.04396744372882735\n",
      "Gradient Descent(8479/9999): loss=0.043967443705105316\n",
      "Gradient Descent(8480/9999): loss=0.04396744368142059\n",
      "Gradient Descent(8481/9999): loss=0.04396744365777315\n",
      "Gradient Descent(8482/9999): loss=0.043967443634162856\n",
      "Gradient Descent(8483/9999): loss=0.04396744361058966\n",
      "Gradient Descent(8484/9999): loss=0.043967443587053553\n",
      "Gradient Descent(8485/9999): loss=0.04396744356355446\n",
      "Gradient Descent(8486/9999): loss=0.04396744354009232\n",
      "Gradient Descent(8487/9999): loss=0.04396744351666706\n",
      "Gradient Descent(8488/9999): loss=0.043967443493278635\n",
      "Gradient Descent(8489/9999): loss=0.04396744346992697\n",
      "Gradient Descent(8490/9999): loss=0.043967443446612055\n",
      "Gradient Descent(8491/9999): loss=0.04396744342333376\n",
      "Gradient Descent(8492/9999): loss=0.0439674434000921\n",
      "Gradient Descent(8493/9999): loss=0.04396744337688697\n",
      "Gradient Descent(8494/9999): loss=0.0439674433537183\n",
      "Gradient Descent(8495/9999): loss=0.043967443330586116\n",
      "Gradient Descent(8496/9999): loss=0.04396744330749026\n",
      "Gradient Descent(8497/9999): loss=0.043967443284430724\n",
      "Gradient Descent(8498/9999): loss=0.04396744326140745\n",
      "Gradient Descent(8499/9999): loss=0.04396744323842037\n",
      "Gradient Descent(8500/9999): loss=0.043967443215469436\n",
      "Gradient Descent(8501/9999): loss=0.04396744319255461\n",
      "Gradient Descent(8502/9999): loss=0.04396744316967578\n",
      "Gradient Descent(8503/9999): loss=0.043967443146832944\n",
      "Gradient Descent(8504/9999): loss=0.043967443124026014\n",
      "Gradient Descent(8505/9999): loss=0.04396744310125496\n",
      "Gradient Descent(8506/9999): loss=0.043967443078519707\n",
      "Gradient Descent(8507/9999): loss=0.04396744305582018\n",
      "Gradient Descent(8508/9999): loss=0.04396744303315637\n",
      "Gradient Descent(8509/9999): loss=0.043967443010528164\n",
      "Gradient Descent(8510/9999): loss=0.04396744298793556\n",
      "Gradient Descent(8511/9999): loss=0.04396744296537847\n",
      "Gradient Descent(8512/9999): loss=0.04396744294285686\n",
      "Gradient Descent(8513/9999): loss=0.043967442920370664\n",
      "Gradient Descent(8514/9999): loss=0.043967442897919824\n",
      "Gradient Descent(8515/9999): loss=0.04396744287550427\n",
      "Gradient Descent(8516/9999): loss=0.043967442853123956\n",
      "Gradient Descent(8517/9999): loss=0.04396744283077884\n",
      "Gradient Descent(8518/9999): loss=0.043967442808468864\n",
      "Gradient Descent(8519/9999): loss=0.04396744278619394\n",
      "Gradient Descent(8520/9999): loss=0.04396744276395409\n",
      "Gradient Descent(8521/9999): loss=0.04396744274174915\n",
      "Gradient Descent(8522/9999): loss=0.043967442719579176\n",
      "Gradient Descent(8523/9999): loss=0.04396744269744403\n",
      "Gradient Descent(8524/9999): loss=0.04396744267534368\n",
      "Gradient Descent(8525/9999): loss=0.0439674426532781\n",
      "Gradient Descent(8526/9999): loss=0.043967442631247196\n",
      "Gradient Descent(8527/9999): loss=0.04396744260925098\n",
      "Gradient Descent(8528/9999): loss=0.04396744258728931\n",
      "Gradient Descent(8529/9999): loss=0.043967442565362155\n",
      "Gradient Descent(8530/9999): loss=0.04396744254346951\n",
      "Gradient Descent(8531/9999): loss=0.043967442521611264\n",
      "Gradient Descent(8532/9999): loss=0.04396744249978741\n",
      "Gradient Descent(8533/9999): loss=0.04396744247799785\n",
      "Gradient Descent(8534/9999): loss=0.04396744245624256\n",
      "Gradient Descent(8535/9999): loss=0.043967442434521484\n",
      "Gradient Descent(8536/9999): loss=0.04396744241283454\n",
      "Gradient Descent(8537/9999): loss=0.043967442391181694\n",
      "Gradient Descent(8538/9999): loss=0.043967442369562904\n",
      "Gradient Descent(8539/9999): loss=0.043967442347978114\n",
      "Gradient Descent(8540/9999): loss=0.04396744232642725\n",
      "Gradient Descent(8541/9999): loss=0.0439674423049103\n",
      "Gradient Descent(8542/9999): loss=0.04396744228342714\n",
      "Gradient Descent(8543/9999): loss=0.04396744226197777\n",
      "Gradient Descent(8544/9999): loss=0.04396744224056214\n",
      "Gradient Descent(8545/9999): loss=0.04396744221918017\n",
      "Gradient Descent(8546/9999): loss=0.043967442197831844\n",
      "Gradient Descent(8547/9999): loss=0.043967442176517046\n",
      "Gradient Descent(8548/9999): loss=0.04396744215523579\n",
      "Gradient Descent(8549/9999): loss=0.04396744213398798\n",
      "Gradient Descent(8550/9999): loss=0.0439674421127736\n",
      "Gradient Descent(8551/9999): loss=0.04396744209159256\n",
      "Gradient Descent(8552/9999): loss=0.04396744207044481\n",
      "Gradient Descent(8553/9999): loss=0.04396744204933034\n",
      "Gradient Descent(8554/9999): loss=0.043967442028249044\n",
      "Gradient Descent(8555/9999): loss=0.04396744200720092\n",
      "Gradient Descent(8556/9999): loss=0.04396744198618586\n",
      "Gradient Descent(8557/9999): loss=0.043967441965203864\n",
      "Gradient Descent(8558/9999): loss=0.04396744194425486\n",
      "Gradient Descent(8559/9999): loss=0.04396744192333878\n",
      "Gradient Descent(8560/9999): loss=0.04396744190245561\n",
      "Gradient Descent(8561/9999): loss=0.04396744188160526\n",
      "Gradient Descent(8562/9999): loss=0.043967441860787694\n",
      "Gradient Descent(8563/9999): loss=0.04396744184000286\n",
      "Gradient Descent(8564/9999): loss=0.0439674418192507\n",
      "Gradient Descent(8565/9999): loss=0.04396744179853118\n",
      "Gradient Descent(8566/9999): loss=0.04396744177784423\n",
      "Gradient Descent(8567/9999): loss=0.043967441757189835\n",
      "Gradient Descent(8568/9999): loss=0.04396744173656788\n",
      "Gradient Descent(8569/9999): loss=0.04396744171597836\n",
      "Gradient Descent(8570/9999): loss=0.04396744169542122\n",
      "Gradient Descent(8571/9999): loss=0.0439674416748964\n",
      "Gradient Descent(8572/9999): loss=0.04396744165440385\n",
      "Gradient Descent(8573/9999): loss=0.043967441633943526\n",
      "Gradient Descent(8574/9999): loss=0.04396744161351536\n",
      "Gradient Descent(8575/9999): loss=0.043967441593119314\n",
      "Gradient Descent(8576/9999): loss=0.04396744157275536\n",
      "Gradient Descent(8577/9999): loss=0.0439674415524234\n",
      "Gradient Descent(8578/9999): loss=0.04396744153212342\n",
      "Gradient Descent(8579/9999): loss=0.043967441511855346\n",
      "Gradient Descent(8580/9999): loss=0.043967441491619165\n",
      "Gradient Descent(8581/9999): loss=0.04396744147141478\n",
      "Gradient Descent(8582/9999): loss=0.043967441451242185\n",
      "Gradient Descent(8583/9999): loss=0.04396744143110128\n",
      "Gradient Descent(8584/9999): loss=0.04396744141099206\n",
      "Gradient Descent(8585/9999): loss=0.043967441390914457\n",
      "Gradient Descent(8586/9999): loss=0.043967441370868415\n",
      "Gradient Descent(8587/9999): loss=0.0439674413508539\n",
      "Gradient Descent(8588/9999): loss=0.043967441330870854\n",
      "Gradient Descent(8589/9999): loss=0.04396744131091923\n",
      "Gradient Descent(8590/9999): loss=0.043967441290999\n",
      "Gradient Descent(8591/9999): loss=0.04396744127111005\n",
      "Gradient Descent(8592/9999): loss=0.043967441251252376\n",
      "Gradient Descent(8593/9999): loss=0.04396744123142594\n",
      "Gradient Descent(8594/9999): loss=0.04396744121163068\n",
      "Gradient Descent(8595/9999): loss=0.04396744119186653\n",
      "Gradient Descent(8596/9999): loss=0.04396744117213346\n",
      "Gradient Descent(8597/9999): loss=0.04396744115243143\n",
      "Gradient Descent(8598/9999): loss=0.043967441132760354\n",
      "Gradient Descent(8599/9999): loss=0.04396744111312024\n",
      "Gradient Descent(8600/9999): loss=0.043967441093511\n",
      "Gradient Descent(8601/9999): loss=0.04396744107393257\n",
      "Gradient Descent(8602/9999): loss=0.04396744105438492\n",
      "Gradient Descent(8603/9999): loss=0.04396744103486803\n",
      "Gradient Descent(8604/9999): loss=0.0439674410153818\n",
      "Gradient Descent(8605/9999): loss=0.04396744099592624\n",
      "Gradient Descent(8606/9999): loss=0.04396744097650127\n",
      "Gradient Descent(8607/9999): loss=0.04396744095710683\n",
      "Gradient Descent(8608/9999): loss=0.04396744093774288\n",
      "Gradient Descent(8609/9999): loss=0.04396744091840937\n",
      "Gradient Descent(8610/9999): loss=0.0439674408991063\n",
      "Gradient Descent(8611/9999): loss=0.04396744087983353\n",
      "Gradient Descent(8612/9999): loss=0.04396744086059108\n",
      "Gradient Descent(8613/9999): loss=0.043967440841378894\n",
      "Gradient Descent(8614/9999): loss=0.04396744082219692\n",
      "Gradient Descent(8615/9999): loss=0.0439674408030451\n",
      "Gradient Descent(8616/9999): loss=0.04396744078392339\n",
      "Gradient Descent(8617/9999): loss=0.04396744076483175\n",
      "Gradient Descent(8618/9999): loss=0.04396744074577013\n",
      "Gradient Descent(8619/9999): loss=0.04396744072673846\n",
      "Gradient Descent(8620/9999): loss=0.04396744070773672\n",
      "Gradient Descent(8621/9999): loss=0.04396744068876487\n",
      "Gradient Descent(8622/9999): loss=0.04396744066982285\n",
      "Gradient Descent(8623/9999): loss=0.043967440650910614\n",
      "Gradient Descent(8624/9999): loss=0.04396744063202811\n",
      "Gradient Descent(8625/9999): loss=0.043967440613175306\n",
      "Gradient Descent(8626/9999): loss=0.04396744059435212\n",
      "Gradient Descent(8627/9999): loss=0.04396744057555855\n",
      "Gradient Descent(8628/9999): loss=0.04396744055679455\n",
      "Gradient Descent(8629/9999): loss=0.04396744053806\n",
      "Gradient Descent(8630/9999): loss=0.04396744051935493\n",
      "Gradient Descent(8631/9999): loss=0.04396744050067929\n",
      "Gradient Descent(8632/9999): loss=0.043967440482033\n",
      "Gradient Descent(8633/9999): loss=0.04396744046341603\n",
      "Gradient Descent(8634/9999): loss=0.04396744044482834\n",
      "Gradient Descent(8635/9999): loss=0.04396744042626987\n",
      "Gradient Descent(8636/9999): loss=0.04396744040774058\n",
      "Gradient Descent(8637/9999): loss=0.0439674403892404\n",
      "Gradient Descent(8638/9999): loss=0.043967440370769335\n",
      "Gradient Descent(8639/9999): loss=0.0439674403523273\n",
      "Gradient Descent(8640/9999): loss=0.04396744033391428\n",
      "Gradient Descent(8641/9999): loss=0.043967440315530196\n",
      "Gradient Descent(8642/9999): loss=0.043967440297175024\n",
      "Gradient Descent(8643/9999): loss=0.043967440278848705\n",
      "Gradient Descent(8644/9999): loss=0.043967440260551216\n",
      "Gradient Descent(8645/9999): loss=0.043967440242282496\n",
      "Gradient Descent(8646/9999): loss=0.043967440224042476\n",
      "Gradient Descent(8647/9999): loss=0.04396744020583117\n",
      "Gradient Descent(8648/9999): loss=0.043967440187648484\n",
      "Gradient Descent(8649/9999): loss=0.04396744016949437\n",
      "Gradient Descent(8650/9999): loss=0.04396744015136881\n",
      "Gradient Descent(8651/9999): loss=0.04396744013327175\n",
      "Gradient Descent(8652/9999): loss=0.04396744011520316\n",
      "Gradient Descent(8653/9999): loss=0.04396744009716296\n",
      "Gradient Descent(8654/9999): loss=0.04396744007915114\n",
      "Gradient Descent(8655/9999): loss=0.04396744006116764\n",
      "Gradient Descent(8656/9999): loss=0.04396744004321241\n",
      "Gradient Descent(8657/9999): loss=0.04396744002528539\n",
      "Gradient Descent(8658/9999): loss=0.04396744000738659\n",
      "Gradient Descent(8659/9999): loss=0.04396743998951594\n",
      "Gradient Descent(8660/9999): loss=0.04396743997167335\n",
      "Gradient Descent(8661/9999): loss=0.043967439953858836\n",
      "Gradient Descent(8662/9999): loss=0.043967439936072335\n",
      "Gradient Descent(8663/9999): loss=0.04396743991831378\n",
      "Gradient Descent(8664/9999): loss=0.04396743990058317\n",
      "Gradient Descent(8665/9999): loss=0.04396743988288044\n",
      "Gradient Descent(8666/9999): loss=0.04396743986520552\n",
      "Gradient Descent(8667/9999): loss=0.04396743984755842\n",
      "Gradient Descent(8668/9999): loss=0.04396743982993906\n",
      "Gradient Descent(8669/9999): loss=0.04396743981234738\n",
      "Gradient Descent(8670/9999): loss=0.043967439794783375\n",
      "Gradient Descent(8671/9999): loss=0.04396743977724699\n",
      "Gradient Descent(8672/9999): loss=0.043967439759738186\n",
      "Gradient Descent(8673/9999): loss=0.04396743974225691\n",
      "Gradient Descent(8674/9999): loss=0.043967439724803124\n",
      "Gradient Descent(8675/9999): loss=0.04396743970737676\n",
      "Gradient Descent(8676/9999): loss=0.043967439689977794\n",
      "Gradient Descent(8677/9999): loss=0.0439674396726062\n",
      "Gradient Descent(8678/9999): loss=0.04396743965526191\n",
      "Gradient Descent(8679/9999): loss=0.04396743963794492\n",
      "Gradient Descent(8680/9999): loss=0.043967439620655115\n",
      "Gradient Descent(8681/9999): loss=0.04396743960339253\n",
      "Gradient Descent(8682/9999): loss=0.043967439586157064\n",
      "Gradient Descent(8683/9999): loss=0.04396743956894872\n",
      "Gradient Descent(8684/9999): loss=0.04396743955176743\n",
      "Gradient Descent(8685/9999): loss=0.043967439534613136\n",
      "Gradient Descent(8686/9999): loss=0.04396743951748583\n",
      "Gradient Descent(8687/9999): loss=0.04396743950038544\n",
      "Gradient Descent(8688/9999): loss=0.043967439483311936\n",
      "Gradient Descent(8689/9999): loss=0.043967439466265315\n",
      "Gradient Descent(8690/9999): loss=0.04396743944924546\n",
      "Gradient Descent(8691/9999): loss=0.04396743943225237\n",
      "Gradient Descent(8692/9999): loss=0.04396743941528602\n",
      "Gradient Descent(8693/9999): loss=0.04396743939834631\n",
      "Gradient Descent(8694/9999): loss=0.04396743938143326\n",
      "Gradient Descent(8695/9999): loss=0.0439674393645468\n",
      "Gradient Descent(8696/9999): loss=0.04396743934768688\n",
      "Gradient Descent(8697/9999): loss=0.04396743933085348\n",
      "Gradient Descent(8698/9999): loss=0.04396743931404656\n",
      "Gradient Descent(8699/9999): loss=0.043967439297266056\n",
      "Gradient Descent(8700/9999): loss=0.04396743928051193\n",
      "Gradient Descent(8701/9999): loss=0.043967439263784144\n",
      "Gradient Descent(8702/9999): loss=0.04396743924708267\n",
      "Gradient Descent(8703/9999): loss=0.04396743923040745\n",
      "Gradient Descent(8704/9999): loss=0.04396743921375844\n",
      "Gradient Descent(8705/9999): loss=0.04396743919713564\n",
      "Gradient Descent(8706/9999): loss=0.04396743918053895\n",
      "Gradient Descent(8707/9999): loss=0.043967439163968364\n",
      "Gradient Descent(8708/9999): loss=0.043967439147423806\n",
      "Gradient Descent(8709/9999): loss=0.043967439130905304\n",
      "Gradient Descent(8710/9999): loss=0.04396743911441277\n",
      "Gradient Descent(8711/9999): loss=0.04396743909794612\n",
      "Gradient Descent(8712/9999): loss=0.04396743908150541\n",
      "Gradient Descent(8713/9999): loss=0.04396743906509054\n",
      "Gradient Descent(8714/9999): loss=0.04396743904870148\n",
      "Gradient Descent(8715/9999): loss=0.043967439032338164\n",
      "Gradient Descent(8716/9999): loss=0.0439674390160006\n",
      "Gradient Descent(8717/9999): loss=0.04396743899968872\n",
      "Gradient Descent(8718/9999): loss=0.0439674389834025\n",
      "Gradient Descent(8719/9999): loss=0.04396743896714187\n",
      "Gradient Descent(8720/9999): loss=0.04396743895090682\n",
      "Gradient Descent(8721/9999): loss=0.043967438934697275\n",
      "Gradient Descent(8722/9999): loss=0.04396743891851322\n",
      "Gradient Descent(8723/9999): loss=0.04396743890235463\n",
      "Gradient Descent(8724/9999): loss=0.043967438886221455\n",
      "Gradient Descent(8725/9999): loss=0.04396743887011363\n",
      "Gradient Descent(8726/9999): loss=0.043967438854031114\n",
      "Gradient Descent(8727/9999): loss=0.043967438837973924\n",
      "Gradient Descent(8728/9999): loss=0.04396743882194193\n",
      "Gradient Descent(8729/9999): loss=0.04396743880593519\n",
      "Gradient Descent(8730/9999): loss=0.043967438789953614\n",
      "Gradient Descent(8731/9999): loss=0.043967438773997156\n",
      "Gradient Descent(8732/9999): loss=0.04396743875806578\n",
      "Gradient Descent(8733/9999): loss=0.04396743874215948\n",
      "Gradient Descent(8734/9999): loss=0.04396743872627815\n",
      "Gradient Descent(8735/9999): loss=0.043967438710421816\n",
      "Gradient Descent(8736/9999): loss=0.0439674386945904\n",
      "Gradient Descent(8737/9999): loss=0.043967438678783874\n",
      "Gradient Descent(8738/9999): loss=0.04396743866300223\n",
      "Gradient Descent(8739/9999): loss=0.04396743864724538\n",
      "Gradient Descent(8740/9999): loss=0.0439674386315133\n",
      "Gradient Descent(8741/9999): loss=0.043967438615805966\n",
      "Gradient Descent(8742/9999): loss=0.043967438600123324\n",
      "Gradient Descent(8743/9999): loss=0.04396743858446532\n",
      "Gradient Descent(8744/9999): loss=0.04396743856883198\n",
      "Gradient Descent(8745/9999): loss=0.043967438553223194\n",
      "Gradient Descent(8746/9999): loss=0.04396743853763894\n",
      "Gradient Descent(8747/9999): loss=0.04396743852207921\n",
      "Gradient Descent(8748/9999): loss=0.04396743850654395\n",
      "Gradient Descent(8749/9999): loss=0.04396743849103312\n",
      "Gradient Descent(8750/9999): loss=0.04396743847554666\n",
      "Gradient Descent(8751/9999): loss=0.04396743846008453\n",
      "Gradient Descent(8752/9999): loss=0.04396743844464675\n",
      "Gradient Descent(8753/9999): loss=0.0439674384292332\n",
      "Gradient Descent(8754/9999): loss=0.04396743841384394\n",
      "Gradient Descent(8755/9999): loss=0.04396743839847885\n",
      "Gradient Descent(8756/9999): loss=0.043967438383137915\n",
      "Gradient Descent(8757/9999): loss=0.04396743836782112\n",
      "Gradient Descent(8758/9999): loss=0.04396743835252839\n",
      "Gradient Descent(8759/9999): loss=0.0439674383372597\n",
      "Gradient Descent(8760/9999): loss=0.04396743832201505\n",
      "Gradient Descent(8761/9999): loss=0.04396743830679432\n",
      "Gradient Descent(8762/9999): loss=0.04396743829159757\n",
      "Gradient Descent(8763/9999): loss=0.04396743827642469\n",
      "Gradient Descent(8764/9999): loss=0.043967438261275665\n",
      "Gradient Descent(8765/9999): loss=0.04396743824615047\n",
      "Gradient Descent(8766/9999): loss=0.04396743823104903\n",
      "Gradient Descent(8767/9999): loss=0.04396743821597137\n",
      "Gradient Descent(8768/9999): loss=0.0439674382009174\n",
      "Gradient Descent(8769/9999): loss=0.0439674381858871\n",
      "Gradient Descent(8770/9999): loss=0.04396743817088043\n",
      "Gradient Descent(8771/9999): loss=0.043967438155897376\n",
      "Gradient Descent(8772/9999): loss=0.04396743814093786\n",
      "Gradient Descent(8773/9999): loss=0.04396743812600187\n",
      "Gradient Descent(8774/9999): loss=0.04396743811108936\n",
      "Gradient Descent(8775/9999): loss=0.0439674380962003\n",
      "Gradient Descent(8776/9999): loss=0.04396743808133466\n",
      "Gradient Descent(8777/9999): loss=0.04396743806649238\n",
      "Gradient Descent(8778/9999): loss=0.04396743805167344\n",
      "Gradient Descent(8779/9999): loss=0.04396743803687781\n",
      "Gradient Descent(8780/9999): loss=0.04396743802210544\n",
      "Gradient Descent(8781/9999): loss=0.04396743800735628\n",
      "Gradient Descent(8782/9999): loss=0.04396743799263032\n",
      "Gradient Descent(8783/9999): loss=0.04396743797792754\n",
      "Gradient Descent(8784/9999): loss=0.04396743796324785\n",
      "Gradient Descent(8785/9999): loss=0.04396743794859126\n",
      "Gradient Descent(8786/9999): loss=0.043967437933957694\n",
      "Gradient Descent(8787/9999): loss=0.04396743791934715\n",
      "Gradient Descent(8788/9999): loss=0.04396743790475957\n",
      "Gradient Descent(8789/9999): loss=0.04396743789019492\n",
      "Gradient Descent(8790/9999): loss=0.04396743787565319\n",
      "Gradient Descent(8791/9999): loss=0.04396743786113431\n",
      "Gradient Descent(8792/9999): loss=0.043967437846638265\n",
      "Gradient Descent(8793/9999): loss=0.043967437832165016\n",
      "Gradient Descent(8794/9999): loss=0.04396743781771452\n",
      "Gradient Descent(8795/9999): loss=0.043967437803286755\n",
      "Gradient Descent(8796/9999): loss=0.04396743778888165\n",
      "Gradient Descent(8797/9999): loss=0.043967437774499206\n",
      "Gradient Descent(8798/9999): loss=0.0439674377601394\n",
      "Gradient Descent(8799/9999): loss=0.04396743774580215\n",
      "Gradient Descent(8800/9999): loss=0.04396743773148743\n",
      "Gradient Descent(8801/9999): loss=0.04396743771719524\n",
      "Gradient Descent(8802/9999): loss=0.04396743770292552\n",
      "Gradient Descent(8803/9999): loss=0.043967437688678224\n",
      "Gradient Descent(8804/9999): loss=0.043967437674453345\n",
      "Gradient Descent(8805/9999): loss=0.04396743766025082\n",
      "Gradient Descent(8806/9999): loss=0.04396743764607064\n",
      "Gradient Descent(8807/9999): loss=0.043967437631912736\n",
      "Gradient Descent(8808/9999): loss=0.0439674376177771\n",
      "Gradient Descent(8809/9999): loss=0.0439674376036637\n",
      "Gradient Descent(8810/9999): loss=0.043967437589572486\n",
      "Gradient Descent(8811/9999): loss=0.04396743757550342\n",
      "Gradient Descent(8812/9999): loss=0.043967437561456484\n",
      "Gradient Descent(8813/9999): loss=0.04396743754743164\n",
      "Gradient Descent(8814/9999): loss=0.04396743753342883\n",
      "Gradient Descent(8815/9999): loss=0.04396743751944804\n",
      "Gradient Descent(8816/9999): loss=0.04396743750548926\n",
      "Gradient Descent(8817/9999): loss=0.0439674374915524\n",
      "Gradient Descent(8818/9999): loss=0.04396743747763747\n",
      "Gradient Descent(8819/9999): loss=0.043967437463744395\n",
      "Gradient Descent(8820/9999): loss=0.043967437449873185\n",
      "Gradient Descent(8821/9999): loss=0.04396743743602376\n",
      "Gradient Descent(8822/9999): loss=0.043967437422196165\n",
      "Gradient Descent(8823/9999): loss=0.04396743740839026\n",
      "Gradient Descent(8824/9999): loss=0.04396743739460609\n",
      "Gradient Descent(8825/9999): loss=0.04396743738084359\n",
      "Gradient Descent(8826/9999): loss=0.04396743736710269\n",
      "Gradient Descent(8827/9999): loss=0.04396743735338344\n",
      "Gradient Descent(8828/9999): loss=0.04396743733968575\n",
      "Gradient Descent(8829/9999): loss=0.04396743732600959\n",
      "Gradient Descent(8830/9999): loss=0.04396743731235493\n",
      "Gradient Descent(8831/9999): loss=0.04396743729872176\n",
      "Gradient Descent(8832/9999): loss=0.043967437285110024\n",
      "Gradient Descent(8833/9999): loss=0.043967437271519666\n",
      "Gradient Descent(8834/9999): loss=0.04396743725795069\n",
      "Gradient Descent(8835/9999): loss=0.04396743724440306\n",
      "Gradient Descent(8836/9999): loss=0.043967437230876705\n",
      "Gradient Descent(8837/9999): loss=0.04396743721737165\n",
      "Gradient Descent(8838/9999): loss=0.043967437203887814\n",
      "Gradient Descent(8839/9999): loss=0.04396743719042516\n",
      "Gradient Descent(8840/9999): loss=0.04396743717698372\n",
      "Gradient Descent(8841/9999): loss=0.043967437163563376\n",
      "Gradient Descent(8842/9999): loss=0.043967437150164136\n",
      "Gradient Descent(8843/9999): loss=0.043967437136785976\n",
      "Gradient Descent(8844/9999): loss=0.04396743712342885\n",
      "Gradient Descent(8845/9999): loss=0.043967437110092725\n",
      "Gradient Descent(8846/9999): loss=0.04396743709677756\n",
      "Gradient Descent(8847/9999): loss=0.043967437083483336\n",
      "Gradient Descent(8848/9999): loss=0.04396743707021003\n",
      "Gradient Descent(8849/9999): loss=0.043967437056957576\n",
      "Gradient Descent(8850/9999): loss=0.043967437043725965\n",
      "Gradient Descent(8851/9999): loss=0.04396743703051517\n",
      "Gradient Descent(8852/9999): loss=0.04396743701732511\n",
      "Gradient Descent(8853/9999): loss=0.04396743700415582\n",
      "Gradient Descent(8854/9999): loss=0.043967436991007226\n",
      "Gradient Descent(8855/9999): loss=0.04396743697787931\n",
      "Gradient Descent(8856/9999): loss=0.04396743696477204\n",
      "Gradient Descent(8857/9999): loss=0.043967436951685375\n",
      "Gradient Descent(8858/9999): loss=0.04396743693861929\n",
      "Gradient Descent(8859/9999): loss=0.04396743692557375\n",
      "Gradient Descent(8860/9999): loss=0.04396743691254871\n",
      "Gradient Descent(8861/9999): loss=0.04396743689954417\n",
      "Gradient Descent(8862/9999): loss=0.043967436886560046\n",
      "Gradient Descent(8863/9999): loss=0.04396743687359637\n",
      "Gradient Descent(8864/9999): loss=0.04396743686065305\n",
      "Gradient Descent(8865/9999): loss=0.0439674368477301\n",
      "Gradient Descent(8866/9999): loss=0.043967436834827456\n",
      "Gradient Descent(8867/9999): loss=0.043967436821945115\n",
      "Gradient Descent(8868/9999): loss=0.04396743680908302\n",
      "Gradient Descent(8869/9999): loss=0.04396743679624115\n",
      "Gradient Descent(8870/9999): loss=0.04396743678341948\n",
      "Gradient Descent(8871/9999): loss=0.04396743677061796\n",
      "Gradient Descent(8872/9999): loss=0.04396743675783657\n",
      "Gradient Descent(8873/9999): loss=0.04396743674507528\n",
      "Gradient Descent(8874/9999): loss=0.043967436732334046\n",
      "Gradient Descent(8875/9999): loss=0.043967436719612854\n",
      "Gradient Descent(8876/9999): loss=0.043967436706911646\n",
      "Gradient Descent(8877/9999): loss=0.04396743669423043\n",
      "Gradient Descent(8878/9999): loss=0.043967436681569154\n",
      "Gradient Descent(8879/9999): loss=0.04396743666892777\n",
      "Gradient Descent(8880/9999): loss=0.04396743665630626\n",
      "Gradient Descent(8881/9999): loss=0.043967436643704616\n",
      "Gradient Descent(8882/9999): loss=0.043967436631122785\n",
      "Gradient Descent(8883/9999): loss=0.04396743661856072\n",
      "Gradient Descent(8884/9999): loss=0.04396743660601841\n",
      "Gradient Descent(8885/9999): loss=0.043967436593495834\n",
      "Gradient Descent(8886/9999): loss=0.04396743658099293\n",
      "Gradient Descent(8887/9999): loss=0.043967436568509696\n",
      "Gradient Descent(8888/9999): loss=0.04396743655604609\n",
      "Gradient Descent(8889/9999): loss=0.043967436543602086\n",
      "Gradient Descent(8890/9999): loss=0.043967436531177635\n",
      "Gradient Descent(8891/9999): loss=0.04396743651877271\n",
      "Gradient Descent(8892/9999): loss=0.04396743650638731\n",
      "Gradient Descent(8893/9999): loss=0.04396743649402138\n",
      "Gradient Descent(8894/9999): loss=0.04396743648167489\n",
      "Gradient Descent(8895/9999): loss=0.04396743646934781\n",
      "Gradient Descent(8896/9999): loss=0.04396743645704013\n",
      "Gradient Descent(8897/9999): loss=0.04396743644475177\n",
      "Gradient Descent(8898/9999): loss=0.04396743643248276\n",
      "Gradient Descent(8899/9999): loss=0.04396743642023303\n",
      "Gradient Descent(8900/9999): loss=0.043967436408002575\n",
      "Gradient Descent(8901/9999): loss=0.04396743639579133\n",
      "Gradient Descent(8902/9999): loss=0.04396743638359928\n",
      "Gradient Descent(8903/9999): loss=0.043967436371426416\n",
      "Gradient Descent(8904/9999): loss=0.0439674363592727\n",
      "Gradient Descent(8905/9999): loss=0.043967436347138074\n",
      "Gradient Descent(8906/9999): loss=0.04396743633502254\n",
      "Gradient Descent(8907/9999): loss=0.04396743632292606\n",
      "Gradient Descent(8908/9999): loss=0.043967436310848554\n",
      "Gradient Descent(8909/9999): loss=0.0439674362987901\n",
      "Gradient Descent(8910/9999): loss=0.04396743628675059\n",
      "Gradient Descent(8911/9999): loss=0.04396743627472999\n",
      "Gradient Descent(8912/9999): loss=0.04396743626272829\n",
      "Gradient Descent(8913/9999): loss=0.04396743625074548\n",
      "Gradient Descent(8914/9999): loss=0.0439674362387815\n",
      "Gradient Descent(8915/9999): loss=0.04396743622683634\n",
      "Gradient Descent(8916/9999): loss=0.043967436214909936\n",
      "Gradient Descent(8917/9999): loss=0.04396743620300231\n",
      "Gradient Descent(8918/9999): loss=0.04396743619111341\n",
      "Gradient Descent(8919/9999): loss=0.04396743617924319\n",
      "Gradient Descent(8920/9999): loss=0.04396743616739165\n",
      "Gradient Descent(8921/9999): loss=0.04396743615555873\n",
      "Gradient Descent(8922/9999): loss=0.04396743614374441\n",
      "Gradient Descent(8923/9999): loss=0.04396743613194867\n",
      "Gradient Descent(8924/9999): loss=0.04396743612017148\n",
      "Gradient Descent(8925/9999): loss=0.043967436108412805\n",
      "Gradient Descent(8926/9999): loss=0.04396743609667262\n",
      "Gradient Descent(8927/9999): loss=0.04396743608495091\n",
      "Gradient Descent(8928/9999): loss=0.04396743607324761\n",
      "Gradient Descent(8929/9999): loss=0.04396743606156272\n",
      "Gradient Descent(8930/9999): loss=0.04396743604989618\n",
      "Gradient Descent(8931/9999): loss=0.04396743603824802\n",
      "Gradient Descent(8932/9999): loss=0.043967436026618165\n",
      "Gradient Descent(8933/9999): loss=0.04396743601500659\n",
      "Gradient Descent(8934/9999): loss=0.04396743600341327\n",
      "Gradient Descent(8935/9999): loss=0.04396743599183818\n",
      "Gradient Descent(8936/9999): loss=0.043967435980281265\n",
      "Gradient Descent(8937/9999): loss=0.04396743596874257\n",
      "Gradient Descent(8938/9999): loss=0.043967435957221995\n",
      "Gradient Descent(8939/9999): loss=0.04396743594571952\n",
      "Gradient Descent(8940/9999): loss=0.04396743593423515\n",
      "Gradient Descent(8941/9999): loss=0.04396743592276884\n",
      "Gradient Descent(8942/9999): loss=0.04396743591132056\n",
      "Gradient Descent(8943/9999): loss=0.04396743589989025\n",
      "Gradient Descent(8944/9999): loss=0.04396743588847795\n",
      "Gradient Descent(8945/9999): loss=0.043967435877083565\n",
      "Gradient Descent(8946/9999): loss=0.043967435865707116\n",
      "Gradient Descent(8947/9999): loss=0.04396743585434855\n",
      "Gradient Descent(8948/9999): loss=0.04396743584300784\n",
      "Gradient Descent(8949/9999): loss=0.043967435831684984\n",
      "Gradient Descent(8950/9999): loss=0.043967435820379895\n",
      "Gradient Descent(8951/9999): loss=0.04396743580909259\n",
      "Gradient Descent(8952/9999): loss=0.043967435797823036\n",
      "Gradient Descent(8953/9999): loss=0.04396743578657121\n",
      "Gradient Descent(8954/9999): loss=0.04396743577533707\n",
      "Gradient Descent(8955/9999): loss=0.043967435764120585\n",
      "Gradient Descent(8956/9999): loss=0.043967435752921745\n",
      "Gradient Descent(8957/9999): loss=0.043967435741740495\n",
      "Gradient Descent(8958/9999): loss=0.043967435730576855\n",
      "Gradient Descent(8959/9999): loss=0.04396743571943075\n",
      "Gradient Descent(8960/9999): loss=0.04396743570830219\n",
      "Gradient Descent(8961/9999): loss=0.04396743569719109\n",
      "Gradient Descent(8962/9999): loss=0.043967435686097504\n",
      "Gradient Descent(8963/9999): loss=0.04396743567502133\n",
      "Gradient Descent(8964/9999): loss=0.04396743566396259\n",
      "Gradient Descent(8965/9999): loss=0.04396743565292123\n",
      "Gradient Descent(8966/9999): loss=0.04396743564189723\n",
      "Gradient Descent(8967/9999): loss=0.04396743563089056\n",
      "Gradient Descent(8968/9999): loss=0.04396743561990121\n",
      "Gradient Descent(8969/9999): loss=0.043967435608929135\n",
      "Gradient Descent(8970/9999): loss=0.04396743559797429\n",
      "Gradient Descent(8971/9999): loss=0.043967435587036696\n",
      "Gradient Descent(8972/9999): loss=0.04396743557611628\n",
      "Gradient Descent(8973/9999): loss=0.043967435565213056\n",
      "Gradient Descent(8974/9999): loss=0.043967435554326945\n",
      "Gradient Descent(8975/9999): loss=0.04396743554345799\n",
      "Gradient Descent(8976/9999): loss=0.04396743553260609\n",
      "Gradient Descent(8977/9999): loss=0.043967435521771285\n",
      "Gradient Descent(8978/9999): loss=0.04396743551095349\n",
      "Gradient Descent(8979/9999): loss=0.0439674355001527\n",
      "Gradient Descent(8980/9999): loss=0.04396743548936892\n",
      "Gradient Descent(8981/9999): loss=0.04396743547860207\n",
      "Gradient Descent(8982/9999): loss=0.04396743546785217\n",
      "Gradient Descent(8983/9999): loss=0.04396743545711914\n",
      "Gradient Descent(8984/9999): loss=0.04396743544640302\n",
      "Gradient Descent(8985/9999): loss=0.04396743543570372\n",
      "Gradient Descent(8986/9999): loss=0.04396743542502126\n",
      "Gradient Descent(8987/9999): loss=0.04396743541435561\n",
      "Gradient Descent(8988/9999): loss=0.04396743540370673\n",
      "Gradient Descent(8989/9999): loss=0.04396743539307455\n",
      "Gradient Descent(8990/9999): loss=0.043967435382459154\n",
      "Gradient Descent(8991/9999): loss=0.04396743537186041\n",
      "Gradient Descent(8992/9999): loss=0.04396743536127832\n",
      "Gradient Descent(8993/9999): loss=0.043967435350712875\n",
      "Gradient Descent(8994/9999): loss=0.04396743534016405\n",
      "Gradient Descent(8995/9999): loss=0.04396743532963182\n",
      "Gradient Descent(8996/9999): loss=0.043967435319116115\n",
      "Gradient Descent(8997/9999): loss=0.043967435308616985\n",
      "Gradient Descent(8998/9999): loss=0.04396743529813433\n",
      "Gradient Descent(8999/9999): loss=0.043967435287668194\n",
      "Gradient Descent(9000/9999): loss=0.04396743527721848\n",
      "Gradient Descent(9001/9999): loss=0.04396743526678521\n",
      "Gradient Descent(9002/9999): loss=0.04396743525636835\n",
      "Gradient Descent(9003/9999): loss=0.04396743524596786\n",
      "Gradient Descent(9004/9999): loss=0.043967435235583725\n",
      "Gradient Descent(9005/9999): loss=0.04396743522521593\n",
      "Gradient Descent(9006/9999): loss=0.04396743521486442\n",
      "Gradient Descent(9007/9999): loss=0.0439674352045292\n",
      "Gradient Descent(9008/9999): loss=0.043967435194210217\n",
      "Gradient Descent(9009/9999): loss=0.043967435183907465\n",
      "Gradient Descent(9010/9999): loss=0.04396743517362091\n",
      "Gradient Descent(9011/9999): loss=0.043967435163350534\n",
      "Gradient Descent(9012/9999): loss=0.04396743515309629\n",
      "Gradient Descent(9013/9999): loss=0.0439674351428582\n",
      "Gradient Descent(9014/9999): loss=0.04396743513263619\n",
      "Gradient Descent(9015/9999): loss=0.043967435122430266\n",
      "Gradient Descent(9016/9999): loss=0.04396743511224035\n",
      "Gradient Descent(9017/9999): loss=0.04396743510206647\n",
      "Gradient Descent(9018/9999): loss=0.04396743509190861\n",
      "Gradient Descent(9019/9999): loss=0.043967435081766724\n",
      "Gradient Descent(9020/9999): loss=0.04396743507164076\n",
      "Gradient Descent(9021/9999): loss=0.043967435061530724\n",
      "Gradient Descent(9022/9999): loss=0.04396743505143659\n",
      "Gradient Descent(9023/9999): loss=0.04396743504135832\n",
      "Gradient Descent(9024/9999): loss=0.04396743503129591\n",
      "Gradient Descent(9025/9999): loss=0.043967435021249314\n",
      "Gradient Descent(9026/9999): loss=0.043967435011218525\n",
      "Gradient Descent(9027/9999): loss=0.043967435001203495\n",
      "Gradient Descent(9028/9999): loss=0.043967434991204195\n",
      "Gradient Descent(9029/9999): loss=0.04396743498122066\n",
      "Gradient Descent(9030/9999): loss=0.04396743497125279\n",
      "Gradient Descent(9031/9999): loss=0.043967434961300594\n",
      "Gradient Descent(9032/9999): loss=0.04396743495136406\n",
      "Gradient Descent(9033/9999): loss=0.04396743494144313\n",
      "Gradient Descent(9034/9999): loss=0.04396743493153782\n",
      "Gradient Descent(9035/9999): loss=0.043967434921648084\n",
      "Gradient Descent(9036/9999): loss=0.04396743491177389\n",
      "Gradient Descent(9037/9999): loss=0.0439674349019152\n",
      "Gradient Descent(9038/9999): loss=0.043967434892072055\n",
      "Gradient Descent(9039/9999): loss=0.04396743488224436\n",
      "Gradient Descent(9040/9999): loss=0.04396743487243211\n",
      "Gradient Descent(9041/9999): loss=0.04396743486263532\n",
      "Gradient Descent(9042/9999): loss=0.04396743485285391\n",
      "Gradient Descent(9043/9999): loss=0.04396743484308788\n",
      "Gradient Descent(9044/9999): loss=0.04396743483333721\n",
      "Gradient Descent(9045/9999): loss=0.04396743482360185\n",
      "Gradient Descent(9046/9999): loss=0.043967434813881816\n",
      "Gradient Descent(9047/9999): loss=0.04396743480417709\n",
      "Gradient Descent(9048/9999): loss=0.04396743479448757\n",
      "Gradient Descent(9049/9999): loss=0.043967434784813346\n",
      "Gradient Descent(9050/9999): loss=0.043967434775154295\n",
      "Gradient Descent(9051/9999): loss=0.04396743476551044\n",
      "Gradient Descent(9052/9999): loss=0.043967434755881746\n",
      "Gradient Descent(9053/9999): loss=0.043967434746268186\n",
      "Gradient Descent(9054/9999): loss=0.04396743473666974\n",
      "Gradient Descent(9055/9999): loss=0.0439674347270864\n",
      "Gradient Descent(9056/9999): loss=0.04396743471751813\n",
      "Gradient Descent(9057/9999): loss=0.04396743470796488\n",
      "Gradient Descent(9058/9999): loss=0.043967434698426684\n",
      "Gradient Descent(9059/9999): loss=0.04396743468890346\n",
      "Gradient Descent(9060/9999): loss=0.04396743467939521\n",
      "Gradient Descent(9061/9999): loss=0.04396743466990192\n",
      "Gradient Descent(9062/9999): loss=0.04396743466042356\n",
      "Gradient Descent(9063/9999): loss=0.043967434650960105\n",
      "Gradient Descent(9064/9999): loss=0.04396743464151153\n",
      "Gradient Descent(9065/9999): loss=0.04396743463207779\n",
      "Gradient Descent(9066/9999): loss=0.0439674346226589\n",
      "Gradient Descent(9067/9999): loss=0.04396743461325483\n",
      "Gradient Descent(9068/9999): loss=0.04396743460386553\n",
      "Gradient Descent(9069/9999): loss=0.043967434594490984\n",
      "Gradient Descent(9070/9999): loss=0.0439674345851312\n",
      "Gradient Descent(9071/9999): loss=0.04396743457578611\n",
      "Gradient Descent(9072/9999): loss=0.043967434566455736\n",
      "Gradient Descent(9073/9999): loss=0.043967434557140014\n",
      "Gradient Descent(9074/9999): loss=0.04396743454783895\n",
      "Gradient Descent(9075/9999): loss=0.04396743453855251\n",
      "Gradient Descent(9076/9999): loss=0.04396743452928067\n",
      "Gradient Descent(9077/9999): loss=0.04396743452002341\n",
      "Gradient Descent(9078/9999): loss=0.04396743451078071\n",
      "Gradient Descent(9079/9999): loss=0.043967434501552535\n",
      "Gradient Descent(9080/9999): loss=0.04396743449233887\n",
      "Gradient Descent(9081/9999): loss=0.043967434483139695\n",
      "Gradient Descent(9082/9999): loss=0.043967434473954965\n",
      "Gradient Descent(9083/9999): loss=0.04396743446478471\n",
      "Gradient Descent(9084/9999): loss=0.04396743445562887\n",
      "Gradient Descent(9085/9999): loss=0.0439674344464874\n",
      "Gradient Descent(9086/9999): loss=0.043967434437360314\n",
      "Gradient Descent(9087/9999): loss=0.04396743442824757\n",
      "Gradient Descent(9088/9999): loss=0.043967434419149166\n",
      "Gradient Descent(9089/9999): loss=0.043967434410065065\n",
      "Gradient Descent(9090/9999): loss=0.043967434400995264\n",
      "Gradient Descent(9091/9999): loss=0.04396743439193971\n",
      "Gradient Descent(9092/9999): loss=0.0439674343828984\n",
      "Gradient Descent(9093/9999): loss=0.0439674343738713\n",
      "Gradient Descent(9094/9999): loss=0.04396743436485837\n",
      "Gradient Descent(9095/9999): loss=0.04396743435585965\n",
      "Gradient Descent(9096/9999): loss=0.04396743434687507\n",
      "Gradient Descent(9097/9999): loss=0.0439674343379046\n",
      "Gradient Descent(9098/9999): loss=0.04396743432894826\n",
      "Gradient Descent(9099/9999): loss=0.04396743432000598\n",
      "Gradient Descent(9100/9999): loss=0.04396743431107777\n",
      "Gradient Descent(9101/9999): loss=0.0439674343021636\n",
      "Gradient Descent(9102/9999): loss=0.043967434293263434\n",
      "Gradient Descent(9103/9999): loss=0.04396743428437727\n",
      "Gradient Descent(9104/9999): loss=0.04396743427550507\n",
      "Gradient Descent(9105/9999): loss=0.04396743426664683\n",
      "Gradient Descent(9106/9999): loss=0.04396743425780251\n",
      "Gradient Descent(9107/9999): loss=0.04396743424897212\n",
      "Gradient Descent(9108/9999): loss=0.043967434240155596\n",
      "Gradient Descent(9109/9999): loss=0.04396743423135294\n",
      "Gradient Descent(9110/9999): loss=0.04396743422256412\n",
      "Gradient Descent(9111/9999): loss=0.04396743421378911\n",
      "Gradient Descent(9112/9999): loss=0.043967434205027904\n",
      "Gradient Descent(9113/9999): loss=0.04396743419628048\n",
      "Gradient Descent(9114/9999): loss=0.043967434187546804\n",
      "Gradient Descent(9115/9999): loss=0.04396743417882684\n",
      "Gradient Descent(9116/9999): loss=0.04396743417012062\n",
      "Gradient Descent(9117/9999): loss=0.04396743416142809\n",
      "Gradient Descent(9118/9999): loss=0.043967434152749216\n",
      "Gradient Descent(9119/9999): loss=0.04396743414408399\n",
      "Gradient Descent(9120/9999): loss=0.04396743413543238\n",
      "Gradient Descent(9121/9999): loss=0.04396743412679439\n",
      "Gradient Descent(9122/9999): loss=0.04396743411816995\n",
      "Gradient Descent(9123/9999): loss=0.04396743410955909\n",
      "Gradient Descent(9124/9999): loss=0.04396743410096178\n",
      "Gradient Descent(9125/9999): loss=0.043967434092377966\n",
      "Gradient Descent(9126/9999): loss=0.04396743408380766\n",
      "Gradient Descent(9127/9999): loss=0.04396743407525085\n",
      "Gradient Descent(9128/9999): loss=0.043967434066707466\n",
      "Gradient Descent(9129/9999): loss=0.04396743405817753\n",
      "Gradient Descent(9130/9999): loss=0.043967434049661\n",
      "Gradient Descent(9131/9999): loss=0.043967434041157855\n",
      "Gradient Descent(9132/9999): loss=0.04396743403266809\n",
      "Gradient Descent(9133/9999): loss=0.04396743402419166\n",
      "Gradient Descent(9134/9999): loss=0.04396743401572856\n",
      "Gradient Descent(9135/9999): loss=0.0439674340072788\n",
      "Gradient Descent(9136/9999): loss=0.04396743399884229\n",
      "Gradient Descent(9137/9999): loss=0.04396743399041903\n",
      "Gradient Descent(9138/9999): loss=0.04396743398200906\n",
      "Gradient Descent(9139/9999): loss=0.04396743397361229\n",
      "Gradient Descent(9140/9999): loss=0.04396743396522871\n",
      "Gradient Descent(9141/9999): loss=0.04396743395685834\n",
      "Gradient Descent(9142/9999): loss=0.043967433948501104\n",
      "Gradient Descent(9143/9999): loss=0.04396743394015703\n",
      "Gradient Descent(9144/9999): loss=0.04396743393182606\n",
      "Gradient Descent(9145/9999): loss=0.04396743392350822\n",
      "Gradient Descent(9146/9999): loss=0.04396743391520342\n",
      "Gradient Descent(9147/9999): loss=0.0439674339069117\n",
      "Gradient Descent(9148/9999): loss=0.04396743389863301\n",
      "Gradient Descent(9149/9999): loss=0.043967433890367336\n",
      "Gradient Descent(9150/9999): loss=0.04396743388211465\n",
      "Gradient Descent(9151/9999): loss=0.04396743387387494\n",
      "Gradient Descent(9152/9999): loss=0.04396743386564819\n",
      "Gradient Descent(9153/9999): loss=0.043967433857434395\n",
      "Gradient Descent(9154/9999): loss=0.04396743384923351\n",
      "Gradient Descent(9155/9999): loss=0.0439674338410455\n",
      "Gradient Descent(9156/9999): loss=0.04396743383287038\n",
      "Gradient Descent(9157/9999): loss=0.043967433824708094\n",
      "Gradient Descent(9158/9999): loss=0.04396743381655867\n",
      "Gradient Descent(9159/9999): loss=0.04396743380842204\n",
      "Gradient Descent(9160/9999): loss=0.043967433800298196\n",
      "Gradient Descent(9161/9999): loss=0.04396743379218714\n",
      "Gradient Descent(9162/9999): loss=0.04396743378408882\n",
      "Gradient Descent(9163/9999): loss=0.04396743377600327\n",
      "Gradient Descent(9164/9999): loss=0.0439674337679304\n",
      "Gradient Descent(9165/9999): loss=0.04396743375987025\n",
      "Gradient Descent(9166/9999): loss=0.04396743375182277\n",
      "Gradient Descent(9167/9999): loss=0.043967433743787906\n",
      "Gradient Descent(9168/9999): loss=0.043967433735765706\n",
      "Gradient Descent(9169/9999): loss=0.04396743372775611\n",
      "Gradient Descent(9170/9999): loss=0.04396743371975912\n",
      "Gradient Descent(9171/9999): loss=0.04396743371177469\n",
      "Gradient Descent(9172/9999): loss=0.04396743370380283\n",
      "Gradient Descent(9173/9999): loss=0.04396743369584348\n",
      "Gradient Descent(9174/9999): loss=0.043967433687896657\n",
      "Gradient Descent(9175/9999): loss=0.04396743367996234\n",
      "Gradient Descent(9176/9999): loss=0.04396743367204047\n",
      "Gradient Descent(9177/9999): loss=0.043967433664131084\n",
      "Gradient Descent(9178/9999): loss=0.04396743365623415\n",
      "Gradient Descent(9179/9999): loss=0.0439674336483496\n",
      "Gradient Descent(9180/9999): loss=0.04396743364047745\n",
      "Gradient Descent(9181/9999): loss=0.04396743363261768\n",
      "Gradient Descent(9182/9999): loss=0.04396743362477028\n",
      "Gradient Descent(9183/9999): loss=0.0439674336169352\n",
      "Gradient Descent(9184/9999): loss=0.04396743360911245\n",
      "Gradient Descent(9185/9999): loss=0.043967433601302\n",
      "Gradient Descent(9186/9999): loss=0.04396743359350384\n",
      "Gradient Descent(9187/9999): loss=0.043967433585717926\n",
      "Gradient Descent(9188/9999): loss=0.04396743357794426\n",
      "Gradient Descent(9189/9999): loss=0.04396743357018282\n",
      "Gradient Descent(9190/9999): loss=0.04396743356243358\n",
      "Gradient Descent(9191/9999): loss=0.04396743355469652\n",
      "Gradient Descent(9192/9999): loss=0.04396743354697163\n",
      "Gradient Descent(9193/9999): loss=0.043967433539258895\n",
      "Gradient Descent(9194/9999): loss=0.043967433531558256\n",
      "Gradient Descent(9195/9999): loss=0.043967433523869746\n",
      "Gradient Descent(9196/9999): loss=0.04396743351619334\n",
      "Gradient Descent(9197/9999): loss=0.04396743350852898\n",
      "Gradient Descent(9198/9999): loss=0.043967433500876674\n",
      "Gradient Descent(9199/9999): loss=0.04396743349323641\n",
      "Gradient Descent(9200/9999): loss=0.04396743348560816\n",
      "Gradient Descent(9201/9999): loss=0.04396743347799191\n",
      "Gradient Descent(9202/9999): loss=0.04396743347038761\n",
      "Gradient Descent(9203/9999): loss=0.04396743346279527\n",
      "Gradient Descent(9204/9999): loss=0.04396743345521488\n",
      "Gradient Descent(9205/9999): loss=0.04396743344764643\n",
      "Gradient Descent(9206/9999): loss=0.04396743344008985\n",
      "Gradient Descent(9207/9999): loss=0.04396743343254516\n",
      "Gradient Descent(9208/9999): loss=0.043967433425012324\n",
      "Gradient Descent(9209/9999): loss=0.043967433417491354\n",
      "Gradient Descent(9210/9999): loss=0.04396743340998219\n",
      "Gradient Descent(9211/9999): loss=0.04396743340248482\n",
      "Gradient Descent(9212/9999): loss=0.04396743339499928\n",
      "Gradient Descent(9213/9999): loss=0.04396743338752548\n",
      "Gradient Descent(9214/9999): loss=0.043967433380063425\n",
      "Gradient Descent(9215/9999): loss=0.04396743337261312\n",
      "Gradient Descent(9216/9999): loss=0.04396743336517451\n",
      "Gradient Descent(9217/9999): loss=0.04396743335774762\n",
      "Gradient Descent(9218/9999): loss=0.04396743335033241\n",
      "Gradient Descent(9219/9999): loss=0.043967433342928845\n",
      "Gradient Descent(9220/9999): loss=0.04396743333553693\n",
      "Gradient Descent(9221/9999): loss=0.04396743332815662\n",
      "Gradient Descent(9222/9999): loss=0.04396743332078795\n",
      "Gradient Descent(9223/9999): loss=0.043967433313430816\n",
      "Gradient Descent(9224/9999): loss=0.04396743330608529\n",
      "Gradient Descent(9225/9999): loss=0.04396743329875131\n",
      "Gradient Descent(9226/9999): loss=0.04396743329142883\n",
      "Gradient Descent(9227/9999): loss=0.04396743328411789\n",
      "Gradient Descent(9228/9999): loss=0.04396743327681842\n",
      "Gradient Descent(9229/9999): loss=0.04396743326953045\n",
      "Gradient Descent(9230/9999): loss=0.04396743326225395\n",
      "Gradient Descent(9231/9999): loss=0.04396743325498887\n",
      "Gradient Descent(9232/9999): loss=0.04396743324773523\n",
      "Gradient Descent(9233/9999): loss=0.04396743324049299\n",
      "Gradient Descent(9234/9999): loss=0.04396743323326211\n",
      "Gradient Descent(9235/9999): loss=0.04396743322604263\n",
      "Gradient Descent(9236/9999): loss=0.04396743321883449\n",
      "Gradient Descent(9237/9999): loss=0.043967433211637694\n",
      "Gradient Descent(9238/9999): loss=0.0439674332044522\n",
      "Gradient Descent(9239/9999): loss=0.04396743319727802\n",
      "Gradient Descent(9240/9999): loss=0.0439674331901151\n",
      "Gradient Descent(9241/9999): loss=0.043967433182963436\n",
      "Gradient Descent(9242/9999): loss=0.043967433175823044\n",
      "Gradient Descent(9243/9999): loss=0.043967433168693865\n",
      "Gradient Descent(9244/9999): loss=0.04396743316157591\n",
      "Gradient Descent(9245/9999): loss=0.04396743315446914\n",
      "Gradient Descent(9246/9999): loss=0.043967433147373544\n",
      "Gradient Descent(9247/9999): loss=0.043967433140289086\n",
      "Gradient Descent(9248/9999): loss=0.043967433133215786\n",
      "Gradient Descent(9249/9999): loss=0.043967433126153595\n",
      "Gradient Descent(9250/9999): loss=0.043967433119102534\n",
      "Gradient Descent(9251/9999): loss=0.04396743311206255\n",
      "Gradient Descent(9252/9999): loss=0.04396743310503361\n",
      "Gradient Descent(9253/9999): loss=0.04396743309801574\n",
      "Gradient Descent(9254/9999): loss=0.0439674330910089\n",
      "Gradient Descent(9255/9999): loss=0.04396743308401309\n",
      "Gradient Descent(9256/9999): loss=0.043967433077028266\n",
      "Gradient Descent(9257/9999): loss=0.04396743307005442\n",
      "Gradient Descent(9258/9999): loss=0.04396743306309156\n",
      "Gradient Descent(9259/9999): loss=0.043967433056139635\n",
      "Gradient Descent(9260/9999): loss=0.043967433049198645\n",
      "Gradient Descent(9261/9999): loss=0.04396743304226856\n",
      "Gradient Descent(9262/9999): loss=0.043967433035349376\n",
      "Gradient Descent(9263/9999): loss=0.04396743302844107\n",
      "Gradient Descent(9264/9999): loss=0.04396743302154363\n",
      "Gradient Descent(9265/9999): loss=0.043967433014657026\n",
      "Gradient Descent(9266/9999): loss=0.04396743300778128\n",
      "Gradient Descent(9267/9999): loss=0.0439674330009163\n",
      "Gradient Descent(9268/9999): loss=0.04396743299406215\n",
      "Gradient Descent(9269/9999): loss=0.043967432987218745\n",
      "Gradient Descent(9270/9999): loss=0.04396743298038614\n",
      "Gradient Descent(9271/9999): loss=0.043967432973564265\n",
      "Gradient Descent(9272/9999): loss=0.04396743296675309\n",
      "Gradient Descent(9273/9999): loss=0.04396743295995265\n",
      "Gradient Descent(9274/9999): loss=0.0439674329531629\n",
      "Gradient Descent(9275/9999): loss=0.043967432946383805\n",
      "Gradient Descent(9276/9999): loss=0.043967432939615386\n",
      "Gradient Descent(9277/9999): loss=0.04396743293285761\n",
      "Gradient Descent(9278/9999): loss=0.04396743292611045\n",
      "Gradient Descent(9279/9999): loss=0.043967432919373904\n",
      "Gradient Descent(9280/9999): loss=0.04396743291264795\n",
      "Gradient Descent(9281/9999): loss=0.043967432905932594\n",
      "Gradient Descent(9282/9999): loss=0.043967432899227755\n",
      "Gradient Descent(9283/9999): loss=0.04396743289253349\n",
      "Gradient Descent(9284/9999): loss=0.043967432885849735\n",
      "Gradient Descent(9285/9999): loss=0.0439674328791765\n",
      "Gradient Descent(9286/9999): loss=0.043967432872513736\n",
      "Gradient Descent(9287/9999): loss=0.04396743286586147\n",
      "Gradient Descent(9288/9999): loss=0.04396743285921965\n",
      "Gradient Descent(9289/9999): loss=0.0439674328525883\n",
      "Gradient Descent(9290/9999): loss=0.04396743284596735\n",
      "Gradient Descent(9291/9999): loss=0.04396743283935682\n",
      "Gradient Descent(9292/9999): loss=0.04396743283275668\n",
      "Gradient Descent(9293/9999): loss=0.04396743282616693\n",
      "Gradient Descent(9294/9999): loss=0.04396743281958751\n",
      "Gradient Descent(9295/9999): loss=0.043967432813018466\n",
      "Gradient Descent(9296/9999): loss=0.04396743280645973\n",
      "Gradient Descent(9297/9999): loss=0.04396743279991133\n",
      "Gradient Descent(9298/9999): loss=0.04396743279337321\n",
      "Gradient Descent(9299/9999): loss=0.04396743278684536\n",
      "Gradient Descent(9300/9999): loss=0.0439674327803278\n",
      "Gradient Descent(9301/9999): loss=0.04396743277382048\n",
      "Gradient Descent(9302/9999): loss=0.04396743276732338\n",
      "Gradient Descent(9303/9999): loss=0.043967432760836506\n",
      "Gradient Descent(9304/9999): loss=0.043967432754359825\n",
      "Gradient Descent(9305/9999): loss=0.043967432747893324\n",
      "Gradient Descent(9306/9999): loss=0.043967432741436975\n",
      "Gradient Descent(9307/9999): loss=0.043967432734990826\n",
      "Gradient Descent(9308/9999): loss=0.04396743272855479\n",
      "Gradient Descent(9309/9999): loss=0.04396743272212886\n",
      "Gradient Descent(9310/9999): loss=0.04396743271571304\n",
      "Gradient Descent(9311/9999): loss=0.04396743270930731\n",
      "Gradient Descent(9312/9999): loss=0.04396743270291166\n",
      "Gradient Descent(9313/9999): loss=0.04396743269652606\n",
      "Gradient Descent(9314/9999): loss=0.04396743269015049\n",
      "Gradient Descent(9315/9999): loss=0.04396743268378498\n",
      "Gradient Descent(9316/9999): loss=0.043967432677429426\n",
      "Gradient Descent(9317/9999): loss=0.0439674326710839\n",
      "Gradient Descent(9318/9999): loss=0.04396743266474834\n",
      "Gradient Descent(9319/9999): loss=0.043967432658422734\n",
      "Gradient Descent(9320/9999): loss=0.04396743265210709\n",
      "Gradient Descent(9321/9999): loss=0.043967432645801365\n",
      "Gradient Descent(9322/9999): loss=0.043967432639505574\n",
      "Gradient Descent(9323/9999): loss=0.043967432633219665\n",
      "Gradient Descent(9324/9999): loss=0.04396743262694363\n",
      "Gradient Descent(9325/9999): loss=0.04396743262067749\n",
      "Gradient Descent(9326/9999): loss=0.043967432614421174\n",
      "Gradient Descent(9327/9999): loss=0.04396743260817473\n",
      "Gradient Descent(9328/9999): loss=0.04396743260193808\n",
      "Gradient Descent(9329/9999): loss=0.043967432595711224\n",
      "Gradient Descent(9330/9999): loss=0.04396743258949419\n",
      "Gradient Descent(9331/9999): loss=0.043967432583286926\n",
      "Gradient Descent(9332/9999): loss=0.04396743257708941\n",
      "Gradient Descent(9333/9999): loss=0.04396743257090165\n",
      "Gradient Descent(9334/9999): loss=0.043967432564723616\n",
      "Gradient Descent(9335/9999): loss=0.043967432558555286\n",
      "Gradient Descent(9336/9999): loss=0.04396743255239666\n",
      "Gradient Descent(9337/9999): loss=0.043967432546247714\n",
      "Gradient Descent(9338/9999): loss=0.04396743254010845\n",
      "Gradient Descent(9339/9999): loss=0.04396743253397882\n",
      "Gradient Descent(9340/9999): loss=0.043967432527858834\n",
      "Gradient Descent(9341/9999): loss=0.04396743252174847\n",
      "Gradient Descent(9342/9999): loss=0.04396743251564771\n",
      "Gradient Descent(9343/9999): loss=0.04396743250955657\n",
      "Gradient Descent(9344/9999): loss=0.043967432503474985\n",
      "Gradient Descent(9345/9999): loss=0.04396743249740297\n",
      "Gradient Descent(9346/9999): loss=0.043967432491340504\n",
      "Gradient Descent(9347/9999): loss=0.04396743248528755\n",
      "Gradient Descent(9348/9999): loss=0.04396743247924413\n",
      "Gradient Descent(9349/9999): loss=0.043967432473210215\n",
      "Gradient Descent(9350/9999): loss=0.04396743246718578\n",
      "Gradient Descent(9351/9999): loss=0.04396743246117081\n",
      "Gradient Descent(9352/9999): loss=0.043967432455165295\n",
      "Gradient Descent(9353/9999): loss=0.04396743244916925\n",
      "Gradient Descent(9354/9999): loss=0.043967432443182596\n",
      "Gradient Descent(9355/9999): loss=0.0439674324372054\n",
      "Gradient Descent(9356/9999): loss=0.04396743243123755\n",
      "Gradient Descent(9357/9999): loss=0.043967432425279146\n",
      "Gradient Descent(9358/9999): loss=0.04396743241933008\n",
      "Gradient Descent(9359/9999): loss=0.04396743241339036\n",
      "Gradient Descent(9360/9999): loss=0.04396743240745996\n",
      "Gradient Descent(9361/9999): loss=0.04396743240153892\n",
      "Gradient Descent(9362/9999): loss=0.04396743239562717\n",
      "Gradient Descent(9363/9999): loss=0.04396743238972472\n",
      "Gradient Descent(9364/9999): loss=0.04396743238383155\n",
      "Gradient Descent(9365/9999): loss=0.04396743237794766\n",
      "Gradient Descent(9366/9999): loss=0.043967432372073\n",
      "Gradient Descent(9367/9999): loss=0.043967432366207586\n",
      "Gradient Descent(9368/9999): loss=0.043967432360351416\n",
      "Gradient Descent(9369/9999): loss=0.04396743235450443\n",
      "Gradient Descent(9370/9999): loss=0.04396743234866663\n",
      "Gradient Descent(9371/9999): loss=0.04396743234283802\n",
      "Gradient Descent(9372/9999): loss=0.04396743233701857\n",
      "Gradient Descent(9373/9999): loss=0.043967432331208284\n",
      "Gradient Descent(9374/9999): loss=0.04396743232540712\n",
      "Gradient Descent(9375/9999): loss=0.04396743231961508\n",
      "Gradient Descent(9376/9999): loss=0.04396743231383215\n",
      "Gradient Descent(9377/9999): loss=0.0439674323080583\n",
      "Gradient Descent(9378/9999): loss=0.04396743230229355\n",
      "Gradient Descent(9379/9999): loss=0.04396743229653786\n",
      "Gradient Descent(9380/9999): loss=0.04396743229079121\n",
      "Gradient Descent(9381/9999): loss=0.04396743228505361\n",
      "Gradient Descent(9382/9999): loss=0.043967432279325\n",
      "Gradient Descent(9383/9999): loss=0.04396743227360544\n",
      "Gradient Descent(9384/9999): loss=0.04396743226789484\n",
      "Gradient Descent(9385/9999): loss=0.04396743226219322\n",
      "Gradient Descent(9386/9999): loss=0.04396743225650058\n",
      "Gradient Descent(9387/9999): loss=0.0439674322508169\n",
      "Gradient Descent(9388/9999): loss=0.04396743224514213\n",
      "Gradient Descent(9389/9999): loss=0.043967432239476294\n",
      "Gradient Descent(9390/9999): loss=0.043967432233819354\n",
      "Gradient Descent(9391/9999): loss=0.04396743222817132\n",
      "Gradient Descent(9392/9999): loss=0.04396743222253218\n",
      "Gradient Descent(9393/9999): loss=0.04396743221690189\n",
      "Gradient Descent(9394/9999): loss=0.04396743221128047\n",
      "Gradient Descent(9395/9999): loss=0.04396743220566788\n",
      "Gradient Descent(9396/9999): loss=0.04396743220006412\n",
      "Gradient Descent(9397/9999): loss=0.04396743219446916\n",
      "Gradient Descent(9398/9999): loss=0.043967432188883014\n",
      "Gradient Descent(9399/9999): loss=0.043967432183305614\n",
      "Gradient Descent(9400/9999): loss=0.043967432177737\n",
      "Gradient Descent(9401/9999): loss=0.04396743217217715\n",
      "Gradient Descent(9402/9999): loss=0.04396743216662605\n",
      "Gradient Descent(9403/9999): loss=0.04396743216108367\n",
      "Gradient Descent(9404/9999): loss=0.04396743215555\n",
      "Gradient Descent(9405/9999): loss=0.04396743215002503\n",
      "Gradient Descent(9406/9999): loss=0.04396743214450874\n",
      "Gradient Descent(9407/9999): loss=0.043967432139001145\n",
      "Gradient Descent(9408/9999): loss=0.04396743213350221\n",
      "Gradient Descent(9409/9999): loss=0.0439674321280119\n",
      "Gradient Descent(9410/9999): loss=0.04396743212253023\n",
      "Gradient Descent(9411/9999): loss=0.04396743211705718\n",
      "Gradient Descent(9412/9999): loss=0.043967432111592736\n",
      "Gradient Descent(9413/9999): loss=0.04396743210613688\n",
      "Gradient Descent(9414/9999): loss=0.043967432100689624\n",
      "Gradient Descent(9415/9999): loss=0.04396743209525091\n",
      "Gradient Descent(9416/9999): loss=0.04396743208982074\n",
      "Gradient Descent(9417/9999): loss=0.043967432084399134\n",
      "Gradient Descent(9418/9999): loss=0.04396743207898604\n",
      "Gradient Descent(9419/9999): loss=0.043967432073581454\n",
      "Gradient Descent(9420/9999): loss=0.043967432068185375\n",
      "Gradient Descent(9421/9999): loss=0.04396743206279778\n",
      "Gradient Descent(9422/9999): loss=0.04396743205741866\n",
      "Gradient Descent(9423/9999): loss=0.04396743205204798\n",
      "Gradient Descent(9424/9999): loss=0.043967432046685746\n",
      "Gradient Descent(9425/9999): loss=0.04396743204133195\n",
      "Gradient Descent(9426/9999): loss=0.043967432035986576\n",
      "Gradient Descent(9427/9999): loss=0.043967432030649595\n",
      "Gradient Descent(9428/9999): loss=0.04396743202532101\n",
      "Gradient Descent(9429/9999): loss=0.043967432020000814\n",
      "Gradient Descent(9430/9999): loss=0.043967432014688966\n",
      "Gradient Descent(9431/9999): loss=0.04396743200938547\n",
      "Gradient Descent(9432/9999): loss=0.04396743200409034\n",
      "Gradient Descent(9433/9999): loss=0.04396743199880353\n",
      "Gradient Descent(9434/9999): loss=0.043967431993525\n",
      "Gradient Descent(9435/9999): loss=0.04396743198825479\n",
      "Gradient Descent(9436/9999): loss=0.04396743198299287\n",
      "Gradient Descent(9437/9999): loss=0.043967431977739224\n",
      "Gradient Descent(9438/9999): loss=0.04396743197249384\n",
      "Gradient Descent(9439/9999): loss=0.043967431967256713\n",
      "Gradient Descent(9440/9999): loss=0.04396743196202779\n",
      "Gradient Descent(9441/9999): loss=0.04396743195680712\n",
      "Gradient Descent(9442/9999): loss=0.043967431951594665\n",
      "Gradient Descent(9443/9999): loss=0.043967431946390356\n",
      "Gradient Descent(9444/9999): loss=0.043967431941194256\n",
      "Gradient Descent(9445/9999): loss=0.04396743193600633\n",
      "Gradient Descent(9446/9999): loss=0.04396743193082656\n",
      "Gradient Descent(9447/9999): loss=0.04396743192565492\n",
      "Gradient Descent(9448/9999): loss=0.043967431920491434\n",
      "Gradient Descent(9449/9999): loss=0.04396743191533606\n",
      "Gradient Descent(9450/9999): loss=0.043967431910188794\n",
      "Gradient Descent(9451/9999): loss=0.043967431905049634\n",
      "Gradient Descent(9452/9999): loss=0.043967431899918516\n",
      "Gradient Descent(9453/9999): loss=0.04396743189479549\n",
      "Gradient Descent(9454/9999): loss=0.04396743188968051\n",
      "Gradient Descent(9455/9999): loss=0.04396743188457359\n",
      "Gradient Descent(9456/9999): loss=0.04396743187947469\n",
      "Gradient Descent(9457/9999): loss=0.0439674318743838\n",
      "Gradient Descent(9458/9999): loss=0.04396743186930092\n",
      "Gradient Descent(9459/9999): loss=0.043967431864226046\n",
      "Gradient Descent(9460/9999): loss=0.04396743185915913\n",
      "Gradient Descent(9461/9999): loss=0.043967431854100195\n",
      "Gradient Descent(9462/9999): loss=0.04396743184904919\n",
      "Gradient Descent(9463/9999): loss=0.04396743184400616\n",
      "Gradient Descent(9464/9999): loss=0.04396743183897105\n",
      "Gradient Descent(9465/9999): loss=0.04396743183394384\n",
      "Gradient Descent(9466/9999): loss=0.04396743182892453\n",
      "Gradient Descent(9467/9999): loss=0.04396743182391313\n",
      "Gradient Descent(9468/9999): loss=0.04396743181890963\n",
      "Gradient Descent(9469/9999): loss=0.043967431813913965\n",
      "Gradient Descent(9470/9999): loss=0.04396743180892616\n",
      "Gradient Descent(9471/9999): loss=0.04396743180394621\n",
      "Gradient Descent(9472/9999): loss=0.04396743179897408\n",
      "Gradient Descent(9473/9999): loss=0.04396743179400978\n",
      "Gradient Descent(9474/9999): loss=0.04396743178905325\n",
      "Gradient Descent(9475/9999): loss=0.04396743178410454\n",
      "Gradient Descent(9476/9999): loss=0.04396743177916362\n",
      "Gradient Descent(9477/9999): loss=0.043967431774230466\n",
      "Gradient Descent(9478/9999): loss=0.04396743176930504\n",
      "Gradient Descent(9479/9999): loss=0.04396743176438738\n",
      "Gradient Descent(9480/9999): loss=0.043967431759477454\n",
      "Gradient Descent(9481/9999): loss=0.04396743175457525\n",
      "Gradient Descent(9482/9999): loss=0.043967431749680756\n",
      "Gradient Descent(9483/9999): loss=0.04396743174479394\n",
      "Gradient Descent(9484/9999): loss=0.04396743173991483\n",
      "Gradient Descent(9485/9999): loss=0.04396743173504339\n",
      "Gradient Descent(9486/9999): loss=0.04396743173017959\n",
      "Gradient Descent(9487/9999): loss=0.04396743172532344\n",
      "Gradient Descent(9488/9999): loss=0.043967431720474945\n",
      "Gradient Descent(9489/9999): loss=0.04396743171563404\n",
      "Gradient Descent(9490/9999): loss=0.04396743171080078\n",
      "Gradient Descent(9491/9999): loss=0.04396743170597509\n",
      "Gradient Descent(9492/9999): loss=0.043967431701157016\n",
      "Gradient Descent(9493/9999): loss=0.0439674316963465\n",
      "Gradient Descent(9494/9999): loss=0.04396743169154356\n",
      "Gradient Descent(9495/9999): loss=0.04396743168674818\n",
      "Gradient Descent(9496/9999): loss=0.04396743168196032\n",
      "Gradient Descent(9497/9999): loss=0.043967431677179994\n",
      "Gradient Descent(9498/9999): loss=0.04396743167240719\n",
      "Gradient Descent(9499/9999): loss=0.04396743166764188\n",
      "Gradient Descent(9500/9999): loss=0.04396743166288407\n",
      "Gradient Descent(9501/9999): loss=0.043967431658133736\n",
      "Gradient Descent(9502/9999): loss=0.043967431653390864\n",
      "Gradient Descent(9503/9999): loss=0.04396743164865548\n",
      "Gradient Descent(9504/9999): loss=0.04396743164392751\n",
      "Gradient Descent(9505/9999): loss=0.04396743163920698\n",
      "Gradient Descent(9506/9999): loss=0.04396743163449389\n",
      "Gradient Descent(9507/9999): loss=0.04396743162978819\n",
      "Gradient Descent(9508/9999): loss=0.04396743162508991\n",
      "Gradient Descent(9509/9999): loss=0.043967431620399\n",
      "Gradient Descent(9510/9999): loss=0.04396743161571546\n",
      "Gradient Descent(9511/9999): loss=0.04396743161103929\n",
      "Gradient Descent(9512/9999): loss=0.043967431606370476\n",
      "Gradient Descent(9513/9999): loss=0.043967431601709\n",
      "Gradient Descent(9514/9999): loss=0.04396743159705487\n",
      "Gradient Descent(9515/9999): loss=0.04396743159240803\n",
      "Gradient Descent(9516/9999): loss=0.04396743158776853\n",
      "Gradient Descent(9517/9999): loss=0.043967431583136304\n",
      "Gradient Descent(9518/9999): loss=0.04396743157851134\n",
      "Gradient Descent(9519/9999): loss=0.04396743157389369\n",
      "Gradient Descent(9520/9999): loss=0.04396743156928327\n",
      "Gradient Descent(9521/9999): loss=0.0439674315646801\n",
      "Gradient Descent(9522/9999): loss=0.043967431560084175\n",
      "Gradient Descent(9523/9999): loss=0.043967431555495484\n",
      "Gradient Descent(9524/9999): loss=0.04396743155091402\n",
      "Gradient Descent(9525/9999): loss=0.04396743154633973\n",
      "Gradient Descent(9526/9999): loss=0.043967431541772656\n",
      "Gradient Descent(9527/9999): loss=0.04396743153721275\n",
      "Gradient Descent(9528/9999): loss=0.04396743153266\n",
      "Gradient Descent(9529/9999): loss=0.043967431528114435\n",
      "Gradient Descent(9530/9999): loss=0.043967431523576\n",
      "Gradient Descent(9531/9999): loss=0.04396743151904471\n",
      "Gradient Descent(9532/9999): loss=0.04396743151452055\n",
      "Gradient Descent(9533/9999): loss=0.0439674315100035\n",
      "Gradient Descent(9534/9999): loss=0.04396743150549352\n",
      "Gradient Descent(9535/9999): loss=0.04396743150099069\n",
      "Gradient Descent(9536/9999): loss=0.04396743149649491\n",
      "Gradient Descent(9537/9999): loss=0.04396743149200617\n",
      "Gradient Descent(9538/9999): loss=0.043967431487524536\n",
      "Gradient Descent(9539/9999): loss=0.04396743148304992\n",
      "Gradient Descent(9540/9999): loss=0.04396743147858237\n",
      "Gradient Descent(9541/9999): loss=0.0439674314741218\n",
      "Gradient Descent(9542/9999): loss=0.043967431469668264\n",
      "Gradient Descent(9543/9999): loss=0.04396743146522173\n",
      "Gradient Descent(9544/9999): loss=0.0439674314607822\n",
      "Gradient Descent(9545/9999): loss=0.04396743145634965\n",
      "Gradient Descent(9546/9999): loss=0.043967431451924034\n",
      "Gradient Descent(9547/9999): loss=0.0439674314475054\n",
      "Gradient Descent(9548/9999): loss=0.04396743144309373\n",
      "Gradient Descent(9549/9999): loss=0.04396743143868898\n",
      "Gradient Descent(9550/9999): loss=0.04396743143429117\n",
      "Gradient Descent(9551/9999): loss=0.04396743142990027\n",
      "Gradient Descent(9552/9999): loss=0.043967431425516235\n",
      "Gradient Descent(9553/9999): loss=0.043967431421139146\n",
      "Gradient Descent(9554/9999): loss=0.04396743141676891\n",
      "Gradient Descent(9555/9999): loss=0.04396743141240555\n",
      "Gradient Descent(9556/9999): loss=0.04396743140804906\n",
      "Gradient Descent(9557/9999): loss=0.0439674314036994\n",
      "Gradient Descent(9558/9999): loss=0.043967431399356605\n",
      "Gradient Descent(9559/9999): loss=0.04396743139502062\n",
      "Gradient Descent(9560/9999): loss=0.04396743139069146\n",
      "Gradient Descent(9561/9999): loss=0.043967431386369125\n",
      "Gradient Descent(9562/9999): loss=0.04396743138205356\n",
      "Gradient Descent(9563/9999): loss=0.04396743137774479\n",
      "Gradient Descent(9564/9999): loss=0.04396743137344279\n",
      "Gradient Descent(9565/9999): loss=0.04396743136914754\n",
      "Gradient Descent(9566/9999): loss=0.043967431364859054\n",
      "Gradient Descent(9567/9999): loss=0.043967431360577326\n",
      "Gradient Descent(9568/9999): loss=0.04396743135630234\n",
      "Gradient Descent(9569/9999): loss=0.04396743135203404\n",
      "Gradient Descent(9570/9999): loss=0.04396743134777247\n",
      "Gradient Descent(9571/9999): loss=0.0439674313435176\n",
      "Gradient Descent(9572/9999): loss=0.04396743133926942\n",
      "Gradient Descent(9573/9999): loss=0.043967431335027916\n",
      "Gradient Descent(9574/9999): loss=0.04396743133079308\n",
      "Gradient Descent(9575/9999): loss=0.043967431326564936\n",
      "Gradient Descent(9576/9999): loss=0.043967431322343396\n",
      "Gradient Descent(9577/9999): loss=0.0439674313181285\n",
      "Gradient Descent(9578/9999): loss=0.043967431313920245\n",
      "Gradient Descent(9579/9999): loss=0.043967431309718606\n",
      "Gradient Descent(9580/9999): loss=0.04396743130552355\n",
      "Gradient Descent(9581/9999): loss=0.043967431301335125\n",
      "Gradient Descent(9582/9999): loss=0.043967431297153255\n",
      "Gradient Descent(9583/9999): loss=0.04396743129297798\n",
      "Gradient Descent(9584/9999): loss=0.04396743128880926\n",
      "Gradient Descent(9585/9999): loss=0.0439674312846471\n",
      "Gradient Descent(9586/9999): loss=0.04396743128049148\n",
      "Gradient Descent(9587/9999): loss=0.04396743127634241\n",
      "Gradient Descent(9588/9999): loss=0.04396743127219984\n",
      "Gradient Descent(9589/9999): loss=0.043967431268063795\n",
      "Gradient Descent(9590/9999): loss=0.04396743126393423\n",
      "Gradient Descent(9591/9999): loss=0.0439674312598112\n",
      "Gradient Descent(9592/9999): loss=0.04396743125569464\n",
      "Gradient Descent(9593/9999): loss=0.04396743125158454\n",
      "Gradient Descent(9594/9999): loss=0.043967431247480905\n",
      "Gradient Descent(9595/9999): loss=0.04396743124338373\n",
      "Gradient Descent(9596/9999): loss=0.04396743123929299\n",
      "Gradient Descent(9597/9999): loss=0.04396743123520868\n",
      "Gradient Descent(9598/9999): loss=0.0439674312311308\n",
      "Gradient Descent(9599/9999): loss=0.04396743122705933\n",
      "Gradient Descent(9600/9999): loss=0.043967431222994256\n",
      "Gradient Descent(9601/9999): loss=0.04396743121893559\n",
      "Gradient Descent(9602/9999): loss=0.043967431214883286\n",
      "Gradient Descent(9603/9999): loss=0.04396743121083735\n",
      "Gradient Descent(9604/9999): loss=0.043967431206797795\n",
      "Gradient Descent(9605/9999): loss=0.04396743120276459\n",
      "Gradient Descent(9606/9999): loss=0.04396743119873772\n",
      "Gradient Descent(9607/9999): loss=0.04396743119471719\n",
      "Gradient Descent(9608/9999): loss=0.043967431190702955\n",
      "Gradient Descent(9609/9999): loss=0.04396743118669505\n",
      "Gradient Descent(9610/9999): loss=0.04396743118269345\n",
      "Gradient Descent(9611/9999): loss=0.04396743117869815\n",
      "Gradient Descent(9612/9999): loss=0.043967431174709116\n",
      "Gradient Descent(9613/9999): loss=0.04396743117072636\n",
      "Gradient Descent(9614/9999): loss=0.04396743116674986\n",
      "Gradient Descent(9615/9999): loss=0.04396743116277961\n",
      "Gradient Descent(9616/9999): loss=0.04396743115881563\n",
      "Gradient Descent(9617/9999): loss=0.04396743115485784\n",
      "Gradient Descent(9618/9999): loss=0.0439674311509063\n",
      "Gradient Descent(9619/9999): loss=0.04396743114696097\n",
      "Gradient Descent(9620/9999): loss=0.04396743114302185\n",
      "Gradient Descent(9621/9999): loss=0.04396743113908891\n",
      "Gradient Descent(9622/9999): loss=0.04396743113516217\n",
      "Gradient Descent(9623/9999): loss=0.04396743113124158\n",
      "Gradient Descent(9624/9999): loss=0.04396743112732717\n",
      "Gradient Descent(9625/9999): loss=0.043967431123418894\n",
      "Gradient Descent(9626/9999): loss=0.04396743111951681\n",
      "Gradient Descent(9627/9999): loss=0.04396743111562082\n",
      "Gradient Descent(9628/9999): loss=0.043967431111730966\n",
      "Gradient Descent(9629/9999): loss=0.04396743110784722\n",
      "Gradient Descent(9630/9999): loss=0.043967431103969605\n",
      "Gradient Descent(9631/9999): loss=0.04396743110009807\n",
      "Gradient Descent(9632/9999): loss=0.04396743109623262\n",
      "Gradient Descent(9633/9999): loss=0.04396743109237326\n",
      "Gradient Descent(9634/9999): loss=0.043967431088519957\n",
      "Gradient Descent(9635/9999): loss=0.04396743108467272\n",
      "Gradient Descent(9636/9999): loss=0.04396743108083153\n",
      "Gradient Descent(9637/9999): loss=0.04396743107699638\n",
      "Gradient Descent(9638/9999): loss=0.04396743107316727\n",
      "Gradient Descent(9639/9999): loss=0.04396743106934417\n",
      "Gradient Descent(9640/9999): loss=0.04396743106552707\n",
      "Gradient Descent(9641/9999): loss=0.04396743106171599\n",
      "Gradient Descent(9642/9999): loss=0.04396743105791089\n",
      "Gradient Descent(9643/9999): loss=0.04396743105411177\n",
      "Gradient Descent(9644/9999): loss=0.043967431050318646\n",
      "Gradient Descent(9645/9999): loss=0.043967431046531474\n",
      "Gradient Descent(9646/9999): loss=0.04396743104275024\n",
      "Gradient Descent(9647/9999): loss=0.04396743103897498\n",
      "Gradient Descent(9648/9999): loss=0.043967431035205645\n",
      "Gradient Descent(9649/9999): loss=0.04396743103144224\n",
      "Gradient Descent(9650/9999): loss=0.043967431027684724\n",
      "Gradient Descent(9651/9999): loss=0.043967431023933135\n",
      "Gradient Descent(9652/9999): loss=0.04396743102018744\n",
      "Gradient Descent(9653/9999): loss=0.04396743101644765\n",
      "Gradient Descent(9654/9999): loss=0.043967431012713734\n",
      "Gradient Descent(9655/9999): loss=0.0439674310089857\n",
      "Gradient Descent(9656/9999): loss=0.0439674310052635\n",
      "Gradient Descent(9657/9999): loss=0.043967431001547166\n",
      "Gradient Descent(9658/9999): loss=0.04396743099783668\n",
      "Gradient Descent(9659/9999): loss=0.04396743099413203\n",
      "Gradient Descent(9660/9999): loss=0.04396743099043318\n",
      "Gradient Descent(9661/9999): loss=0.043967430986740184\n",
      "Gradient Descent(9662/9999): loss=0.043967430983052974\n",
      "Gradient Descent(9663/9999): loss=0.04396743097937156\n",
      "Gradient Descent(9664/9999): loss=0.04396743097569595\n",
      "Gradient Descent(9665/9999): loss=0.04396743097202609\n",
      "Gradient Descent(9666/9999): loss=0.04396743096836202\n",
      "Gradient Descent(9667/9999): loss=0.0439674309647037\n",
      "Gradient Descent(9668/9999): loss=0.04396743096105116\n",
      "Gradient Descent(9669/9999): loss=0.04396743095740434\n",
      "Gradient Descent(9670/9999): loss=0.04396743095376325\n",
      "Gradient Descent(9671/9999): loss=0.043967430950127895\n",
      "Gradient Descent(9672/9999): loss=0.043967430946498236\n",
      "Gradient Descent(9673/9999): loss=0.04396743094287432\n",
      "Gradient Descent(9674/9999): loss=0.043967430939256084\n",
      "Gradient Descent(9675/9999): loss=0.04396743093564354\n",
      "Gradient Descent(9676/9999): loss=0.04396743093203668\n",
      "Gradient Descent(9677/9999): loss=0.04396743092843547\n",
      "Gradient Descent(9678/9999): loss=0.043967430924839936\n",
      "Gradient Descent(9679/9999): loss=0.04396743092125005\n",
      "Gradient Descent(9680/9999): loss=0.04396743091766582\n",
      "Gradient Descent(9681/9999): loss=0.04396743091408723\n",
      "Gradient Descent(9682/9999): loss=0.043967430910514256\n",
      "Gradient Descent(9683/9999): loss=0.0439674309069469\n",
      "Gradient Descent(9684/9999): loss=0.04396743090338516\n",
      "Gradient Descent(9685/9999): loss=0.043967430899829\n",
      "Gradient Descent(9686/9999): loss=0.04396743089627846\n",
      "Gradient Descent(9687/9999): loss=0.043967430892733486\n",
      "Gradient Descent(9688/9999): loss=0.04396743088919409\n",
      "Gradient Descent(9689/9999): loss=0.043967430885660234\n",
      "Gradient Descent(9690/9999): loss=0.04396743088213196\n",
      "Gradient Descent(9691/9999): loss=0.043967430878609236\n",
      "Gradient Descent(9692/9999): loss=0.04396743087509204\n",
      "Gradient Descent(9693/9999): loss=0.043967430871580386\n",
      "Gradient Descent(9694/9999): loss=0.04396743086807424\n",
      "Gradient Descent(9695/9999): loss=0.04396743086457363\n",
      "Gradient Descent(9696/9999): loss=0.043967430861078516\n",
      "Gradient Descent(9697/9999): loss=0.04396743085758888\n",
      "Gradient Descent(9698/9999): loss=0.04396743085410473\n",
      "Gradient Descent(9699/9999): loss=0.043967430850626064\n",
      "Gradient Descent(9700/9999): loss=0.04396743084715288\n",
      "Gradient Descent(9701/9999): loss=0.043967430843685144\n",
      "Gradient Descent(9702/9999): loss=0.04396743084022288\n",
      "Gradient Descent(9703/9999): loss=0.04396743083676603\n",
      "Gradient Descent(9704/9999): loss=0.043967430833314634\n",
      "Gradient Descent(9705/9999): loss=0.04396743082986866\n",
      "Gradient Descent(9706/9999): loss=0.04396743082642811\n",
      "Gradient Descent(9707/9999): loss=0.04396743082299296\n",
      "Gradient Descent(9708/9999): loss=0.043967430819563225\n",
      "Gradient Descent(9709/9999): loss=0.043967430816138874\n",
      "Gradient Descent(9710/9999): loss=0.043967430812719935\n",
      "Gradient Descent(9711/9999): loss=0.043967430809306325\n",
      "Gradient Descent(9712/9999): loss=0.0439674308058981\n",
      "Gradient Descent(9713/9999): loss=0.04396743080249523\n",
      "Gradient Descent(9714/9999): loss=0.04396743079909772\n",
      "Gradient Descent(9715/9999): loss=0.043967430795705545\n",
      "Gradient Descent(9716/9999): loss=0.043967430792318726\n",
      "Gradient Descent(9717/9999): loss=0.043967430788937216\n",
      "Gradient Descent(9718/9999): loss=0.04396743078556101\n",
      "Gradient Descent(9719/9999): loss=0.04396743078219013\n",
      "Gradient Descent(9720/9999): loss=0.043967430778824534\n",
      "Gradient Descent(9721/9999): loss=0.04396743077546422\n",
      "Gradient Descent(9722/9999): loss=0.04396743077210922\n",
      "Gradient Descent(9723/9999): loss=0.0439674307687595\n",
      "Gradient Descent(9724/9999): loss=0.04396743076541501\n",
      "Gradient Descent(9725/9999): loss=0.043967430762075814\n",
      "Gradient Descent(9726/9999): loss=0.04396743075874184\n",
      "Gradient Descent(9727/9999): loss=0.04396743075541312\n",
      "Gradient Descent(9728/9999): loss=0.043967430752089635\n",
      "Gradient Descent(9729/9999): loss=0.04396743074877139\n",
      "Gradient Descent(9730/9999): loss=0.04396743074545834\n",
      "Gradient Descent(9731/9999): loss=0.0439674307421505\n",
      "Gradient Descent(9732/9999): loss=0.04396743073884786\n",
      "Gradient Descent(9733/9999): loss=0.04396743073555043\n",
      "Gradient Descent(9734/9999): loss=0.04396743073225818\n",
      "Gradient Descent(9735/9999): loss=0.04396743072897111\n",
      "Gradient Descent(9736/9999): loss=0.043967430725689205\n",
      "Gradient Descent(9737/9999): loss=0.04396743072241246\n",
      "Gradient Descent(9738/9999): loss=0.043967430719140846\n",
      "Gradient Descent(9739/9999): loss=0.04396743071587439\n",
      "Gradient Descent(9740/9999): loss=0.04396743071261309\n",
      "Gradient Descent(9741/9999): loss=0.04396743070935691\n",
      "Gradient Descent(9742/9999): loss=0.04396743070610583\n",
      "Gradient Descent(9743/9999): loss=0.043967430702859855\n",
      "Gradient Descent(9744/9999): loss=0.04396743069961902\n",
      "Gradient Descent(9745/9999): loss=0.043967430696383245\n",
      "Gradient Descent(9746/9999): loss=0.04396743069315259\n",
      "Gradient Descent(9747/9999): loss=0.043967430689926985\n",
      "Gradient Descent(9748/9999): loss=0.04396743068670647\n",
      "Gradient Descent(9749/9999): loss=0.043967430683491016\n",
      "Gradient Descent(9750/9999): loss=0.04396743068028061\n",
      "Gradient Descent(9751/9999): loss=0.043967430677075266\n",
      "Gradient Descent(9752/9999): loss=0.04396743067387496\n",
      "Gradient Descent(9753/9999): loss=0.04396743067067969\n",
      "Gradient Descent(9754/9999): loss=0.04396743066748942\n",
      "Gradient Descent(9755/9999): loss=0.043967430664304176\n",
      "Gradient Descent(9756/9999): loss=0.04396743066112394\n",
      "Gradient Descent(9757/9999): loss=0.043967430657948725\n",
      "Gradient Descent(9758/9999): loss=0.04396743065477849\n",
      "Gradient Descent(9759/9999): loss=0.04396743065161323\n",
      "Gradient Descent(9760/9999): loss=0.043967430648452946\n",
      "Gradient Descent(9761/9999): loss=0.043967430645297643\n",
      "Gradient Descent(9762/9999): loss=0.0439674306421473\n",
      "Gradient Descent(9763/9999): loss=0.0439674306390019\n",
      "Gradient Descent(9764/9999): loss=0.04396743063586146\n",
      "Gradient Descent(9765/9999): loss=0.04396743063272594\n",
      "Gradient Descent(9766/9999): loss=0.04396743062959537\n",
      "Gradient Descent(9767/9999): loss=0.043967430626469704\n",
      "Gradient Descent(9768/9999): loss=0.04396743062334897\n",
      "Gradient Descent(9769/9999): loss=0.043967430620233124\n",
      "Gradient Descent(9770/9999): loss=0.0439674306171222\n",
      "Gradient Descent(9771/9999): loss=0.043967430614016145\n",
      "Gradient Descent(9772/9999): loss=0.043967430610914994\n",
      "Gradient Descent(9773/9999): loss=0.043967430607818686\n",
      "Gradient Descent(9774/9999): loss=0.04396743060472729\n",
      "Gradient Descent(9775/9999): loss=0.04396743060164074\n",
      "Gradient Descent(9776/9999): loss=0.043967430598559024\n",
      "Gradient Descent(9777/9999): loss=0.043967430595482165\n",
      "Gradient Descent(9778/9999): loss=0.043967430592410144\n",
      "Gradient Descent(9779/9999): loss=0.043967430589342944\n",
      "Gradient Descent(9780/9999): loss=0.043967430586280595\n",
      "Gradient Descent(9781/9999): loss=0.04396743058322304\n",
      "Gradient Descent(9782/9999): loss=0.04396743058017028\n",
      "Gradient Descent(9783/9999): loss=0.04396743057712234\n",
      "Gradient Descent(9784/9999): loss=0.04396743057407918\n",
      "Gradient Descent(9785/9999): loss=0.04396743057104082\n",
      "Gradient Descent(9786/9999): loss=0.04396743056800722\n",
      "Gradient Descent(9787/9999): loss=0.0439674305649784\n",
      "Gradient Descent(9788/9999): loss=0.04396743056195436\n",
      "Gradient Descent(9789/9999): loss=0.043967430558935046\n",
      "Gradient Descent(9790/9999): loss=0.0439674305559205\n",
      "Gradient Descent(9791/9999): loss=0.043967430552910684\n",
      "Gradient Descent(9792/9999): loss=0.043967430549905595\n",
      "Gradient Descent(9793/9999): loss=0.04396743054690523\n",
      "Gradient Descent(9794/9999): loss=0.0439674305439096\n",
      "Gradient Descent(9795/9999): loss=0.043967430540918666\n",
      "Gradient Descent(9796/9999): loss=0.043967430537932436\n",
      "Gradient Descent(9797/9999): loss=0.04396743053495091\n",
      "Gradient Descent(9798/9999): loss=0.04396743053197406\n",
      "Gradient Descent(9799/9999): loss=0.04396743052900188\n",
      "Gradient Descent(9800/9999): loss=0.04396743052603441\n",
      "Gradient Descent(9801/9999): loss=0.04396743052307159\n",
      "Gradient Descent(9802/9999): loss=0.0439674305201134\n",
      "Gradient Descent(9803/9999): loss=0.04396743051715991\n",
      "Gradient Descent(9804/9999): loss=0.04396743051421104\n",
      "Gradient Descent(9805/9999): loss=0.04396743051126679\n",
      "Gradient Descent(9806/9999): loss=0.043967430508327195\n",
      "Gradient Descent(9807/9999): loss=0.043967430505392195\n",
      "Gradient Descent(9808/9999): loss=0.04396743050246184\n",
      "Gradient Descent(9809/9999): loss=0.04396743049953607\n",
      "Gradient Descent(9810/9999): loss=0.04396743049661492\n",
      "Gradient Descent(9811/9999): loss=0.04396743049369834\n",
      "Gradient Descent(9812/9999): loss=0.04396743049078637\n",
      "Gradient Descent(9813/9999): loss=0.043967430487878954\n",
      "Gradient Descent(9814/9999): loss=0.04396743048497612\n",
      "Gradient Descent(9815/9999): loss=0.04396743048207785\n",
      "Gradient Descent(9816/9999): loss=0.04396743047918414\n",
      "Gradient Descent(9817/9999): loss=0.04396743047629498\n",
      "Gradient Descent(9818/9999): loss=0.043967430473410375\n",
      "Gradient Descent(9819/9999): loss=0.04396743047053027\n",
      "Gradient Descent(9820/9999): loss=0.04396743046765473\n",
      "Gradient Descent(9821/9999): loss=0.043967430464783706\n",
      "Gradient Descent(9822/9999): loss=0.04396743046191718\n",
      "Gradient Descent(9823/9999): loss=0.04396743045905518\n",
      "Gradient Descent(9824/9999): loss=0.04396743045619766\n",
      "Gradient Descent(9825/9999): loss=0.04396743045334464\n",
      "Gradient Descent(9826/9999): loss=0.04396743045049611\n",
      "Gradient Descent(9827/9999): loss=0.043967430447652056\n",
      "Gradient Descent(9828/9999): loss=0.04396743044481246\n",
      "Gradient Descent(9829/9999): loss=0.043967430441977345\n",
      "Gradient Descent(9830/9999): loss=0.04396743043914668\n",
      "Gradient Descent(9831/9999): loss=0.04396743043632049\n",
      "Gradient Descent(9832/9999): loss=0.043967430433498704\n",
      "Gradient Descent(9833/9999): loss=0.04396743043068141\n",
      "Gradient Descent(9834/9999): loss=0.043967430427868506\n",
      "Gradient Descent(9835/9999): loss=0.04396743042506003\n",
      "Gradient Descent(9836/9999): loss=0.043967430422255954\n",
      "Gradient Descent(9837/9999): loss=0.04396743041945632\n",
      "Gradient Descent(9838/9999): loss=0.043967430416661075\n",
      "Gradient Descent(9839/9999): loss=0.04396743041387019\n",
      "Gradient Descent(9840/9999): loss=0.04396743041108373\n",
      "Gradient Descent(9841/9999): loss=0.04396743040830164\n",
      "Gradient Descent(9842/9999): loss=0.04396743040552395\n",
      "Gradient Descent(9843/9999): loss=0.043967430402750605\n",
      "Gradient Descent(9844/9999): loss=0.043967430399981626\n",
      "Gradient Descent(9845/9999): loss=0.043967430397217004\n",
      "Gradient Descent(9846/9999): loss=0.04396743039445671\n",
      "Gradient Descent(9847/9999): loss=0.04396743039170077\n",
      "Gradient Descent(9848/9999): loss=0.043967430388949166\n",
      "Gradient Descent(9849/9999): loss=0.043967430386201885\n",
      "Gradient Descent(9850/9999): loss=0.04396743038345892\n",
      "Gradient Descent(9851/9999): loss=0.04396743038072027\n",
      "Gradient Descent(9852/9999): loss=0.04396743037798592\n",
      "Gradient Descent(9853/9999): loss=0.0439674303752559\n",
      "Gradient Descent(9854/9999): loss=0.04396743037253013\n",
      "Gradient Descent(9855/9999): loss=0.04396743036980865\n",
      "Gradient Descent(9856/9999): loss=0.043967430367091484\n",
      "Gradient Descent(9857/9999): loss=0.04396743036437857\n",
      "Gradient Descent(9858/9999): loss=0.04396743036166992\n",
      "Gradient Descent(9859/9999): loss=0.043967430358965526\n",
      "Gradient Descent(9860/9999): loss=0.04396743035626539\n",
      "Gradient Descent(9861/9999): loss=0.043967430353569495\n",
      "Gradient Descent(9862/9999): loss=0.04396743035087783\n",
      "Gradient Descent(9863/9999): loss=0.043967430348190416\n",
      "Gradient Descent(9864/9999): loss=0.043967430345507215\n",
      "Gradient Descent(9865/9999): loss=0.04396743034282826\n",
      "Gradient Descent(9866/9999): loss=0.043967430340153504\n",
      "Gradient Descent(9867/9999): loss=0.04396743033748293\n",
      "Gradient Descent(9868/9999): loss=0.04396743033481657\n",
      "Gradient Descent(9869/9999): loss=0.0439674303321544\n",
      "Gradient Descent(9870/9999): loss=0.04396743032949642\n",
      "Gradient Descent(9871/9999): loss=0.04396743032684263\n",
      "Gradient Descent(9872/9999): loss=0.043967430324193\n",
      "Gradient Descent(9873/9999): loss=0.04396743032154753\n",
      "Gradient Descent(9874/9999): loss=0.04396743031890622\n",
      "Gradient Descent(9875/9999): loss=0.04396743031626908\n",
      "Gradient Descent(9876/9999): loss=0.04396743031363606\n",
      "Gradient Descent(9877/9999): loss=0.0439674303110072\n",
      "Gradient Descent(9878/9999): loss=0.04396743030838248\n",
      "Gradient Descent(9879/9999): loss=0.043967430305761876\n",
      "Gradient Descent(9880/9999): loss=0.043967430303145386\n",
      "Gradient Descent(9881/9999): loss=0.04396743030053302\n",
      "Gradient Descent(9882/9999): loss=0.04396743029792475\n",
      "Gradient Descent(9883/9999): loss=0.0439674302953206\n",
      "Gradient Descent(9884/9999): loss=0.04396743029272052\n",
      "Gradient Descent(9885/9999): loss=0.04396743029012455\n",
      "Gradient Descent(9886/9999): loss=0.04396743028753265\n",
      "Gradient Descent(9887/9999): loss=0.04396743028494482\n",
      "Gradient Descent(9888/9999): loss=0.043967430282361095\n",
      "Gradient Descent(9889/9999): loss=0.04396743027978141\n",
      "Gradient Descent(9890/9999): loss=0.04396743027720576\n",
      "Gradient Descent(9891/9999): loss=0.04396743027463418\n",
      "Gradient Descent(9892/9999): loss=0.04396743027206664\n",
      "Gradient Descent(9893/9999): loss=0.04396743026950313\n",
      "Gradient Descent(9894/9999): loss=0.04396743026694366\n",
      "Gradient Descent(9895/9999): loss=0.04396743026438821\n",
      "Gradient Descent(9896/9999): loss=0.04396743026183679\n",
      "Gradient Descent(9897/9999): loss=0.043967430259289376\n",
      "Gradient Descent(9898/9999): loss=0.04396743025674596\n",
      "Gradient Descent(9899/9999): loss=0.043967430254206546\n",
      "Gradient Descent(9900/9999): loss=0.04396743025167113\n",
      "Gradient Descent(9901/9999): loss=0.0439674302491397\n",
      "Gradient Descent(9902/9999): loss=0.043967430246612225\n",
      "Gradient Descent(9903/9999): loss=0.04396743024408875\n",
      "Gradient Descent(9904/9999): loss=0.04396743024156923\n",
      "Gradient Descent(9905/9999): loss=0.04396743023905368\n",
      "Gradient Descent(9906/9999): loss=0.0439674302365421\n",
      "Gradient Descent(9907/9999): loss=0.04396743023403443\n",
      "Gradient Descent(9908/9999): loss=0.043967430231530734\n",
      "Gradient Descent(9909/9999): loss=0.043967430229030956\n",
      "Gradient Descent(9910/9999): loss=0.04396743022653513\n",
      "Gradient Descent(9911/9999): loss=0.04396743022404323\n",
      "Gradient Descent(9912/9999): loss=0.04396743022155523\n",
      "Gradient Descent(9913/9999): loss=0.04396743021907113\n",
      "Gradient Descent(9914/9999): loss=0.043967430216590976\n",
      "Gradient Descent(9915/9999): loss=0.04396743021411468\n",
      "Gradient Descent(9916/9999): loss=0.04396743021164232\n",
      "Gradient Descent(9917/9999): loss=0.04396743020917381\n",
      "Gradient Descent(9918/9999): loss=0.04396743020670919\n",
      "Gradient Descent(9919/9999): loss=0.04396743020424845\n",
      "Gradient Descent(9920/9999): loss=0.043967430201791585\n",
      "Gradient Descent(9921/9999): loss=0.04396743019933856\n",
      "Gradient Descent(9922/9999): loss=0.043967430196889416\n",
      "Gradient Descent(9923/9999): loss=0.04396743019444413\n",
      "Gradient Descent(9924/9999): loss=0.04396743019200267\n",
      "Gradient Descent(9925/9999): loss=0.043967430189565046\n",
      "Gradient Descent(9926/9999): loss=0.04396743018713128\n",
      "Gradient Descent(9927/9999): loss=0.04396743018470131\n",
      "Gradient Descent(9928/9999): loss=0.04396743018227519\n",
      "Gradient Descent(9929/9999): loss=0.04396743017985284\n",
      "Gradient Descent(9930/9999): loss=0.04396743017743435\n",
      "Gradient Descent(9931/9999): loss=0.04396743017501965\n",
      "Gradient Descent(9932/9999): loss=0.04396743017260873\n",
      "Gradient Descent(9933/9999): loss=0.04396743017020162\n",
      "Gradient Descent(9934/9999): loss=0.043967430167798256\n",
      "Gradient Descent(9935/9999): loss=0.043967430165398724\n",
      "Gradient Descent(9936/9999): loss=0.043967430163002905\n",
      "Gradient Descent(9937/9999): loss=0.04396743016061091\n",
      "Gradient Descent(9938/9999): loss=0.04396743015822266\n",
      "Gradient Descent(9939/9999): loss=0.04396743015583814\n",
      "Gradient Descent(9940/9999): loss=0.04396743015345738\n",
      "Gradient Descent(9941/9999): loss=0.04396743015108038\n",
      "Gradient Descent(9942/9999): loss=0.0439674301487071\n",
      "Gradient Descent(9943/9999): loss=0.04396743014633756\n",
      "Gradient Descent(9944/9999): loss=0.04396743014397174\n",
      "Gradient Descent(9945/9999): loss=0.043967430141609656\n",
      "Gradient Descent(9946/9999): loss=0.043967430139251265\n",
      "Gradient Descent(9947/9999): loss=0.04396743013689659\n",
      "Gradient Descent(9948/9999): loss=0.04396743013454563\n",
      "Gradient Descent(9949/9999): loss=0.04396743013219836\n",
      "Gradient Descent(9950/9999): loss=0.043967430129854775\n",
      "Gradient Descent(9951/9999): loss=0.043967430127514875\n",
      "Gradient Descent(9952/9999): loss=0.04396743012517865\n",
      "Gradient Descent(9953/9999): loss=0.04396743012284612\n",
      "Gradient Descent(9954/9999): loss=0.04396743012051722\n",
      "Gradient Descent(9955/9999): loss=0.04396743011819201\n",
      "Gradient Descent(9956/9999): loss=0.04396743011587045\n",
      "Gradient Descent(9957/9999): loss=0.04396743011355252\n",
      "Gradient Descent(9958/9999): loss=0.04396743011123827\n",
      "Gradient Descent(9959/9999): loss=0.043967430108927646\n",
      "Gradient Descent(9960/9999): loss=0.04396743010662064\n",
      "Gradient Descent(9961/9999): loss=0.04396743010431727\n",
      "Gradient Descent(9962/9999): loss=0.04396743010201754\n",
      "Gradient Descent(9963/9999): loss=0.04396743009972143\n",
      "Gradient Descent(9964/9999): loss=0.04396743009742889\n",
      "Gradient Descent(9965/9999): loss=0.043967430095139974\n",
      "Gradient Descent(9966/9999): loss=0.04396743009285465\n",
      "Gradient Descent(9967/9999): loss=0.04396743009057294\n",
      "Gradient Descent(9968/9999): loss=0.04396743008829479\n",
      "Gradient Descent(9969/9999): loss=0.043967430086020255\n",
      "Gradient Descent(9970/9999): loss=0.043967430083749266\n",
      "Gradient Descent(9971/9999): loss=0.04396743008148188\n",
      "Gradient Descent(9972/9999): loss=0.04396743007921802\n",
      "Gradient Descent(9973/9999): loss=0.043967430076957754\n",
      "Gradient Descent(9974/9999): loss=0.04396743007470101\n",
      "Gradient Descent(9975/9999): loss=0.04396743007244785\n",
      "Gradient Descent(9976/9999): loss=0.0439674300701982\n",
      "Gradient Descent(9977/9999): loss=0.043967430067952104\n",
      "Gradient Descent(9978/9999): loss=0.04396743006570954\n",
      "Gradient Descent(9979/9999): loss=0.04396743006347049\n",
      "Gradient Descent(9980/9999): loss=0.04396743006123498\n",
      "Gradient Descent(9981/9999): loss=0.04396743005900298\n",
      "Gradient Descent(9982/9999): loss=0.04396743005677447\n",
      "Gradient Descent(9983/9999): loss=0.04396743005454948\n",
      "Gradient Descent(9984/9999): loss=0.04396743005232799\n",
      "Gradient Descent(9985/9999): loss=0.04396743005010999\n",
      "Gradient Descent(9986/9999): loss=0.04396743004789548\n",
      "Gradient Descent(9987/9999): loss=0.043967430045684444\n",
      "Gradient Descent(9988/9999): loss=0.0439674300434769\n",
      "Gradient Descent(9989/9999): loss=0.0439674300412728\n",
      "Gradient Descent(9990/9999): loss=0.04396743003907219\n",
      "Gradient Descent(9991/9999): loss=0.043967430036875026\n",
      "Gradient Descent(9992/9999): loss=0.04396743003468132\n",
      "Gradient Descent(9993/9999): loss=0.04396743003249107\n",
      "Gradient Descent(9994/9999): loss=0.043967430030304254\n",
      "Gradient Descent(9995/9999): loss=0.04396743002812088\n",
      "Gradient Descent(9996/9999): loss=0.043967430025940946\n",
      "Gradient Descent(9997/9999): loss=0.04396743002376444\n",
      "Gradient Descent(9998/9999): loss=0.04396743002159134\n",
      "Gradient Descent(9999/9999): loss=0.043967430019421674\n",
      "mse loss by least square: 0.043967430019421674\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def least_square_classification_demo(y, x):\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    print(tx.shape)\n",
    "    # w = least squares with respect to tx and y\n",
    "    w = np.zeros(tx.shape[1])\n",
    "    w, mse = least_squares_GD(y, tx, w, 10000, 0.01)\n",
    "    print('mse loss by least square: {}'.format(mse))\n",
    "    visualization(y, x, mean_x, std_x, w, \"classification_by_least_square\")\n",
    "    \n",
    "least_square_classification_demo(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3)\n",
      "SGD (100/10000): loss=0.2359669731905585\n",
      "SGD (200/10000): loss=0.20614909482167498\n",
      "SGD (300/10000): loss=0.053977238934767295\n",
      "SGD (400/10000): loss=0.15466444790969447\n",
      "SGD (500/10000): loss=0.05185761651037462\n",
      "SGD (600/10000): loss=0.14930743093259277\n",
      "SGD (700/10000): loss=0.05001859643839032\n",
      "SGD (800/10000): loss=0.14938752950081577\n",
      "SGD (900/10000): loss=0.048582073574479946\n",
      "SGD (1000/10000): loss=0.15008238853695458\n",
      "SGD (1100/10000): loss=0.04752588969567095\n",
      "SGD (1200/10000): loss=0.15076647279378552\n",
      "SGD (1300/10000): loss=0.04676109839055849\n",
      "SGD (1400/10000): loss=0.15136796760380888\n",
      "SGD (1500/10000): loss=0.046211197414286774\n",
      "SGD (1600/10000): loss=0.1518881705196097\n",
      "SGD (1700/10000): loss=0.04581836562245592\n",
      "SGD (1800/10000): loss=0.15233681963863013\n",
      "SGD (1900/10000): loss=0.04553988989559133\n",
      "SGD (2000/10000): loss=0.15272351769764722\n",
      "SGD (2100/10000): loss=0.04534438399475429\n",
      "SGD (2200/10000): loss=0.15305673428717148\n",
      "SGD (2300/10000): loss=0.0452088375382906\n",
      "SGD (2400/10000): loss=0.15334381578711684\n",
      "SGD (2500/10000): loss=0.04511641412079362\n",
      "SGD (2600/10000): loss=0.15359111427605807\n",
      "SGD (2700/10000): loss=0.04505482205965521\n",
      "SGD (2800/10000): loss=0.15380411674081143\n",
      "SGD (2900/10000): loss=0.0450151111794152\n",
      "SGD (3000/10000): loss=0.15398756020548718\n",
      "SGD (3100/10000): loss=0.04499078496345047\n",
      "SGD (3200/10000): loss=0.15414553242413065\n",
      "SGD (3300/10000): loss=0.04497714587571092\n",
      "SGD (3400/10000): loss=0.15428155955627762\n",
      "SGD (3500/10000): loss=0.04497081299185927\n",
      "SGD (3600/10000): loss=0.15439868235281654\n",
      "SGD (3700/10000): loss=0.04496936690981833\n",
      "SGD (3800/10000): loss=0.15449952226155544\n",
      "SGD (3900/10000): loss=0.044971088632984595\n",
      "SGD (4000/10000): loss=0.15458633871796096\n",
      "SGD (4100/10000): loss=0.044974767797081365\n",
      "SGD (4200/10000): loss=0.15466107874661794\n",
      "SGD (4300/10000): loss=0.044979562033761966\n",
      "SGD (4400/10000): loss=0.154725419868801\n",
      "SGD (4500/10000): loss=0.044984894016134654\n",
      "SGD (4600/10000): loss=0.15478080719254808\n",
      "SGD (4700/10000): loss=0.04499037624703811\n",
      "SGD (4800/10000): loss=0.15482848545411262\n",
      "SGD (4900/10000): loss=0.04499575625130176\n",
      "SGD (5000/10000): loss=0.15486952668333906\n",
      "SGD (5100/10000): loss=0.0450008767561818\n",
      "SGD (5200/10000): loss=0.15490485407981086\n",
      "SGD (5300/10000): loss=0.045005646865762264\n",
      "SGD (5400/10000): loss=0.15493526261077764\n",
      "SGD (5500/10000): loss=0.04501002128571821\n",
      "SGD (5600/10000): loss=0.15496143677506558\n",
      "SGD (5700/10000): loss=0.04501398543097267\n",
      "SGD (5800/10000): loss=0.15498396591853217\n",
      "SGD (5900/10000): loss=0.04501754482189311\n",
      "SGD (6000/10000): loss=0.15500335743532437\n",
      "SGD (6100/10000): loss=0.045020717597647066\n",
      "SGD (6200/10000): loss=0.15502004814440076\n",
      "SGD (6300/10000): loss=0.04502352928731469\n",
      "SGD (6400/10000): loss=0.15503441409178315\n",
      "SGD (6500/10000): loss=0.04502600920929686\n",
      "SGD (6600/10000): loss=0.15504677899507494\n",
      "SGD (6700/10000): loss=0.04502818803889379\n",
      "SGD (6800/10000): loss=0.15505742151734742\n",
      "SGD (6900/10000): loss=0.045030096208507474\n",
      "SGD (7000/10000): loss=0.1550665815319537\n",
      "SGD (7100/10000): loss=0.045031762896466945\n",
      "SGD (7200/10000): loss=0.15507446551773504\n",
      "SGD (7300/10000): loss=0.045033215427651714\n",
      "SGD (7400/10000): loss=0.155081251204927\n",
      "SGD (7500/10000): loss=0.0450344789583072\n",
      "SGD (7600/10000): loss=0.15508709157554681\n",
      "SGD (7700/10000): loss=0.04503557635342955\n",
      "SGD (7800/10000): loss=0.15509211830773628\n",
      "SGD (7900/10000): loss=0.04503652819135225\n",
      "SGD (8000/10000): loss=0.1550964447411888\n",
      "SGD (8100/10000): loss=0.04503735284926424\n",
      "SGD (8200/10000): loss=0.15510016843013663\n",
      "SGD (8300/10000): loss=0.04503806663723682\n",
      "SGD (8400/10000): loss=0.15510337334117058\n",
      "SGD (8500/10000): loss=0.04503868395833642\n",
      "SGD (8600/10000): loss=0.15510613174524107\n",
      "SGD (8700/10000): loss=0.045039217479584615\n",
      "SGD (8800/10000): loss=0.15510850584634603\n",
      "SGD (8900/10000): loss=0.04503967830365958\n",
      "SGD (9000/10000): loss=0.1551105491835135\n",
      "SGD (9100/10000): loss=0.045040076134867664\n",
      "SGD (9200/10000): loss=0.15511230783761118\n",
      "SGD (9300/10000): loss=0.04504041943546725\n",
      "SGD (9400/10000): loss=0.15511382147013594\n",
      "SGD (9500/10000): loss=0.045040715570195476\n",
      "SGD (9600/10000): loss=0.1551151242173554\n",
      "SGD (9700/10000): loss=0.045040970938053315\n",
      "SGD (9800/10000): loss=0.1551162454599406\n",
      "SGD (9900/10000): loss=0.04504119109120719\n",
      "SGD (10000/10000): loss=0.15511721048541222\n",
      "mse loss by least square: 0.15511721048541222\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def least_square_classification_demo(y, x):\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    print(tx.shape)\n",
    "    # w = least squares with respect to tx and y\n",
    "    w = np.zeros(tx.shape[1])\n",
    "    w, mse = least_squares_SGD(y, tx, w, 10000, 0.01)\n",
    "    print('mse loss by least square: {}'.format(mse))\n",
    "    visualization(y, x, mean_x, std_x, w, \"classification_by_least_square\")\n",
    "    \n",
    "least_square_classification_demo(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3)\n",
      "mse loss by least square: 0.04441077212668929\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def least_square_classification_demo(y, x):\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    print(tx.shape)\n",
    "    # w = least squares with respect to tx and y\n",
    "    w = np.zeros(tx.shape[1])\n",
    "    w, mse = ridge_regression(y, tx, lambda_=0.01)\n",
    "    print('mse loss by least square: {}'.format(mse))\n",
    "    visualization(y, x, mean_x, std_x, w, \"classification_by_least_square\")\n",
    "    \n",
    "least_square_classification_demo(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3)\n",
      "Current iteration=0, loss: 138.62943611198907\n",
      "Current iteration=100, loss: 43.46403230562902\n",
      "Current iteration=200, loss: 41.545892808759405\n",
      "Current iteration=300, loss: 41.098638973663114\n",
      "Current iteration=400, loss: 40.96487063560558\n",
      "mse loss by least square: 40.920344364179215\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def least_square_classification_demo(y, x):\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    print(tx.shape)\n",
    "    # w = least squares with respect to tx and y\n",
    "    w = np.zeros(tx.shape[1])\n",
    "    w, mse = logstic_regression(y, tx, w, 500, 0.01)\n",
    "    print('mse loss by least square: {}'.format(mse))\n",
    "    visualization(y, x, mean_x, std_x, w, \"classification_by_least_square\")\n",
    "    \n",
    "least_square_classification_demo(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3)\n",
      "Current iteration=0, loss: 138.62943611198907\n",
      "Current iteration=100, loss: 46.076072692504965\n",
      "Current iteration=200, loss: 45.13699080586536\n",
      "Current iteration=300, loss: 45.028169895329874\n",
      "Current iteration=400, loss: 45.013256927182994\n",
      "mse loss by least square: 45.01110201213487\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def least_square_classification_demo(y, x):\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    print(tx.shape)\n",
    "    # w = least squares with respect to tx and y\n",
    "    w = np.zeros(tx.shape[1])\n",
    "    w, mse = reg_logistic_regression(y, tx, 0.1, w, 500, 0.01)\n",
    "    print('mse loss by least square: {}'.format(mse))\n",
    "    visualization(y, x, mean_x, std_x, w, \"classification_by_least_square\")\n",
    "    \n",
    "least_square_classification_demo(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b0668826dfdb2edaafd20e37675f95a02c7f4a92d40f94c782579ea2d31134d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('pyenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
